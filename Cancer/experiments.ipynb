{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, cross_val_predict, train_test_split, KFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, roc_curve, balanced_accuracy_score, accuracy_score, mean_squared_error\n",
    "from scipy.stats import pearsonr, ttest_rel\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE, Isomap\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import LinearSegmentedColormap, ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "import random\n",
    "import re\n",
    "from itertools import product\n",
    "import os\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#proteomics architecture for classification\n",
    "class proteomics_net(nn.Module):\n",
    "    def __init__(self, input_size, proteomics_hidden_layers, output_size, dropout=0.1):\n",
    "        super(proteomics_net, self).__init__()\n",
    "            \n",
    "        self.proteomics_layers = nn.Sequential(\n",
    "            nn.Linear(input_size, proteomics_hidden_layers[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(proteomics_hidden_layers[0], output_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proteomics_layers(x)\n",
    "        return x\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        pretrained_dict = torch.load(path, map_location=lambda storage, loc: storage)\n",
    "        model_dict = self.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        self.load_state_dict(model_dict)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#GRU architecture for classification\n",
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_size_codes, hidden_size, prediction_module_hidden_sizes, num_layers, output_size, dropout=0.1):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size_codes, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        self.prediction_module = nn.Sequential(\n",
    "            nn.Linear(prediction_module_hidden_sizes[0], output_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, lengths, interpretability=False):\n",
    "        device = x.device\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        x_packed = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        out_packed, _ = self.gru(x_packed, h0)\n",
    "        out, _ = pad_packed_sequence(out_packed, batch_first=True)\n",
    "        \n",
    "        out = out[torch.arange(x.size(0)), lengths-1, :]\n",
    "        out_final = self.prediction_module(out)\n",
    "        if interpretability == False:\n",
    "            return out_final\n",
    "        else:\n",
    "            return out_final, out\n",
    "\n",
    "# Prepare the dataset\n",
    "class PatientDataset(Dataset):\n",
    "    def __init__(self, data, labels, lengths):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.lengths = lengths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx], self.lengths[idx]\n",
    "\n",
    "# Custom collate function for DataLoader\n",
    "def collate_fn(batch):\n",
    "    data, labels, lengths = zip(*batch)\n",
    "    data = pad_sequence(data, batch_first=True, padding_value=0)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "    return data, labels, lengths\n",
    "\n",
    "\n",
    "def normalize_dataset(dataset):\n",
    "    for i, sequence in enumerate(dataset):\n",
    "        dataset[i] = (sequence - sequence.mean(dim=0, keepdim=True)) / (sequence.std(dim=0, keepdim=True) + 1e-8)\n",
    "    return dataset\n",
    "\n",
    "def impute_missing_values(dataset):\n",
    "    # Stack all tensors in the dataset along a new dimension, creating a tensor of shape (num_samples, max_seq_length, num_features)\n",
    "    stacked_data = torch.stack(dataset)\n",
    "\n",
    "    # Calculate the mean of each feature across all samples and sequences, ignoring NaN values\n",
    "    feature_means = torch.nanmean(stacked_data, dim=(0, 1))\n",
    "\n",
    "    # Iterate through the dataset (list of tensors)\n",
    "    for i, sequence in enumerate(dataset):\n",
    "        # Create a boolean mask indicating the positions of NaN values in the sequence\n",
    "        mask = torch.isnan(sequence)\n",
    "\n",
    "        # Replace NaN values in the sequence with the corresponding feature means\n",
    "        # 'expand_as' is used to match the dimensions of the mask and the sequence\n",
    "        dataset[i][mask] = feature_means.expand_as(sequence)[mask]\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Create DataLoaders\n",
    "def create_dataloaders(patient_data, patient_outcomes, lengths, batch_size=64, normalize=False):\n",
    "    \n",
    "    X_train = impute_missing_values(patient_data)\n",
    "    y_train = patient_outcomes\n",
    "    \n",
    "    if normalize:\n",
    "        X_train = normalize_dataset(X_train)\n",
    "        \n",
    "    train_dataset = PatientDataset(X_train, y_train, lengths)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, worker_init_fn=worker_init_fn)\n",
    "    return train_loader\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "def worker_init_fn(worker_id):\n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Full COMET architecture for classification\n",
    "class joint_model(nn.Module):\n",
    "    def __init__(self, input_size_codes, hidden_size, prediction_module_hidden_sizes, num_layers, output_size, input_size_proteomics, proteomics_hidden_layers, combined_hidden_layers, dropout=0.1):\n",
    "        super(joint_model, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size_codes, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        self.prediction_module = nn.Sequential(\n",
    "            nn.Linear(prediction_module_hidden_sizes[0], output_size),\n",
    "        )\n",
    "\n",
    "        self.skip_connect_prot = nn.Sequential(\n",
    "            nn.Linear(input_size_proteomics, proteomics_hidden_layers[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(proteomics_hidden_layers[0], output_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.combined_layers = nn.Sequential(\n",
    "            nn.Linear(input_size_proteomics + hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "        self.final_combine = nn.Linear(3, 1, bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, x, x_proteomics, lengths, interpretability=False, better_latent=None, better_ratio=0.5):\n",
    "        device = x.device\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        x_packed = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        out_packed, _ = self.gru(x_packed, h0)\n",
    "        out, _ = pad_packed_sequence(out_packed, batch_first=True)\n",
    "        \n",
    "        out_ehr = out[torch.arange(x.size(0)), lengths-1, :]\n",
    "        \n",
    "        if x_proteomics == None:\n",
    "            return out_ehr\n",
    "\n",
    "        if better_latent is not None:\n",
    "            out_ehr = better_ratio * better_latent + (1-better_ratio) * out_ehr\n",
    "                \n",
    "        out_combined = torch.cat((out_ehr, x_proteomics), 1)\n",
    "        \n",
    "        out_combined = self.combined_layers(out_combined)\n",
    "        \n",
    "        pred_proteomics = self.skip_connect_prot(x_proteomics)\n",
    "        pred_ehr = self.prediction_module(out_ehr)\n",
    "        \n",
    "        final_pred = torch.sigmoid(self.final_combine(torch.cat((pred_proteomics, pred_ehr, out_combined), 1)))\n",
    "#         final_pred = self.final_combine(torch.cat((pred_proteomics, pred_ehr), 1))\n",
    "        \n",
    "        if interpretability == False:\n",
    "            return final_pred\n",
    "        else:\n",
    "            return final_pred, (out_ehr, pred_proteomics, pred_ehr, out_combined, final_pred, self.final_combine.weight)\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        pretrained_dict = torch.load(path, map_location=lambda storage, loc: storage)\n",
    "        model_dict = self.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        self.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataBuilder(Dataset):\n",
    "    def __init__(self, x, y, standardizer):\n",
    "        self.x, self.y, self.standardizer = x, y, standardizer\n",
    "        self.len=self.x.shape[0]\n",
    "    def __getitem__(self,index):      \n",
    "        return (self.x[index], self.y[index])\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_experiment(EHR_codes, proteomics, patient_indices, outcomes, lengths, experiment_name, lr, lr_decay,\n",
    "                   bs, prot_hidden_dim=32, train_indices=None, val_indices=None, test_indices=None, feature_types='EHR', model_path='', fine_tune=False, seed=42, num_layers=2,hidden_dim=400,\n",
    "                   dropout=0.4, return_preds=False, return_interpretability=False, return_grads=False,\n",
    "                   hyperparam_tuning=False):\n",
    "    \"\"\"\n",
    "    EHR_codes: pre-processed data for codes of shape (num_patients, max_length, embedding_dim)v\n",
    "    EHR_vitals: pre-processed data for codes of shape (num_patients, max_length, embedding_dim)\n",
    "    proteomics: dataframe with proteomics data\n",
    "    patient_indices: dataframe with sample IDs and row numbers in pre-processed matrices\n",
    "    outcomes: array with DOS\n",
    "    lengths: array with lengths (i.e. number of visits) to help with padding\n",
    "    experiment_name: string for file name for models\n",
    "    lr: float for learning rate\n",
    "    lr_decay: float for learning rate decay\n",
    "    bs: int for batch size\n",
    "    feature_types: string either 'EHR', 'metab', 'both'\n",
    "    model_path: string for file path to model if loading a pre-trained model\n",
    "    fine_tune: boolean for whether or not EHR weight should be learned, can only be true if model != ''\n",
    "    seed: int, random_seed for train/test/val split and seeding model \n",
    "    num_layers: number of GRU layers in RNN\n",
    "    hidden_dim: hidden_dim of GRU output\n",
    "    dropout: dropout weight in model\n",
    "    return_preds: setting to control output of function, if True we return the predictions\n",
    "    return_interpretability: setting to control output of function, if True we return some additional data to help with interpretability analysis\n",
    "    return_grads: setting to control output of function, if True we return the gradient\n",
    "    hyperparam_tuning: setting to control whether or not we save the model at each epoch (if True, we do not)\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "    prediction_module_hidden_sizes = [hidden_dim,hidden_dim//2, hidden_dim//4, hidden_dim//8]\n",
    "    \n",
    "    assert feature_types in ['EHR','metab','both']   \n",
    "    if feature_types == 'metab': assert model_path == ''\n",
    "    if (model_path != '') & (feature_types == 'both'): assert fine_tune==True\n",
    "    if hyperparam_tuning == False: assert train_indices == None\n",
    "\n",
    "    \n",
    "    if hyperparam_tuning == False:\n",
    "        maternal_IDs = patient_indices['sample_ID']\n",
    "\n",
    "        train_ratio = 0.70\n",
    "        test_ratio = 0.15\n",
    "        val_ratio = 0.15\n",
    "\n",
    "        # First, split the unique_ids into train and temp (test + validation) sets\n",
    "        train_ids, temp_ids = train_test_split(maternal_IDs, test_size=(test_ratio + val_ratio),random_state=seed, stratify=outcomes)\n",
    "        \n",
    "        patient_indices_temp = patient_indices.copy(deep=True)\n",
    "\n",
    "        patient_indices_temp.index = patient_indices_temp['sample_ID']\n",
    "        temp_indices = patient_indices_temp.loc[temp_ids,'array_index'].values\n",
    "\n",
    "        # Next, split the temp_ids into test and validation sets\n",
    "        test_ids, val_ids = train_test_split(temp_ids, test_size=(val_ratio / (test_ratio + val_ratio)), random_state=seed, stratify=outcomes[temp_indices])\n",
    "        proteomics = proteomics.merge(patient_indices[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "        train_indices = patient_indices[patient_indices['sample_ID'].isin(train_ids)]['array_index'].values\n",
    "        np.random.shuffle(train_indices)\n",
    "        test_indices = patient_indices[patient_indices['sample_ID'].isin(test_ids)]['array_index'].values\n",
    "        val_indices = patient_indices[patient_indices['sample_ID'].isin(val_ids)]['array_index'].values\n",
    "\n",
    "    #divide the input data into the train/test/val splits determined above \n",
    "    train_EHR_codes = EHR_codes[train_indices,:,:]\n",
    "    if feature_types != 'EHR':\n",
    "        train_proteomics = proteomics[train_indices,:]\n",
    "        scaler = StandardScaler()\n",
    "        train_proteomics = scaler.fit_transform(train_proteomics)\n",
    "    train_outcomes = outcomes[train_indices]\n",
    "    train_lengths = lengths[train_indices]\n",
    "    \n",
    "    test_EHR_codes = EHR_codes[test_indices, :, :]\n",
    "    if feature_types != 'EHR':\n",
    "        test_proteomics = proteomics[test_indices, :]\n",
    "        scaler = StandardScaler()\n",
    "        test_proteomics = scaler.fit_transform(test_proteomics)\n",
    "    test_outcomes = outcomes[test_indices]\n",
    "    test_lengths = lengths[test_indices]\n",
    "\n",
    "    val_EHR_codes = EHR_codes[val_indices, :, :]\n",
    "    if feature_types != 'EHR':\n",
    "        val_proteomics = proteomics[val_indices, :]\n",
    "        scaler = StandardScaler()\n",
    "        val_proteomics = scaler.fit_transform(val_proteomics)\n",
    "    val_outcomes = outcomes[val_indices]\n",
    "    val_lengths = lengths[val_indices]\n",
    "        \n",
    "    all_EHR_codes = EHR_codes\n",
    "    scaler = StandardScaler()\n",
    "    all_outcomes = outcomes\n",
    "    all_lengths = lengths\n",
    "    \n",
    "    #convert to tensors for input to pytorch models\n",
    "    train_EHR_codes = [torch.tensor(data).float() for data in train_EHR_codes]  \n",
    "    train_EHR_codes = [torch.nan_to_num(x) for x in train_EHR_codes]\n",
    "    if feature_types != 'EHR':\n",
    "        train_proteomics = torch.tensor(train_proteomics).float()\n",
    "        train_proteomics = torch.nan_to_num(train_proteomics)\n",
    "    train_outcomes = torch.tensor(train_outcomes).float()\n",
    "    \n",
    "    test_EHR_codes = [torch.tensor(data).float() for data in test_EHR_codes]\n",
    "    test_EHR_codes = [torch.nan_to_num(x) for x in test_EHR_codes]\n",
    "    if feature_types != 'EHR':\n",
    "        test_proteomics = torch.tensor(test_proteomics).float()\n",
    "        test_proteomics = torch.nan_to_num(test_proteomics)\n",
    "    test_outcomes = torch.tensor(test_outcomes).float()\n",
    "\n",
    "    val_EHR_codes = [torch.tensor(data).float() for data in val_EHR_codes]\n",
    "    val_EHR_codes = [torch.nan_to_num(x) for x in val_EHR_codes]\n",
    "    if feature_types != 'EHR':\n",
    "        val_proteomics = torch.tensor(val_proteomics).float()\n",
    "        val_proteomics = torch.nan_to_num(val_proteomics)\n",
    "    val_outcomes = torch.tensor(val_outcomes).float()\n",
    "    \n",
    "    all_EHR_codes = [torch.tensor(data).float() for data in all_EHR_codes]\n",
    "    all_EHR_codes = [torch.nan_to_num(x) for x in all_EHR_codes]\n",
    "    all_outcomes = torch.tensor(all_outcomes).float()\n",
    "\n",
    "    #create necessary data loaders\n",
    "    if feature_types != 'EHR':\n",
    "        data_set_train = DataBuilder(train_proteomics, train_outcomes, scaler)\n",
    "        train_loader_proteomics = DataLoader(dataset=data_set_train,batch_size=bs, worker_init_fn=worker_init_fn)\n",
    "    train_loader_codes = create_dataloaders(train_EHR_codes, train_outcomes, train_lengths, bs)\n",
    "\n",
    "    if feature_types != 'EHR':\n",
    "        data_set_test = DataBuilder(test_proteomics, test_outcomes, scaler)\n",
    "        test_loader_proteomics = DataLoader(dataset=data_set_test,batch_size=bs, worker_init_fn=worker_init_fn)\n",
    "    test_loader_codes = create_dataloaders(test_EHR_codes, test_outcomes, test_lengths, bs)\n",
    "\n",
    "    if feature_types != 'EHR':\n",
    "        data_set_val = DataBuilder(val_proteomics, val_outcomes, scaler)\n",
    "        val_loader_proteomics = DataLoader(dataset=data_set_val,batch_size=100*bs, worker_init_fn=worker_init_fn)\n",
    "    val_loader_codes = create_dataloaders(val_EHR_codes, val_outcomes, val_lengths, 100*bs)\n",
    "            \n",
    "    all_loader_codes = create_dataloaders(all_EHR_codes, all_outcomes, all_lengths, 1000)   \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    #initialize correct model, if we're doing baseline experiments\n",
    "    if model_path == '':\n",
    "        if feature_types == 'EHR':\n",
    "            model = GRUNet(EHR_codes.shape[2], hidden_dim, prediction_module_hidden_sizes, num_layers, 1, dropout).to(device)\n",
    "        elif feature_types == 'metab':\n",
    "            model = proteomics_net(proteomics.shape[1], [prot_hidden_dim], 1, dropout).to(device)\n",
    "        elif feature_types == 'both':\n",
    "            model = joint_model(EHR_codes.shape[2], hidden_dim, prediction_module_hidden_sizes, num_layers, 1, proteomics.shape[1], [prot_hidden_dim], [64, 32, 16, 8], dropout).to(device)\n",
    "    \n",
    "    #initialize correct model if we're using pre-trained EHR weight from an existing model\n",
    "    else:\n",
    "        if feature_types == 'EHR':\n",
    "            model = GRUNet(EHR_codes.shape[2], hidden_dim, prediction_module_hidden_sizes, num_layers, 1, dropout)\n",
    "            model_state_dict = torch.load(model_path)\n",
    "            model.load_state_dict(model_state_dict)\n",
    "            model.to(device)\n",
    "            if fine_tune == False:\n",
    "                model.eval()\n",
    "                criterion = nn.BCELoss()\n",
    "                val_predictions = []\n",
    "                val_true_labels = []\n",
    "                running_loss_val, num_samples_val = 0, 0\n",
    "                with torch.no_grad():\n",
    "                    for (inputs_codes, labels_codes, lengths_codes) in (val_loader_codes):\n",
    "                            inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                            outputs = model(inputs_codes, lengths_codes)\n",
    "\n",
    "                            loss = criterion(outputs.squeeze(), labels)\n",
    "                            running_loss_val += (loss.item()*lengths_codes.shape[0])\n",
    "                            num_samples_val += lengths_codes.shape[0]\n",
    "                            val_predictions.extend(outputs.squeeze().tolist())\n",
    "                            val_true_labels.extend(labels.tolist())\n",
    "                    \n",
    "                val_loss = running_loss_val / (num_samples_val)\n",
    "                pearson_corr = roc_auc_score(val_true_labels, val_predictions)\n",
    "\n",
    "                print(f'Total Loss: {val_loss:.4f}, AUC: {pearson_corr:.4f}')\n",
    "                if return_preds == True:\n",
    "                    return pearson_corr, val_loss, None, val_true_labels, val_predictions, val_indices\n",
    "                else:\n",
    "                    return pearson_corr, val_loss, None\n",
    "            elif fine_tune == True:\n",
    "                model = GRUNet(EHR_codes.shape[2], hidden_dim, prediction_module_hidden_sizes, num_layers, 1, dropout)\n",
    "                model_state_dict = torch.load(model_path)\n",
    "                model.load_state_dict(model_state_dict, strict=False)\n",
    "                model.to(device)\n",
    "                for name, param in model.named_parameters():\n",
    "                    if ('gru' in name):\n",
    "                        param.requires_grad = False\n",
    "\n",
    "        elif feature_types == 'both':\n",
    "            model = joint_model(EHR_codes.shape[2], hidden_dim, prediction_module_hidden_sizes, num_layers, 1, proteomics.shape[1], [prot_hidden_dim], [64, 32, 16, 8], dropout).to(device)\n",
    "            model_state_dict = torch.load(model_path)\n",
    "            gru_weights = {}\n",
    "            for k,v in zip(model_state_dict.keys(), model_state_dict.values()):\n",
    "                if ('gru' in k) | ('pred' in k):\n",
    "                    gru_weights[k] = v\n",
    "            model.load_state_dict(gru_weights, strict=False)\n",
    "            model.to(device)\n",
    "            \n",
    "            for name, param in model.named_parameters():\n",
    "                if ('gru' in name):\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    #prepare for training    \n",
    "    epoch_arr = []\n",
    "    num_samples_in_batch = []\n",
    "    gradient_arr = []\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=lr_decay)\n",
    "    num_epochs = 200\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    val_losses = []\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        if hyperparam_tuning == False:\n",
    "            torch.cuda.synchronize()  # Wait for all CUDA kernels to finish\n",
    "            torch.save(model.state_dict(), './{}_epoch{}.pth'.format(experiment_name, epoch))\n",
    "        running_loss_train, num_train_samples = 0, 0\n",
    "        if feature_types == 'EHR':\n",
    "            for (inputs_codes, labels_codes, lengths_codes)in train_loader_codes:\n",
    "                if feature_types != 'metab':\n",
    "                    inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                outputs = model(inputs_codes, lengths_codes)\n",
    "                if outputs.shape[0] == 1:\n",
    "                    loss = criterion(outputs.squeeze().unsqueeze(0), labels)\n",
    "                else:\n",
    "                    loss = criterion(outputs.squeeze(), labels)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss_train += (loss.item()*lengths_codes.shape[0])\n",
    "                num_train_samples += lengths_codes.shape[0]\n",
    "        else:\n",
    "            for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(train_loader_codes, train_loader_proteomics)):\n",
    "                if feature_types != 'metab':\n",
    "                    inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                if feature_types != 'EHR':\n",
    "                    inputs_proteomics, labels = inputs_proteomics.to(device), labels_proteomics.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                if feature_types == 'metab':\n",
    "                    outputs = model(inputs_proteomics)\n",
    "                elif feature_types == 'EHR':\n",
    "                    outputs = model(inputs_codes, lengths_codes)\n",
    "                elif feature_types == 'both':\n",
    "                    outputs, interpretability_outputs = model(inputs_codes, inputs_proteomics, lengths_codes, interpretability=True)\n",
    "\n",
    "                if outputs.shape[0] == 1:\n",
    "                    loss = criterion(outputs.squeeze().unsqueeze(0), labels)\n",
    "                else:\n",
    "                    loss = criterion(outputs.squeeze(), labels)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if return_grads:\n",
    "                    epoch_arr.append(epoch)\n",
    "                    num_samples_in_batch.append(outputs.shape[0])\n",
    "                    gradient_arr.append(model.skip_connect_prot[0].weight.grad.cpu().numpy()[0])\n",
    "                running_loss_train += (loss.item()*lengths_codes.shape[0])\n",
    "                num_train_samples += lengths_codes.shape[0]\n",
    "\n",
    "        train_loss = running_loss_train / num_train_samples\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        running_loss_test, num_samples_test = 0, 0\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        if feature_types == 'EHR':\n",
    "            with torch.no_grad():\n",
    "                for (inputs_codes, labels_codes, lengths_codes)in test_loader_codes:\n",
    "                    inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                    outputs = model(inputs_codes, lengths_codes)\n",
    "                    outputs = outputs.squeeze(1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    running_loss_test += (loss.item()*lengths_codes.shape[0])\n",
    "                    num_samples_test += lengths_codes.shape[0]\n",
    "                    predictions.extend(outputs.tolist())\n",
    "                    true_labels.extend(labels.tolist())\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(test_loader_codes, test_loader_proteomics)):\n",
    "                    if feature_types != 'metab':\n",
    "                        inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                    if feature_types != 'EHR':\n",
    "                        inputs_proteomics, labels = inputs_proteomics.to(device), labels_proteomics.to(device)\n",
    "\n",
    "                    if feature_types == 'metab':\n",
    "                        outputs = model(inputs_proteomics)\n",
    "                    elif feature_types == 'EHR':\n",
    "                        outputs = model(inputs_codes, lengths_codes)\n",
    "                    elif feature_types == 'both':\n",
    "                        outputs = model(inputs_codes, inputs_proteomics, lengths_codes)\n",
    "                    \n",
    "                    outputs = outputs.squeeze(1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    running_loss_test += (loss.item()*lengths_codes.shape[0])\n",
    "                    num_samples_test += lengths_codes.shape[0]\n",
    "                    predictions.extend(outputs.tolist())\n",
    "                    true_labels.extend(labels.tolist())\n",
    "        test_loss = running_loss_test / (num_samples_test)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        #check for early stopping on test set\n",
    "        patience = 5\n",
    "        pearson_corr = roc_auc_score(true_labels, predictions)\n",
    "\n",
    "        #print(f'Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, AUC: {pearson_corr:.4f}')\n",
    "        \n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            counter = 0\n",
    "            torch.cuda.synchronize()  # Wait for all CUDA kernels to finish\n",
    "            torch.save(model.state_dict(), './{}.pth'.format(experiment_name))\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                break\n",
    "\n",
    "    #once training is finished, make predictions on validation set\n",
    "    model.load_state_dict(torch.load('./{}.pth'.format(experiment_name)))\n",
    "    running_loss_val, num_samples_val = 0, 0\n",
    "    val_predictions = []\n",
    "    val_true_labels = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        if feature_types == 'EHR':\n",
    "            for (inputs_codes, labels_codes, lengths_codes) in val_loader_codes:\n",
    "                if feature_types != 'metab':\n",
    "                    inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                if feature_types != 'EHR':\n",
    "                    inputs_proteomics, labels = inputs_proteomics.to(device), labels_proteomics.to(device)\n",
    "\n",
    "                if feature_types == 'metab':\n",
    "                    outputs = model(inputs_proteomics)\n",
    "                elif feature_types == 'EHR':\n",
    "                    outputs = model(inputs_codes, lengths_codes)\n",
    "                elif feature_types == 'both':\n",
    "                    outputs, interpretability_outputs = model(inputs_codes, inputs_proteomics, lengths_codes, interpretability=True)\n",
    "                    \n",
    "                loss = criterion(outputs.squeeze(), labels)\n",
    "                running_loss_val += (loss.item()*lengths_codes.shape[0])\n",
    "                num_samples_val += lengths_codes.shape[0]\n",
    "                val_predictions.extend(outputs.squeeze().tolist())\n",
    "                val_true_labels.extend(labels.tolist())\n",
    "        else:\n",
    "            for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(val_loader_codes, val_loader_proteomics)):\n",
    "                if feature_types != 'metab':\n",
    "                    inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                if feature_types != 'EHR':\n",
    "                    inputs_proteomics, labels = inputs_proteomics.to(device), labels_proteomics.to(device)\n",
    "\n",
    "                if feature_types == 'metab':\n",
    "                    outputs = model(inputs_proteomics)\n",
    "                elif feature_types == 'EHR':\n",
    "                    outputs = model(inputs_codes, lengths_codes)\n",
    "                elif feature_types == 'both':\n",
    "                    outputs, interpretability_outputs = model(inputs_codes, inputs_proteomics, lengths_codes, interpretability=True)\n",
    "\n",
    "                loss = criterion(outputs.squeeze(), labels)\n",
    "                running_loss_val += (loss.item()*lengths_codes.shape[0])\n",
    "                num_samples_val += lengths_codes.shape[0]\n",
    "                val_predictions.extend(outputs.squeeze().tolist())\n",
    "                val_true_labels.extend(labels.tolist())\n",
    "    val_loss = running_loss_val / (num_samples_val)\n",
    "    val_losses.append(val_loss)\n",
    "    pearson_corr = roc_auc_score(val_true_labels, val_predictions)\n",
    "\n",
    "    print(f'Epoch: {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, AUC: {pearson_corr:.4f}')\n",
    "    \n",
    "    if hyperparam_tuning == True:\n",
    "        os.remove('./{}.pth'.format(experiment_name))\n",
    "    if return_preds == True:\n",
    "        if return_interpretability == False:\n",
    "            if return_grads:\n",
    "                df = pd.DataFrame([epoch_arr, num_samples_in_batch, gradient_arr]).T\n",
    "                df.columns = ['epoch', 'num_samples_in_batch','gradient']\n",
    "                return pearson_corr, val_loss, None, val_outcomes, np.array(val_predictions), val_indices, df, train_losses, test_losses, val_losses, train_indices, test_indices\n",
    "            else:\n",
    "                return pearson_corr, val_loss, None, val_outcomes, np.array(val_predictions), val_indices\n",
    "        else:\n",
    "            if return_grads:\n",
    "                df = pd.DataFrame([epoch_arr, num_samples_in_batch, gradient_arr]).T\n",
    "                df.columns = ['epoch', 'num_samples_in_batch','gradient']\n",
    "                return pearson_corr, val_loss, None, val_outcomes, np.array(val_predictions), val_indices, interpretability_outputs, None, None, df, train_losses, test_losses, val_losses, train_indices, test_indices\n",
    "            else:\n",
    "                return pearson_corr, val_loss, None, val_outcomes, np.array(val_predictions), val_indices, interpretability_outputs, None, None\n",
    "    else:\n",
    "        return pearson_corr, val_loss, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load omics cohort data w/ omics word2vec model\n",
    "RNN_data_codes_omics = np.load('/mnt/project/cancer_mortality_3yr_LARGER_data/RNN_data_omics_omicsw2v_lc.npy')\n",
    "RNN_data_outcomes_omics = np.load('/mnt/project/cancer_mortality_3yr_LARGER_data/outcomes_omics_omicsw2v_lc_3yr.npy')\n",
    "RNN_data_lengths_omics = np.load('/mnt/project/cancer_mortality_3yr_LARGER_data/visit_count_omics_omicsw2v_lc.npy')\n",
    "patient_indices_omics = pd.read_csv('/mnt/project/cancer_mortality_3yr_LARGER_data/eid_indices_omics_omicsw2v_lc.csv').drop('Unnamed: 0',axis=1)\n",
    "patient_indices_omics.columns = ['sample_ID','array_index']\n",
    "RNN_data_codes_omics.shape, RNN_data_outcomes_omics.shape, RNN_data_lengths_omics.shape, patient_indices_omics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#loads omics cohort data w/ embeddings from pre-trained word2vec model (for COMET experiments)\n",
    "RNN_data_codes = np.load('/mnt/project/cancer_mortality_3yr_LARGER_data/RNN_data_omics_PTw2v_lc.npy')\n",
    "RNN_data_outcomes = np.load('/mnt/project/cancer_mortality_3yr_LARGER_data/outcomes_omics_PTw2v_lc_3yr.npy')\n",
    "RNN_data_lengths = np.load('/mnt/project/cancer_mortality_3yr_LARGER_data/visit_count_omics_PTw2v_lc.npy')\n",
    "patient_indices = pd.read_csv('/mnt/project/cancer_mortality_3yr_LARGER_data/eid_indices_omics_PTw2v_lc.csv').drop('Unnamed: 0',axis=1)\n",
    "patient_indices.columns = ['sample_ID','array_index']\n",
    "RNN_data_codes.shape, RNN_data_outcomes.shape, RNN_data_lengths.shape, patient_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RNN_data_codes_PTMODEL = np.load('/mnt/project/cancer_mortality_3yr_LARGER_data/RNN_data_PT_lc_LARGER.npy')\n",
    "RNN_data_outcomes_PTMODEL = np.load('/mnt/project/cancer_mortality_3yr_LARGER_data/outcomes_PT_lc_LARGER.npy')\n",
    "RNN_data_lengths_PTMODEL = np.load('/mnt/project/cancer_mortality_3yr_LARGER_data/visit_count_PT_lc_LARGER.npy')\n",
    "patient_indices_PTMODEL = pd.read_csv('/mnt/project/cancer_mortality_3yr_LARGER_data/eid_indices_PT_lc_LARGER.csv').drop('Unnamed: 0',axis=1)\n",
    "patient_indices_PTMODEL.columns = ['sample_ID','array_index']\n",
    "RNN_data_codes_PTMODEL.shape, RNN_data_outcomes_PTMODEL.shape, RNN_data_lengths_PTMODEL.shape, patient_indices_PTMODEL.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "overall_best_params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outcome_list = ['mortality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_TRIALS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#load all proteomics\n",
    "proteomics = pd.read_csv('/mnt/project/all_proteomics_lc.csv')\n",
    "proteomics = proteomics[proteomics['eid'].isin(patient_indices['sample_ID'])]\n",
    "proteomics = proteomics.dropna(axis=1, how='all')\n",
    "proteomics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "proteomics = proteomics.rename({'eid':'sample_ID'},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#impute missing values\n",
    "min_values = proteomics.drop(columns=['sample_ID']).min()\n",
    "\n",
    "impute_values = min_values / 2\n",
    "\n",
    "# Replace NaN values with the impute values calculated for each protein\n",
    "for column in proteomics.columns:\n",
    "    if column != 'sample_ID':\n",
    "        proteomics[column].fillna(impute_values[column], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run cell below if loading old hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#if we're using the cell for downstream analysis and skipping the hyperparam selection, run this cell to load previously saved hyperparams\n",
    "overall_best_params = pickle.load(open('/mnt/project/cancer_mortality_3yr_LARGER_models_results/best_hyperparams_LARGER.pkl','rb'))\n",
    "\n",
    "best_num_layers = overall_best_params['mortality']['PT']['num_layers']\n",
    "best_dropout = overall_best_params['mortality']['PT']['dropout']\n",
    "best_model_name = overall_best_params['mortality']['PT']['model_name']\n",
    "best_hidden_dim = overall_best_params['mortality']['PT']['hidden_dim']\n",
    "print(best_num_layers, best_dropout, best_hidden_dim, best_model_name)\n",
    "\n",
    "best_num_layers_omics = overall_best_params['mortality']['exp1']['num_layers']\n",
    "best_dropout_omics = overall_best_params['mortality']['exp1']['dropout']\n",
    "best_hidden_dim_omics = overall_best_params['mortality']['exp1']['hidden_dim']\n",
    "print(best_num_layers_omics, best_dropout_omics, best_hidden_dim_omics)\n",
    "\n",
    "best_protein_hidden = overall_best_params['mortality']['exp2']['prot_hidden_dim']\n",
    "print(best_protein_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hyperparam Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "grid_search = {\n",
    "    'batch_size': [512],\n",
    "    'lr': [1e-2, 1e-3, 1e-4],\n",
    "    'dropout': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'lr_decay': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "    'layers': [2, 4],\n",
    "    'hidden_dim': [400]\n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "all_params = [dict(zip(grid_search.keys(), values)) for values in product(*grid_search.values())]\n",
    "\n",
    "#run hyperparam selection for PT model\n",
    "num_layers_arr = []\n",
    "dropout_arr = []\n",
    "lr_arr = []\n",
    "lr_decay_arr = []\n",
    "hidden_dim_arr = []\n",
    "batch_size_arr = []\n",
    "split_num_arr = []\n",
    "loss_arr = []\n",
    "for i in tqdm(outcome_list):\n",
    "    overall_best_params[i] = {'PT': {}}\n",
    "\n",
    "    maternal_IDs = patient_indices_PTMODEL['sample_ID'].unique()\n",
    "    \n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    split_num = 0\n",
    "    for train_index, test_index in kf.split(maternal_IDs):\n",
    "        split_num += 1\n",
    "        results_dict = {}\n",
    "        train_IDs = maternal_IDs[train_index]\n",
    "        test_IDs = maternal_IDs[test_index]\n",
    "        \n",
    "        sample_size = int(0.2 * len(train_index))\n",
    "        random_indices = np.random.choice(train_IDs.shape[0], sample_size, replace=False)\n",
    "        val_IDs = train_IDs[random_indices]\n",
    "        train_IDs = np.delete(train_IDs, random_indices)\n",
    "        \n",
    "\n",
    "        train_indices = patient_indices_PTMODEL[patient_indices_PTMODEL['sample_ID'].isin(train_IDs)]['array_index'].values\n",
    "        np.random.shuffle(train_indices)\n",
    "        test_indices = patient_indices_PTMODEL[patient_indices_PTMODEL['sample_ID'].isin(test_IDs)]['array_index'].values\n",
    "        val_indices = patient_indices_PTMODEL[patient_indices_PTMODEL['sample_ID'].isin(val_IDs)]['array_index'].values\n",
    "\n",
    "        # Iterate through the hyperparameter combinations\n",
    "        for param_set in tqdm(all_params):\n",
    "            bs = param_set['batch_size']\n",
    "            lr = param_set['lr']\n",
    "            dropout = param_set['dropout']\n",
    "            lr_decay = param_set['lr_decay']\n",
    "            layers = param_set['layers']\n",
    "            hidden_dim = param_set['hidden_dim']\n",
    "            \n",
    "            model_name = 'PT_MODEL_{}_{}_{}_{}_{}'.format(layers,lr,lr_decay,dropout, hidden_dim)\n",
    "            print(param_set)\n",
    "            val_r, val_loss, val_rmse = run_experiment(RNN_data_codes_PTMODEL, None,\n",
    "            patient_indices_PTMODEL, RNN_data_outcomes_PTMODEL, RNN_data_lengths_PTMODEL, model_name, \n",
    "            lr, lr_decay, bs, train_indices=train_indices, test_indices=test_indices,\n",
    "                                                       val_indices=val_indices,feature_types='EHR', model_path='', fine_tune=False, seed=42, \n",
    "            hidden_dim=hidden_dim, num_layers=layers, dropout=dropout, hyperparam_tuning=True)\n",
    "            num_layers_arr.append(layers)\n",
    "            dropout_arr.append(dropout)\n",
    "            lr_arr.append(lr)\n",
    "            lr_decay_arr.append(lr_decay)\n",
    "            hidden_dim_arr.append(hidden_dim)\n",
    "            batch_size_arr.append(bs)\n",
    "            split_num_arr.append(split_num)\n",
    "            loss_arr.append(val_loss)\n",
    "        print('PT')\n",
    "        print('outcome {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#save best hyperparams\n",
    "hyperparam_df = pd.DataFrame([num_layers_arr, dropout_arr, lr_arr, lr_decay_arr, hidden_dim_arr, batch_size_arr, split_num_arr,loss_arr]).T\n",
    "hyperparam_df.columns = ['num_layers','dropout','lr','lr_decay','hidden_dim','bs','split_num','val_loss']\n",
    "hyperparam_df = hyperparam_df.groupby(['num_layers','dropout','lr','lr_decay','hidden_dim','bs']).mean()\n",
    "num_layers, dropout, lr, lr_decay, hidden_dim, bs = hyperparam_df['val_loss'].idxmin()\n",
    "print(np.min(hyperparam_df['val_loss']))\n",
    "model_name = 'PT_MODEL_{}_{}_{}_{}_{}'.format(num_layers,lr,lr_decay,dropout, hidden_dim)\n",
    "overall_best_params['mortality']['PT'] = {'num_layers': int(num_layers),'lr': lr,'lr_decay': lr_decay, 'dropout': dropout,\n",
    "                                'hidden_dim': int(hidden_dim), 'batch_size': int(bs), 'model_name': model_name}\n",
    "with open(\"best_hyperparams_LARGER.pkl\", \"wb\") as f:\n",
    "        pickle.dump(overall_best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "dx upload best_hyperparams_LARGER.pkl --path /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_num_layers = overall_best_params['mortality']['PT']['num_layers']\n",
    "best_dropout = overall_best_params['mortality']['PT']['dropout']\n",
    "best_model_name = overall_best_params['mortality']['PT']['model_name']\n",
    "best_hidden_dim = overall_best_params['mortality']['PT']['hidden_dim']\n",
    "best_num_layers, best_dropout, best_hidden_dim, best_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparam_df.to_csv('hyperparam_sweep_PT_LARGER.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "dx upload hyperparam_sweep_PT_LARGER.csv --path /\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "overall_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#train pre-trained model!\n",
    "val_r, val_loss, val_rmse = run_experiment(RNN_data_codes_PTMODEL, proteomics,\n",
    "    patient_indices_PTMODEL, RNN_data_outcomes_PTMODEL, RNN_data_lengths_PTMODEL, overall_best_params['mortality']['PT']['model_name'], \n",
    "    overall_best_params['mortality']['PT']['lr'], overall_best_params['mortality']['PT']['lr_decay'],\n",
    "       512, feature_types='EHR', model_path='', fine_tune=False, seed=3, \n",
    "    hidden_dim=overall_best_params['mortality']['PT']['hidden_dim'], num_layers=overall_best_params['mortality']['PT']['num_layers'], dropout=overall_best_params['mortality']['PT']['dropout'], hyperparam_tuning=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid_search = {\n",
    "    'batch_size': [16],\n",
    "    'lr': [1e-2, 1e-1, 1e-3, 1e-4],\n",
    "    'dropout': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'lr_decay': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "    'layers': [2, 4],\n",
    "    'hidden_dim': [400]\n",
    "}\n",
    "all_params = [dict(zip(grid_search.keys(), values)) for values in product(*grid_search.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#run hyperparam loop for omics baseline\n",
    "num_layers_arr = []\n",
    "dropout_arr = []\n",
    "lr_arr = []\n",
    "lr_decay_arr = []\n",
    "hidden_dim_arr = []\n",
    "batch_size_arr = []\n",
    "split_num_arr = []\n",
    "loss_arr = []\n",
    "for i in tqdm(outcome_list):\n",
    "    overall_best_params[i]['exp1'] = {}\n",
    "\n",
    "    IDs = patient_indices_omics['sample_ID'].unique()\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    split_num = 0\n",
    "    for train_index, test_index in kf.split(IDs, RNN_data_outcomes_omics):\n",
    "        split_num += 1\n",
    "        results_dict = {}\n",
    "        train_IDs = IDs[train_index]\n",
    "        test_IDs = IDs[test_index]\n",
    "        \n",
    "        sample_size = int(0.2 * len(train_index))\n",
    "        random_indices = np.random.choice(train_IDs.shape[0], sample_size, replace=False)\n",
    "        val_IDs = train_IDs[random_indices]\n",
    "        train_IDs = np.delete(train_IDs, random_indices)\n",
    "        \n",
    "        input_proteomics = proteomics.merge(patient_indices_omics[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "\n",
    "        train_indices = patient_indices_omics[patient_indices_omics['sample_ID'].isin(train_IDs)]['array_index'].values\n",
    "        np.random.shuffle(train_indices)\n",
    "        test_indices = patient_indices_omics[patient_indices_omics['sample_ID'].isin(test_IDs)]['array_index'].values\n",
    "        val_indices = patient_indices_omics[patient_indices_omics['sample_ID'].isin(val_IDs)]['array_index'].values\n",
    "\n",
    "        # Iterate through the hyperparameter combinations\n",
    "        for param_set in tqdm(all_params):\n",
    "            bs = param_set['batch_size']\n",
    "            lr = param_set['lr']\n",
    "            dropout = param_set['dropout']\n",
    "            lr_decay = param_set['lr_decay']\n",
    "            layers = param_set['layers']\n",
    "            hidden_dim = param_set['hidden_dim']\n",
    "            print(param_set)\n",
    "            val_r, val_loss, val_rmse = run_experiment(RNN_data_codes_omics, input_proteomics,\n",
    "                                                       patient_indices_omics, RNN_data_outcomes_omics,\n",
    "                                                       RNN_data_lengths_omics, 'EHR_omics_only',\n",
    "                                                       lr, lr_decay, bs, \n",
    "                                                       train_indices=train_indices, test_indices=test_indices,\n",
    "                                                       val_indices=val_indices, feature_types='EHR', model_path='',\n",
    "                                                       fine_tune=False, seed=42, hidden_dim=hidden_dim,\n",
    "                                                       num_layers=layers, dropout=dropout, hyperparam_tuning=True)\n",
    "\n",
    "            num_layers_arr.append(layers)\n",
    "            dropout_arr.append(dropout)\n",
    "            lr_arr.append(lr)\n",
    "            lr_decay_arr.append(lr_decay)\n",
    "            hidden_dim_arr.append(hidden_dim)\n",
    "            batch_size_arr.append(bs)\n",
    "            split_num_arr.append(split_num)\n",
    "            loss_arr.append(val_loss)\n",
    "\n",
    "        print('experiment 1')\n",
    "        print('outcome {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparam_df = pd.DataFrame([num_layers_arr, dropout_arr, lr_arr, lr_decay_arr, hidden_dim_arr, batch_size_arr, split_num_arr,loss_arr]).T\n",
    "hyperparam_df.columns = ['num_layers','dropout','lr','lr_decay','hidden_dim','bs','split_num','val_loss']\n",
    "hyperparam_df = hyperparam_df.groupby(['num_layers','dropout','lr','lr_decay','hidden_dim','bs']).mean()\n",
    "num_layers, dropout, lr, lr_decay, hidden_dim, bs = hyperparam_df['val_loss'].idxmin()\n",
    "print(np.min(hyperparam_df['val_loss']))\n",
    "\n",
    "print(num_layers, dropout, lr, lr_decay, hidden_dim, bs)\n",
    "\n",
    "overall_best_params['mortality']['exp1'] = {'num_layers': int(num_layers),'lr': lr,'lr_decay': lr_decay, 'dropout': dropout,'hidden_dim': int(hidden_dim), 'batch_size': int(bs)}\n",
    "with open(\"./best_hyperparams_LARGER.pkl\", \"wb\") as f:\n",
    "        pickle.dump(overall_best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "dx upload best_hyperparams_LARGER.pkl --path /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "overall_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_num_layers_omics = overall_best_params['mortality']['exp1']['num_layers']\n",
    "best_dropout_omics = overall_best_params['mortality']['exp1']['dropout']\n",
    "best_hidden_dim_omics = overall_best_params['mortality']['exp1']['hidden_dim']\n",
    "best_num_layers_omics, best_dropout_omics, best_hidden_dim_omics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid_search = {'batch_size': [16],\n",
    "              'lr': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "              'lr_decay': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "              'prot_hidden_dim': [16,32,64]}\n",
    "\n",
    "all_params = [dict(zip(grid_search.keys(), values)) for values in product(*grid_search.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#run hyperparam loop for EHR baseline\n",
    "num_layers_arr = []\n",
    "dropout_arr = []\n",
    "lr_arr = []\n",
    "lr_decay_arr = []\n",
    "hidden_dim_arr = []\n",
    "batch_size_arr = []\n",
    "split_num_arr = []\n",
    "loss_arr = []\n",
    "phd_arr = []\n",
    "for i in tqdm(outcome_list):\n",
    "    overall_best_params[i]['exp2'] = {}\n",
    "\n",
    "    IDs = patient_indices_omics['sample_ID'].unique()\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    split_num = 0\n",
    "    for train_index, test_index in kf.split(IDs, RNN_data_outcomes_omics):\n",
    "        split_num += 1\n",
    "        results_dict = {}\n",
    "        train_IDs = IDs[train_index]\n",
    "        test_IDs = IDs[test_index]\n",
    "        \n",
    "        sample_size = int(0.2 * len(train_index))\n",
    "        random_indices = np.random.choice(train_IDs.shape[0], sample_size, replace=False)\n",
    "        val_IDs = train_IDs[random_indices]\n",
    "        train_IDs = np.delete(train_IDs, random_indices)\n",
    "        \n",
    "        input_proteomics = proteomics.merge(patient_indices_omics[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "\n",
    "        train_indices = patient_indices_omics[patient_indices_omics['sample_ID'].isin(train_IDs)]['array_index'].values\n",
    "        np.random.shuffle(train_indices)\n",
    "        test_indices = patient_indices_omics[patient_indices_omics['sample_ID'].isin(test_IDs)]['array_index'].values\n",
    "        val_indices = patient_indices_omics[patient_indices_omics['sample_ID'].isin(val_IDs)]['array_index'].values\n",
    "\n",
    "        # Iterate through the hyperparameter combinations\n",
    "        for param_set in tqdm(all_params):\n",
    "            bs = param_set['batch_size']\n",
    "            lr = param_set['lr']\n",
    "            dropout = best_dropout_omics\n",
    "            lr_decay = param_set['lr_decay']\n",
    "            prot_hidden_dim = param_set['prot_hidden_dim']\n",
    "            layers = best_num_layers_omics\n",
    "            hidden_dim = best_hidden_dim_omics\n",
    "            print(param_set)\n",
    "            val_r, val_loss, val_rmse = run_experiment(RNN_data_codes_omics, input_proteomics,\n",
    "                patient_indices_omics, RNN_data_outcomes_omics, RNN_data_lengths_omics, 'proteomics_omics_only', \n",
    "                lr, lr_decay, bs, prot_hidden_dim, train_indices=train_indices, test_indices=test_indices, val_indices=val_indices,\n",
    "                feature_types='metab', model_path='', fine_tune=False, seed=42, hidden_dim=hidden_dim,\n",
    "                num_layers=layers, dropout=dropout, hyperparam_tuning=True)\n",
    "            \n",
    "            num_layers_arr.append(layers)\n",
    "            dropout_arr.append(dropout)\n",
    "            lr_arr.append(lr)\n",
    "            lr_decay_arr.append(lr_decay)\n",
    "            hidden_dim_arr.append(hidden_dim)\n",
    "            batch_size_arr.append(bs)\n",
    "            split_num_arr.append(split_num)\n",
    "            loss_arr.append(val_loss)\n",
    "            phd_arr.append(prot_hidden_dim)\n",
    "\n",
    "        print('experiment 2')\n",
    "        print('outcome {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparam_df = pd.DataFrame([num_layers_arr, dropout_arr, lr_arr, lr_decay_arr, hidden_dim_arr, batch_size_arr, phd_arr, split_num_arr,loss_arr]).T\n",
    "hyperparam_df.columns = ['num_layers','dropout','lr','lr_decay','hidden_dim','bs','phd','split_num','val_loss']\n",
    "hyperparam_df = hyperparam_df.groupby(['num_layers','dropout','lr','lr_decay','hidden_dim','bs','phd']).mean()\n",
    "num_layers, dropout, lr, lr_decay, hidden_dim, bs, phd = hyperparam_df['val_loss'].idxmin()\n",
    "print(np.min(hyperparam_df['val_loss']))\n",
    "print(num_layers, dropout, lr, lr_decay, hidden_dim, bs, phd)\n",
    "\n",
    "overall_best_params['mortality']['exp2'] = {'num_layers': int(num_layers),'lr': lr,'lr_decay': lr_decay, 'dropout': dropout,\n",
    "                                'hidden_dim': int(hidden_dim), 'batch_size': int(bs), 'prot_hidden_dim':int(phd)}\n",
    "with open(\"./best_hyperparams_LARGER.pkl\", \"wb\") as f:\n",
    "        pickle.dump(overall_best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_protein_hidden = overall_best_params['mortality']['exp2']['prot_hidden_dim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "dx upload best_hyperparams_LARGER.pkl --path /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid_search = {'batch_size': [16],\n",
    "              'lr': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "              'dropout': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "              'lr_decay': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "              'prot_hidden_dim': [best_protein_hidden]}\n",
    "\n",
    "all_params = [dict(zip(grid_search.keys(), values)) for values in product(*grid_search.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#run hyperparam loop for joint data baseline\n",
    "num_layers_arr = []\n",
    "dropout_arr = []\n",
    "lr_arr = []\n",
    "lr_decay_arr = []\n",
    "hidden_dim_arr = []\n",
    "batch_size_arr = []\n",
    "split_num_arr = []\n",
    "loss_arr = []\n",
    "phd_arr = []\n",
    "for i in tqdm(outcome_list):\n",
    "    overall_best_params[i]['exp3'] = {}\n",
    "\n",
    "    IDs = patient_indices_omics['sample_ID'].unique()\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    split_num = 0\n",
    "    for train_index, test_index in kf.split(IDs, RNN_data_outcomes_omics):\n",
    "        split_num += 1\n",
    "        results_dict = {}\n",
    "        train_IDs = IDs[train_index]\n",
    "        test_IDs = IDs[test_index]\n",
    "        \n",
    "        sample_size = int(0.2 * len(train_index))\n",
    "        random_indices = np.random.choice(train_IDs.shape[0], sample_size, replace=False)\n",
    "        val_IDs = train_IDs[random_indices]\n",
    "        train_IDs = np.delete(train_IDs, random_indices)\n",
    "        \n",
    "        input_proteomics = proteomics.merge(patient_indices_omics[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "\n",
    "        train_indices = patient_indices_omics[patient_indices_omics['sample_ID'].isin(train_IDs)]['array_index'].values\n",
    "        np.random.shuffle(train_indices)\n",
    "        test_indices = patient_indices_omics[patient_indices_omics['sample_ID'].isin(test_IDs)]['array_index'].values\n",
    "        val_indices = patient_indices_omics[patient_indices_omics['sample_ID'].isin(val_IDs)]['array_index'].values\n",
    "\n",
    "        # Iterate through the hyperparameter combinations\n",
    "        for param_set in tqdm(all_params):\n",
    "            bs = param_set['batch_size']\n",
    "            lr = param_set['lr']\n",
    "            dropout = param_set['dropout']\n",
    "            lr_decay = param_set['lr_decay']\n",
    "            prot_hidden_dim = param_set['prot_hidden_dim']\n",
    "            layers = best_num_layers_omics\n",
    "            hidden_dim = best_hidden_dim_omics\n",
    "            print(param_set)\n",
    "            \n",
    "            val_r, val_loss, val_rmse = run_experiment(RNN_data_codes_omics, input_proteomics,\n",
    "                patient_indices_omics, RNN_data_outcomes_omics, RNN_data_lengths_omics, 'both_omics_only', \n",
    "                lr, lr_decay, bs, prot_hidden_dim, train_indices=train_indices, test_indices=test_indices,\n",
    "                val_indices=val_indices, feature_types='both', model_path='', fine_tune=False, seed=42,\n",
    "                hidden_dim=hidden_dim, num_layers=layers, dropout=dropout, hyperparam_tuning=True)\n",
    "\n",
    "            num_layers_arr.append(layers)\n",
    "            dropout_arr.append(dropout)\n",
    "            lr_arr.append(lr)\n",
    "            lr_decay_arr.append(lr_decay)\n",
    "            hidden_dim_arr.append(hidden_dim)\n",
    "            batch_size_arr.append(bs)\n",
    "            split_num_arr.append(split_num)\n",
    "            loss_arr.append(val_loss)\n",
    "            phd_arr.append(prot_hidden_dim)\n",
    "\n",
    "        print('experiment 3')\n",
    "        print('outcome {}'.format(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparam_df = pd.DataFrame([num_layers_arr, dropout_arr, lr_arr, lr_decay_arr, hidden_dim_arr, batch_size_arr, phd_arr, split_num_arr,loss_arr]).T\n",
    "hyperparam_df.columns = ['num_layers','dropout','lr','lr_decay','hidden_dim','bs','phd','split_num','val_loss']\n",
    "hyperparam_df = hyperparam_df.groupby(['num_layers','dropout','lr','lr_decay','hidden_dim','bs','phd']).mean()\n",
    "num_layers, dropout, lr, lr_decay, hidden_dim, bs, phd = hyperparam_df['val_loss'].idxmin()\n",
    "print(np.min(hyperparam_df['val_loss']))\n",
    "print(num_layers, dropout, lr, lr_decay, hidden_dim, bs, phd)\n",
    "\n",
    "overall_best_params['mortality']['exp3'] = {'num_layers': int(num_layers),'lr': lr,'lr_decay': lr_decay, 'dropout': dropout,\n",
    "                                'hidden_dim': int(hidden_dim), 'batch_size': int(bs), 'prot_hidden_dim':int(phd)}\n",
    "with open(\"./best_hyperparams_LARGER.pkl\", \"wb\") as f:\n",
    "        pickle.dump(overall_best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test PT model on omics population, hyperparams are saved to avoid downstream issues but there is no training done here so hyperparams don't matter\n",
    "for i in tqdm(outcome_list):\n",
    "    overall_best_params[i]['exp4'] = {}\n",
    "    results_dict = {}\n",
    "\n",
    "    # choose large enough bs to do inference in one batch\n",
    "    bs = 1000\n",
    "    lr = 0.1\n",
    "    dropout = best_dropout\n",
    "    lr_decay = 0.1\n",
    "    layers = best_num_layers\n",
    "    hidden_dim = best_hidden_dim\n",
    "    # print(param_set)\n",
    "    val_r, val_loss, val_rmse = run_experiment(RNN_data_codes, proteomics,\n",
    "            patient_indices, RNN_data_outcomes, RNN_data_lengths, 'EHR_omics_PT', \n",
    "            lr, lr_decay, bs, feature_types='EHR', model_path='./{}.pth'.format(best_model_name),\n",
    "                                               fine_tune=False, seed=42,\n",
    "                                               hidden_dim=hidden_dim,\n",
    "                                              num_layers=layers, dropout=dropout, hyperparam_tuning=False)\n",
    "\n",
    "    results_dict[val_loss] = {'num_layers': layers,'lr': lr,'lr_decay': lr_decay, 'dropout': dropout,\n",
    "                            'hidden_dim': hidden_dim, 'batch_size': bs}\n",
    "\n",
    "    print('experiment 4')\n",
    "    print('outcome {}'.format(i))\n",
    "    overall_best_params[i]['exp4'] = results_dict[min(results_dict.keys())]\n",
    "    print(results_dict[min(results_dict.keys())])\n",
    "\n",
    "# Save the best hyperparameters to a pickle file\n",
    "with open(\"./best_hyperparams_LARGER.pkl\", \"wb\") as f:\n",
    "    pickle.dump(overall_best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "dx upload best_hyperparams_LARGER.pkl --path /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid_search = {'batch_size': [16],\n",
    "              'lr': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "               'dropout': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "              'lr_decay': [1e-1, 1e-2, 1e-3, 1e-4]}\n",
    "\n",
    "all_params = [dict(zip(grid_search.keys(), values)) for values in product(*grid_search.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#run hyperparam loop for fine tuning pre-trained EHR model\n",
    "\n",
    "num_layers_arr = []\n",
    "dropout_arr = []\n",
    "lr_arr = []\n",
    "lr_decay_arr = []\n",
    "hidden_dim_arr = []\n",
    "batch_size_arr = []\n",
    "split_num_arr = []\n",
    "loss_arr = []\n",
    "for i in tqdm(outcome_list):\n",
    "    overall_best_params[i]['exp5'] = {}\n",
    "\n",
    "    IDs = patient_indices['sample_ID'].unique()\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    split_num = 0\n",
    "    for train_index, test_index in kf.split(IDs, RNN_data_outcomes):\n",
    "        split_num += 1\n",
    "        results_dict = {}\n",
    "        train_IDs = IDs[train_index]\n",
    "        test_IDs = IDs[test_index]\n",
    "        \n",
    "        sample_size = int(0.2 * len(train_index))\n",
    "        random_indices = np.random.choice(train_IDs.shape[0], sample_size, replace=False)\n",
    "        val_IDs = train_IDs[random_indices]\n",
    "        train_IDs = np.delete(train_IDs, random_indices)\n",
    "        \n",
    "        input_proteomics = proteomics.merge(patient_indices[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "        train_indices = patient_indices[patient_indices['sample_ID'].isin(train_IDs)]['array_index'].values\n",
    "        np.random.shuffle(train_indices)\n",
    "        test_indices = patient_indices[patient_indices['sample_ID'].isin(test_IDs)]['array_index'].values\n",
    "        val_indices = patient_indices[patient_indices['sample_ID'].isin(val_IDs)]['array_index'].values\n",
    "\n",
    "        # Iterate through the hyperparameter combinations\n",
    "        for param_set in tqdm(all_params):\n",
    "            bs = param_set['batch_size']\n",
    "            lr = param_set['lr']\n",
    "            dropout = best_dropout\n",
    "            lr_decay = param_set['lr_decay']\n",
    "            layers = best_num_layers\n",
    "            hidden_dim = best_hidden_dim\n",
    "            print(param_set)\n",
    "            \n",
    "            val_r, val_loss, val_rmse = run_experiment(RNN_data_codes, input_proteomics,\n",
    "                patient_indices, RNN_data_outcomes, RNN_data_lengths, 'EHR_omics_PT_FT',\n",
    "                lr, lr_decay, bs, train_indices=train_indices, test_indices=test_indices,\n",
    "                val_indices=val_indices, feature_types='EHR',\n",
    "                model_path='./{}.pth'.format(best_model_name),\n",
    "                fine_tune=True, seed=42, hidden_dim=hidden_dim, num_layers=layers, dropout=dropout,\n",
    "                                                       hyperparam_tuning=True)\n",
    "            \n",
    "            num_layers_arr.append(layers)\n",
    "            dropout_arr.append(dropout)\n",
    "            lr_arr.append(lr)\n",
    "            lr_decay_arr.append(lr_decay)\n",
    "            hidden_dim_arr.append(hidden_dim)\n",
    "            batch_size_arr.append(bs)\n",
    "            split_num_arr.append(split_num)\n",
    "            loss_arr.append(val_loss)\n",
    "\n",
    "        print('experiment 5')\n",
    "        print('outcome {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparam_df = pd.DataFrame([num_layers_arr, dropout_arr, lr_arr, lr_decay_arr, hidden_dim_arr, batch_size_arr, split_num_arr,loss_arr]).T\n",
    "hyperparam_df.columns = ['num_layers','dropout','lr','lr_decay','hidden_dim','bs','split_num','val_loss']\n",
    "hyperparam_df = hyperparam_df.groupby(['num_layers','dropout','lr','lr_decay','hidden_dim','bs']).mean()\n",
    "num_layers, dropout, lr, lr_decay, hidden_dim, bs = hyperparam_df['val_loss'].idxmin()\n",
    "print(np.min(hyperparam_df['val_loss']))\n",
    "print(num_layers, dropout, lr, lr_decay, hidden_dim, bs)\n",
    "\n",
    "overall_best_params['mortality']['exp5'] = {'num_layers': int(num_layers),'lr': lr,'lr_decay': lr_decay, 'dropout': dropout,\n",
    "                                'hidden_dim': int(hidden_dim), 'batch_size': int(bs)}\n",
    "with open(\"./best_hyperparams_LARGER.pkl\", \"wb\") as f:\n",
    "        pickle.dump(overall_best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "overall_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "dx upload best_hyperparams_LARGER.pkl --path /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid_search = {'batch_size': [16],\n",
    "              'lr': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "              'dropout': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "              'lr_decay': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "              'prot_hidden_dim':[best_protein_hidden]}\n",
    "\n",
    "all_params = [dict(zip(grid_search.keys(), values)) for values in product(*grid_search.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#run hyperparam loop for COMET models\n",
    "\n",
    "num_layers_arr = []\n",
    "dropout_arr = []\n",
    "lr_arr = []\n",
    "lr_decay_arr = []\n",
    "hidden_dim_arr = []\n",
    "batch_size_arr = []\n",
    "split_num_arr = []\n",
    "loss_arr = []\n",
    "phd_arr = []\n",
    "for i in tqdm(outcome_list):\n",
    "    overall_best_params[i]['exp6'] = {}\n",
    "\n",
    "    IDs = patient_indices['sample_ID'].unique()\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    split_num = 0\n",
    "    for train_index, test_index in kf.split(IDs, RNN_data_outcomes):\n",
    "        split_num += 1\n",
    "        results_dict = {}\n",
    "        train_IDs = IDs[train_index]\n",
    "        test_IDs = IDs[test_index]\n",
    "        \n",
    "        sample_size = int(0.2 * len(train_index))\n",
    "        random_indices = np.random.choice(train_IDs.shape[0], sample_size, replace=False)\n",
    "        val_IDs = train_IDs[random_indices]\n",
    "        train_IDs = np.delete(train_IDs, random_indices)\n",
    "        \n",
    "        input_proteomics = proteomics.merge(patient_indices[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "\n",
    "        train_indices = patient_indices[patient_indices['sample_ID'].isin(train_IDs)]['array_index'].values\n",
    "        np.random.shuffle(train_indices)\n",
    "        test_indices = patient_indices[patient_indices['sample_ID'].isin(test_IDs)]['array_index'].values\n",
    "        val_indices = patient_indices[patient_indices['sample_ID'].isin(val_IDs)]['array_index'].values\n",
    "\n",
    "        # Iterate through the hyperparameter combinations\n",
    "        for param_set in tqdm(all_params):\n",
    "            bs = param_set['batch_size']\n",
    "            lr = param_set['lr']\n",
    "            dropout = best_dropout\n",
    "            lr_decay = param_set['lr_decay']\n",
    "            prot_hidden_dim = param_set['prot_hidden_dim']\n",
    "            layers = best_num_layers\n",
    "            hidden_dim = best_hidden_dim\n",
    "            print(param_set)\n",
    "            val_r, val_loss, val_rmse = run_experiment(RNN_data_codes, input_proteomics,\n",
    "                patient_indices, RNN_data_outcomes, RNN_data_lengths, 'both_PT', \n",
    "                lr, lr_decay, bs, prot_hidden_dim, train_indices=train_indices, test_indices=test_indices,\n",
    "                val_indices=val_indices,feature_types='both',\n",
    "                model_path='/mnt/project/cancer_mortality_3yr_LARGER_models_results/{}.pth'.format(best_model_name),\n",
    "                fine_tune=True, seed=42, hidden_dim=hidden_dim, num_layers=layers, dropout=dropout,\n",
    "                                                        hyperparam_tuning=True)\n",
    "            \n",
    "            num_layers_arr.append(layers)\n",
    "            dropout_arr.append(dropout)\n",
    "            lr_arr.append(lr)\n",
    "            lr_decay_arr.append(lr_decay)\n",
    "            hidden_dim_arr.append(hidden_dim)\n",
    "            batch_size_arr.append(bs)\n",
    "            split_num_arr.append(split_num)\n",
    "            loss_arr.append(val_loss)\n",
    "            phd_arr.append(prot_hidden_dim)\n",
    "\n",
    "        print('experiment 6')\n",
    "        print('outcome {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparam_df = pd.DataFrame([num_layers_arr, dropout_arr, lr_arr, lr_decay_arr, hidden_dim_arr, batch_size_arr, phd_arr, split_num_arr,loss_arr]).T\n",
    "hyperparam_df.columns = ['num_layers','dropout','lr','lr_decay','hidden_dim','bs','phd','split_num','val_loss']\n",
    "hyperparam_df = hyperparam_df.groupby(['num_layers','dropout','lr','lr_decay','hidden_dim','bs','phd']).mean()\n",
    "num_layers, dropout, lr, lr_decay, hidden_dim, bs, phd = hyperparam_df['val_loss'].idxmin()\n",
    "#num_layers, dropout, lr, lr_decay, hidden_dim, bs, phd = hyperparam_df.sort_values('val_loss').index[3]\n",
    "print(np.min(hyperparam_df['val_loss']))\n",
    "print(num_layers, dropout, lr, lr_decay, hidden_dim, bs, phd)\n",
    "\n",
    "overall_best_params['mortality']['exp6'] = {'num_layers': int(num_layers),'lr': lr,'lr_decay': lr_decay, 'dropout': dropout,\n",
    "                                'hidden_dim': int(hidden_dim), 'batch_size': int(bs),'prot_hidden_dim':int(phd)}\n",
    "with open(\"./best_hyperparams_LARGER.pkl\", \"wb\") as f:\n",
    "        pickle.dump(overall_best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "overall_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "dx upload best_hyperparams_LARGER.pkl --path /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "dx upload ./PT_MODEL_2.0_0.001_0.001_0.1_400.0.pth --path /\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run cell below if skipping modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load existing modeling results if we're not doing modeling and only want to do downstream analysis\n",
    "results = pickle.load(open('/mnt/project/cancer_mortality_3yr_LARGER_models_results/results_3yr_mortality_LARGER.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#experiment 1 = baseline model EHR features\n",
    "#experiment 2 = baseline model metab features\n",
    "#experiment 3 = baseline model all features\n",
    "#experiment 4 = only pretrained model\n",
    "#experiment 5 = fine tune pretrained model \n",
    "#experiment 6 = full COMET\n",
    "\n",
    "num_iterations = 25\n",
    "results = {}\n",
    "for i in tqdm(outcome_list):\n",
    "    results[i] = {'exp1':[],'exp2':[],'exp3':[],'exp4':[],'exp5':[],'exp6':[]}\n",
    "    for j in tqdm(range(num_iterations)):\n",
    "        print('experiment 1')\n",
    "        val_auc = run_experiment(RNN_data_codes_omics, proteomics,\n",
    "                patient_indices_omics, RNN_data_outcomes_omics, RNN_data_lengths_omics, 'EHR_omics_only_{}'.format(j), \n",
    "                overall_best_params[i]['exp1']['lr'], overall_best_params[i]['exp1']['lr_decay'],\n",
    "                overall_best_params[i]['exp1']['batch_size'], feature_types='EHR', model_path='', fine_tune=False, seed=j,\n",
    "                                hidden_dim=overall_best_params[i]['exp1']['hidden_dim'],\n",
    "                                 num_layers=overall_best_params[i]['exp1']['num_layers'],\n",
    "                                 dropout=overall_best_params[i]['exp1']['dropout'], return_preds=True)\n",
    "        results[i]['exp1'].append(val_auc)\n",
    "        \n",
    "        print('experiment 2')\n",
    "        val_auc = run_experiment(RNN_data_codes_omics, proteomics,\n",
    "                patient_indices_omics, RNN_data_outcomes_omics, RNN_data_lengths_omics, 'proteomics_omics_only_{}'.format(j), \n",
    "                overall_best_params[i]['exp2']['lr'], overall_best_params[i]['exp2']['lr_decay'],\n",
    "                overall_best_params[i]['exp2']['batch_size'], overall_best_params[i]['exp2']['prot_hidden_dim'],\n",
    "                                 feature_types='metab', model_path='', fine_tune=False, seed=j,\n",
    "                                hidden_dim=best_hidden_dim,num_layers=best_num_layers, dropout=best_dropout, return_preds=True)\n",
    "        results[i]['exp2'].append(val_auc)\n",
    "        \n",
    "        print('experiment 3')\n",
    "        val_auc = run_experiment(RNN_data_codes_omics, proteomics,\n",
    "                patient_indices_omics, RNN_data_outcomes_omics, RNN_data_lengths_omics, 'both_omics_only_{}'.format(j), \n",
    "                overall_best_params[i]['exp3']['lr'], overall_best_params[i]['exp3']['lr_decay'],\n",
    "                overall_best_params[i]['exp3']['batch_size'], overall_best_params[i]['exp3']['prot_hidden_dim'],\n",
    "                                 feature_types='both', model_path='', fine_tune=False, seed=j,\n",
    "                                hidden_dim=overall_best_params[i]['exp3']['hidden_dim'],\n",
    "                                 num_layers=overall_best_params[i]['exp3']['num_layers'],\n",
    "                                 dropout=overall_best_params[i]['exp3']['dropout'],\n",
    "                                 return_preds=True, return_interpretability=True, return_grads=True)\n",
    "        results[i]['exp3'].append(val_auc)\n",
    "        \n",
    "        print('experiment 4')\n",
    "        val_auc = run_experiment(RNN_data_codes, proteomics,\n",
    "                patient_indices, RNN_data_outcomes, RNN_data_lengths, 'EHR_PT_{}'.format(j), \n",
    "                overall_best_params[i]['exp4']['lr'], overall_best_params[i]['exp4']['lr_decay'],\n",
    "                overall_best_params[i]['exp4']['batch_size'], feature_types='EHR',\n",
    "                                 model_path='/mnt/project/cancer_mortality_3yr_LARGER_models_results/{}.pth'.format(best_model_name),\n",
    "                                 fine_tune=False, seed=j,\n",
    "                                hidden_dim=best_hidden_dim,num_layers=best_num_layers,\n",
    "                                 dropout=best_dropout, return_preds=True)\n",
    "        results[i]['exp4'].append(val_auc)\n",
    "\n",
    "        print('experiment 5')\n",
    "        val_auc = run_experiment(RNN_data_codes, proteomics,\n",
    "                patient_indices, RNN_data_outcomes, RNN_data_lengths, 'EHR_PT_FT_{}'.format(j), \n",
    "                overall_best_params[i]['exp5']['lr'], overall_best_params[i]['exp5']['lr_decay'],\n",
    "                overall_best_params[i]['exp5']['batch_size'], feature_types='EHR',\n",
    "                                 model_path='/mnt/project/cancer_mortality_3yr_LARGER_models_results/{}.pth'.format(best_model_name),\n",
    "                                 fine_tune=True, seed=j,\n",
    "                                hidden_dim=best_hidden_dim,num_layers=best_num_layers,\n",
    "                                 dropout=best_dropout, return_preds=True)\n",
    "        results[i]['exp5'].append(val_auc)\n",
    "\n",
    "        print('experiment 6')\n",
    "        val_auc = run_experiment(RNN_data_codes, proteomics,\n",
    "                patient_indices, RNN_data_outcomes, RNN_data_lengths, 'both_PT_FT_{}'.format(j), \n",
    "                overall_best_params[i]['exp6']['lr'], overall_best_params[i]['exp6']['lr_decay'],\n",
    "                overall_best_params[i]['exp6']['batch_size'], overall_best_params[i]['exp6']['prot_hidden_dim'],\n",
    "                                 feature_types='both',\n",
    "                                model_path='/mnt/project/cancer_mortality_3yr_LARGER_models_results/{}.pth'.format(best_model_name),\n",
    "                                 fine_tune=True, seed=j,\n",
    "                                 hidden_dim=best_hidden_dim,num_layers=best_num_layers,\n",
    "                                 dropout=best_dropout,\n",
    "                                 return_preds=True, return_interpretability=True, return_grads=True)\n",
    "        results[i]['exp6'].append(val_auc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute performance metrics from validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for exp in results['mortality'].keys():\n",
    "    true_outcomes = []\n",
    "    total_preds = []\n",
    "    indices = []\n",
    "    for i in results['mortality'][exp]:\n",
    "        true_outcomes.extend(i[3])\n",
    "        \n",
    "        total_preds.extend(i[4])\n",
    "        indices.extend(i[5])\n",
    "    \n",
    "        df = pd.DataFrame([true_outcomes,total_preds,indices]).T\n",
    "        df.columns = ['true_outcome','pred','index']\n",
    "        df = df.groupby('index').mean()\n",
    "    print(exp)\n",
    "    print('AUC: {}'.format(roc_auc_score(df['true_outcome'], df['pred'])))\n",
    "    print('AUPRC: {}'.format(average_precision_score(df['true_outcome'], df['pred'])))\n",
    "    epsilon = 1e-15\n",
    "    df['pred'] = np.clip(df['pred'], epsilon, 1 - epsilon)\n",
    "\n",
    "    # Calculating the BCELoss\n",
    "    bce_loss = -np.mean(df['true_outcome'] * np.log(df['pred']) + (1 - df['true_outcome']) * np.log(1 - df['pred']))\n",
    "\n",
    "    print(bce_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Specify the directory containing the files you want to upload\n",
    "directory_path = './'\n",
    "\n",
    "# Specify the destination path on DNA Nexus\n",
    "destination_path = './cancer_mortality_3yr_LARGER_models_results/'\n",
    "\n",
    "# List all files in the directory\n",
    "files = [f for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "progress_bar = tqdm(files, desc=\"Uploading files\")\n",
    "\n",
    "for file in progress_bar:\n",
    "    # Construct the bash command for the current file\n",
    "    command = f'dx upload \"{os.path.join(directory_path, file)}\" --path {destination_path}'\n",
    "    \n",
    "    # Update tqdm progress bar description\n",
    "    progress_bar.set_description(f\"Uploading {file}\")\n",
    "    \n",
    "    # Execute the bash command (ignore the output)\n",
    "    subprocess.run(command, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "print(\"All files have been uploaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"./results_3yr_mortality_LARGER.pkl\", \"wb\") as f:\n",
    "        pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "dx upload ./results_3yr_mortality_LARGER.pkl --path ./cancer_mortality_3yr_LARGER_models_results/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "dx upload ./best_hyperparams_LARGER.pkl --path ./cancer_mortality_3yr_LARGER_models_results/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downstream Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance: Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from captum.attr import DeepLift\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "#5 for val data, 13 for train data\n",
    "imp_index = 5\n",
    "all_data = True\n",
    "\n",
    "\n",
    "# Path to models\n",
    "model_paths = [f'/mnt/project/cancer_mortality_3yr_LARGER_models_results/both_PT_FT_{i}.pth' for i in range(25)]\n",
    "\n",
    "# Initialize array to store feature importance\n",
    "importance_proteomics_all = torch.zeros((len(model_paths), proteomics.shape[1]-1))\n",
    "                                        \n",
    "class ModelWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, proteomics, codes, lengths):\n",
    "        return self.model(codes, proteomics, lengths)\n",
    "# Iterate over all models\n",
    "# mean_proteomics_baseline = torch.mean(inputs_proteomics, dim=0, keepdim=True).to(device)\n",
    "\n",
    "input_proteomics = proteomics.merge(patient_indices[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "for model_idx, model_path in tqdm(enumerate(model_paths),  total=len(model_paths)):\n",
    "    \n",
    "    \n",
    "    model = joint_model(RNN_data_codes.shape[2], best_hidden_dim, [best_hidden_dim], best_num_layers, 1,\n",
    "                    input_proteomics.shape[1], [best_protein_hidden], [], best_dropout).to(device)\n",
    "    \n",
    "\n",
    "    if all_data == False:\n",
    "        val_proteomics = input_proteomics[results['mortality']['exp6'][model_idx][imp_index],:]\n",
    "    else:\n",
    "        val_proteomics = input_proteomics\n",
    "    #This has to be done within the val set!\n",
    "    scaler = StandardScaler()\n",
    "    val_proteomics = scaler.fit_transform(val_proteomics)\n",
    "\n",
    "    if all_data == False:\n",
    "        EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes[results['mortality']['exp6'][model_idx][imp_index],:,:]]  \n",
    "        EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "        val_proteomics = torch.tensor(val_proteomics).float()\n",
    "        val_proteomics = torch.nan_to_num(val_proteomics)\n",
    "        val_outcomes = torch.tensor(RNN_data_outcomes[results['mortality']['exp6'][model_idx][imp_index]]).float()\n",
    "\n",
    "        outcome_mean = torch.mean(val_outcomes)\n",
    "        outcome_sd = torch.std(val_outcomes)\n",
    "\n",
    "        data_set = DataBuilder(val_proteomics, val_outcomes, scaler)\n",
    "        loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "        loader_codes = create_dataloaders(EHR_codes, val_outcomes, RNN_data_lengths[results['mortality']['exp6'][model_idx][imp_index]], 100000)\n",
    "    else:\n",
    "        EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes]  \n",
    "        EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "        val_proteomics = torch.tensor(val_proteomics).float()\n",
    "        val_proteomics = torch.nan_to_num(val_proteomics)\n",
    "        val_outcomes = torch.tensor(RNN_data_outcomes).float()\n",
    "\n",
    "        outcome_mean = torch.mean(val_outcomes)\n",
    "        outcome_sd = torch.std(val_outcomes)\n",
    "\n",
    "        data_set = DataBuilder(val_proteomics, val_outcomes, scaler)\n",
    "        loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "        loader_codes = create_dataloaders(EHR_codes, val_outcomes, RNN_data_lengths, 100000)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Load the model\n",
    "    model_state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    model_wrapper = ModelWrapper(model)\n",
    "    ig = IntegratedGradients(model_wrapper)\n",
    "    # Compute the feature importance for this model\n",
    "    for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in zip(loader_codes, loader_proteomics):\n",
    "        inputs_codes, labels_codes = inputs_codes.to(device), labels_codes.to(device)\n",
    "        inputs_proteomics, labels_proteomics = inputs_proteomics.to(device), labels_proteomics.to(device)\n",
    "        \n",
    "        def forward_func(proteomics, codes, lengths):\n",
    "            return model(codes, proteomics, lengths)\n",
    "        \n",
    "        # Compute feature importances using a custom forward function; CONSIDER THE BASELINE\n",
    "        importance_proteomics = ig.attribute(inputs_proteomics, additional_forward_args=(inputs_codes, lengths_codes))\n",
    "        # Store the feature importance for this model\n",
    "        importance_proteomics_all[model_idx] = importance_proteomics.mean(dim=0).cpu().detach()\n",
    "    \n",
    "# Compute the average feature importance across all models\n",
    "importance_proteomics_avg = torch.mean((importance_proteomics_all), dim=0)*1e11\n",
    "\n",
    "# Create dataframe for feature importances\n",
    "importance_df = pd.DataFrame([importance_proteomics_avg.numpy(), proteomics.drop(['sample_ID'],axis=1).columns]).T\n",
    "importance_df.columns = ['importance_PT','name']\n",
    "importance_df.sort_values('importance_PT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "importance_df['abs_PT'] = np.abs(importance_df['importance_PT'])\n",
    "importance_df.sort_values('abs_PT', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "#5 for val data, 13 for train data\n",
    "imp_index = 5\n",
    "all_data = True\n",
    "\n",
    "\n",
    "# Path to models\n",
    "model_paths = [f'/mnt/project/cancer_mortality_3yr_LARGER_models_results/both_omics_only_{i}.pth' for i in range(25)]\n",
    "\n",
    "# Initialize array to store feature importance\n",
    "importance_proteomics_all = torch.zeros((len(model_paths), proteomics.shape[1]-1))\n",
    "                                        \n",
    "class ModelWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, proteomics, codes, lengths):\n",
    "        return self.model(codes, proteomics, lengths)\n",
    "# Iterate over all models\n",
    "# mean_proteomics_baseline = torch.mean(inputs_proteomics, dim=0, keepdim=True).to(device)\n",
    "\n",
    "input_proteomics = proteomics.merge(patient_indices_omics[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "for model_idx, model_path in tqdm(enumerate(model_paths), total=len(model_paths)):\n",
    "    \n",
    "    \n",
    "    model = joint_model(RNN_data_codes_omics.shape[2], best_hidden_dim_omics, [best_hidden_dim_omics], best_num_layers_omics, 1,\n",
    "                    input_proteomics.shape[1], [best_protein_hidden], [], best_dropout_omics).to(device)\n",
    "    \n",
    "\n",
    "    if all_data == False:\n",
    "        val_proteomics = input_proteomics[results['mortality']['exp3'][model_idx][imp_index],:]\n",
    "    else:\n",
    "        val_proteomics = input_proteomics\n",
    "    #This has to be done within the val set!\n",
    "    scaler = StandardScaler()\n",
    "    val_proteomics = scaler.fit_transform(val_proteomics)\n",
    "\n",
    "    if all_data == False:\n",
    "        EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes_omics[results['mortality']['exp3'][model_idx][imp_index],:,:]]  \n",
    "        EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "        val_proteomics = torch.tensor(val_proteomics).float()\n",
    "        val_proteomics = torch.nan_to_num(val_proteomics)\n",
    "        val_outcomes = torch.tensor(RNN_data_outcomes_omics[results['mortality']['exp3'][model_idx][imp_index]]).float()\n",
    "\n",
    "        outcome_mean = torch.mean(val_outcomes)\n",
    "        outcome_sd = torch.std(val_outcomes)\n",
    "\n",
    "        data_set = DataBuilder(val_proteomics, val_outcomes, scaler)\n",
    "        loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "        loader_codes = create_dataloaders(EHR_codes, val_outcomes, RNN_data_lengths_omics[results['mortality']['exp3'][model_idx][imp_index]], 100000)\n",
    "    else:\n",
    "        EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes]  \n",
    "        EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "        val_proteomics = torch.tensor(val_proteomics).float()\n",
    "        val_proteomics = torch.nan_to_num(val_proteomics)\n",
    "        val_outcomes = torch.tensor(RNN_data_outcomes).float()\n",
    "\n",
    "        outcome_mean = torch.mean(val_outcomes)\n",
    "        outcome_sd = torch.std(val_outcomes)\n",
    "\n",
    "        data_set = DataBuilder(val_proteomics, val_outcomes, scaler)\n",
    "        loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "        loader_codes = create_dataloaders(EHR_codes, val_outcomes, RNN_data_lengths, 100000)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Load the model\n",
    "    model_state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    model_wrapper = ModelWrapper(model)\n",
    "    ig = IntegratedGradients(model_wrapper)\n",
    "    # Compute the feature importance for this model\n",
    "    for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in zip(loader_codes, loader_proteomics):\n",
    "        inputs_codes, labels_codes = inputs_codes.to(device), labels_codes.to(device)\n",
    "        inputs_proteomics, labels_proteomics = inputs_proteomics.to(device), labels_proteomics.to(device)\n",
    "        \n",
    "        def forward_func(proteomics, codes, lengths):\n",
    "            return model(codes, proteomics, lengths)\n",
    "        \n",
    "        # Compute feature importances using a custom forward function; CONSIDER THE BASELINE\n",
    "        importance_proteomics = ig.attribute(inputs_proteomics, additional_forward_args=(inputs_codes, lengths_codes))\n",
    "        # Store the feature importance for this model\n",
    "        importance_proteomics_all[model_idx] = importance_proteomics.mean(dim=0).cpu().detach()\n",
    "    \n",
    "# Compute the average feature importance across all models\n",
    "importance_proteomics_avg = torch.mean((importance_proteomics_all), dim=0)*1e11\n",
    "\n",
    "# Create dataframe for feature importances\n",
    "importance_df_omics = pd.DataFrame([importance_proteomics_avg.numpy(), proteomics.drop(['sample_ID'],axis=1).columns]).T\n",
    "importance_df_omics.columns = ['importance_omics','name']\n",
    "importance_df_omics.sort_values('importance_omics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Losses by Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "experiments = ['exp3', 'exp6']\n",
    "colors = ['blue', 'orange']\n",
    "mean_colors = ['black', 'red']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "max_epoch = max(max(len(model_data[10]) for model_data in results['mortality'][exp]) for exp in experiments) - 5\n",
    "\n",
    "for experiment, color, mean_color in zip(experiments, colors, mean_colors):\n",
    "    train_losses_all = []\n",
    "    test_losses_all = []\n",
    "    \n",
    "    for model_data in results['mortality'][experiment]:\n",
    "        train_losses = model_data[10][:-5]  # Exclude last 5 epochs\n",
    "        test_losses = model_data[11][:-5]  # Exclude last 5 epochs\n",
    "\n",
    "        train_losses_all.append(train_losses + [np.nan]*(max_epoch-len(train_losses)))\n",
    "        test_losses_all.append(test_losses + [np.nan]*(max_epoch-len(test_losses)))\n",
    "\n",
    "        # Plot each model's losses\n",
    "        plt.plot(train_losses, test_losses, marker='o', linestyle='-', color=color, alpha=0.1)\n",
    "    \n",
    "    # Calculate the mean across each epoch for train and test losses separately\n",
    "    train_losses_mean = np.ma.masked_invalid(train_losses_all).mean(axis=0)\n",
    "    test_losses_mean = np.ma.masked_invalid(test_losses_all).mean(axis=0)\n",
    "\n",
    "    # Sort by training loss\n",
    "    sorted_indices = np.argsort(train_losses_mean)\n",
    "\n",
    "    # Only plot the mean losses when we have at least 5 data points\n",
    "    mask = np.count_nonzero(~np.isnan(train_losses_all), axis=0) >= 5\n",
    "\n",
    "    # Plot the mean losses with the specific color\n",
    "    plt.plot(train_losses_mean[sorted_indices][mask[sorted_indices]], \n",
    "             test_losses_mean[sorted_indices][mask[sorted_indices]], \n",
    "             color=mean_color, linewidth=2.0, label=f\"{experiment} mean\", zorder=100)\n",
    "\n",
    "# Create custom patches for the legend\n",
    "patch1 = mpatches.Patch(color='blue', label='Not PT models')\n",
    "patch2 = mpatches.Patch(color='orange', label='PT models')\n",
    "patch3 = mpatches.Patch(color='black', label='Not PT mean')\n",
    "patch4 = mpatches.Patch(color='red', label='PT mean')\n",
    "\n",
    "plt.gca().set_xscale('log')\n",
    "plt.gca().set_yscale('log')\n",
    "plt.xlabel('Training loss', fontsize=18)\n",
    "plt.ylabel('Test loss', fontsize=18)\n",
    "plt.title('Training losses vs Test losses for different models (log scale)',fontsize=18)\n",
    "plt.legend(handles=[patch1, patch2, patch3, patch4])\n",
    "plt.savefig('./losses.png', dpi=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Trajectory: Proteomics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#return final_pred, (out_ehr, pred_proteomics, pred_ehr, out_combined, final_pred, self.final_combine.weight)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = joint_model(RNN_data_codes.shape[2], best_hidden_dim, [best_hidden_dim], best_num_layers, 1,\n",
    "                    proteomics.shape[1]-1, [best_protein_hidden], [], best_dropout).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "input_proteomics = proteomics.merge(patient_indices[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "input_proteomics = scaler.fit_transform(input_proteomics)\n",
    "\n",
    "EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes]  \n",
    "EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "input_proteomics = torch.tensor(input_proteomics).float()\n",
    "input_proteomics = torch.nan_to_num(input_proteomics)\n",
    "outcomes = torch.tensor(RNN_data_outcomes).float()\n",
    "\n",
    "\n",
    "data_set = DataBuilder(input_proteomics, outcomes, scaler)\n",
    "loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "loader_codes = create_dataloaders(EHR_codes, outcomes, RNN_data_lengths, 100000)\n",
    "            \n",
    "\n",
    "model_paths = [f'/mnt/project/cancer_mortality_3yr_LARGER_models_results/both_PT_FT_{i}_epoch{j}.pth' for i in range(100) for j in range(200)]\n",
    "model_number = 0\n",
    "\n",
    "all_outputs = []\n",
    "model_numbers = []\n",
    "epochs = []\n",
    "PT = []\n",
    "\n",
    "for i in tqdm(model_paths):\n",
    "    result = re.search('both_PT_FT_(\\d+)_epoch(\\d+).pth', i)\n",
    "    model_number = int(result.group(1))\n",
    "    epoch_number = int(result.group(2))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            model_state_dict = torch.load(i)\n",
    "            model.load_state_dict(model_state_dict)\n",
    "            for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(loader_codes, loader_proteomics)):\n",
    "                inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                inputs_proteomics = inputs_proteomics.to(device)\n",
    "                outputs = model(inputs_codes, inputs_proteomics, lengths_codes, interpretability=True)[1][1]\n",
    "            all_outputs.append(outputs.squeeze().cpu().numpy())\n",
    "            model_numbers.append(model_number)\n",
    "            epochs.append(epoch_number)\n",
    "            PT.append('PT')\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "model = joint_model(RNN_data_codes_omics.shape[2], best_hidden_dim_omics, [best_hidden_dim_omics], best_num_layers_omics, 1,\n",
    "                    proteomics.shape[1]-1, [best_protein_hidden], [], best_dropout_omics).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "input_proteomics = proteomics.merge(patient_indices_omics[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "input_proteomics = scaler.fit_transform(input_proteomics)\n",
    "\n",
    "EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes_omics]  \n",
    "EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "input_proteomics = torch.tensor(input_proteomics).float()\n",
    "input_proteomics = torch.nan_to_num(input_proteomics)\n",
    "outcomes = torch.tensor(RNN_data_outcomes_omics).float()\n",
    "\n",
    "\n",
    "data_set = DataBuilder(input_proteomics, outcomes, scaler)\n",
    "loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "loader_codes = create_dataloaders(EHR_codes, outcomes, RNN_data_lengths_omics, 100000)\n",
    "            \n",
    "\n",
    "model_paths = [f'/mnt/project/cancer_mortality_3yr_LARGER_models_results/both_omics_only_{i}_epoch{j}.pth' for i in range(100) for j in range(200)]\n",
    "model_number = 0\n",
    "\n",
    "for i in tqdm(model_paths):\n",
    "    result = re.search('both_omics_only_(\\d+)_epoch(\\d+).pth', i)\n",
    "    model_number = int(result.group(1))\n",
    "    epoch_number = int(result.group(2))\n",
    "    if True:\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                model_state_dict = torch.load(i)\n",
    "                model.load_state_dict(model_state_dict)\n",
    "                for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(loader_codes, loader_proteomics)):\n",
    "                    inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                    inputs_proteomics = inputs_proteomics.to(device)\n",
    "                    outputs = model(inputs_codes, inputs_proteomics, lengths_codes, interpretability=True)[1][1]\n",
    "                all_outputs.append(outputs.squeeze().cpu().numpy())\n",
    "                model_numbers.append(model_number)\n",
    "                epochs.append(epoch_number)\n",
    "                PT.append('NPT')\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "epochs = np.array(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create custom lines for the legend\n",
    "line1 = Line2D([0], [0], color='none', marker='o', markersize=10, markerfacecolor='darkred', label='PT')\n",
    "line2 = Line2D([0], [0], color='none', marker='x', markersize=10, markerfacecolor='darkblue', label='NPT')\n",
    "\n",
    "# Create a t-SNE instance and fit_transform the data\n",
    "tsne = TSNE(n_components=2, random_state=42, init='pca', learning_rate='auto')\n",
    "embedding = tsne.fit_transform(np.array(all_outputs))\n",
    "\n",
    "# Create a markers array based on PT array\n",
    "marker_map = {'PT': 'o', 'NPT': 'x'}\n",
    "markers = [marker_map[pt] for pt in PT]\n",
    "\n",
    "# Don't normalize the epochs\n",
    "colors = epochs\n",
    "\n",
    "# Create colormaps that goes from light red to dark red for PT, light blue to dark blue for NPT\n",
    "cmap_pt = LinearSegmentedColormap.from_list(\"mycmap_pt\", [\"lightcoral\", \"darkred\"])\n",
    "cmap_npt = LinearSegmentedColormap.from_list(\"mycmap_npt\", [\"lightblue\", \"darkblue\"])\n",
    "\n",
    "# Plot the results with different markers and colors\n",
    "for marker_type, marker in marker_map.items():\n",
    "    mask = np.array(markers) == marker\n",
    "    cmap = cmap_pt if marker_type == 'PT' else cmap_npt\n",
    "    sc = plt.scatter(embedding[mask, 0], embedding[mask, 1], marker=marker, c=colors[mask], cmap=cmap, alpha = 0.7)\n",
    "\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.gca().set_xticks([])\n",
    "plt.gca().set_yticks([])\n",
    "plt.title('Protein parameter space', fontsize=14)\n",
    "plt.legend(handles=[line1, line2])  # add the custom legend\n",
    "plt.savefig('./protein_params.png', dpi=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform t-SNE dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=42, init='pca', learning_rate='auto')\n",
    "embedding = tsne.fit_transform(np.array(all_outputs))\n",
    "\n",
    "# Create colormaps that go from light red to dark red for PT, and light blue to dark blue for NPT\n",
    "cmap_pt = LinearSegmentedColormap.from_list(\"mycmap_pt\", [\"lightcoral\", \"darkred\"])\n",
    "cmap_npt = LinearSegmentedColormap.from_list(\"mycmap_npt\", [\"lightblue\", \"darkblue\"])\n",
    "\n",
    "fig, axs = plt.subplots(5, 5, figsize=(20, 20))\n",
    "\n",
    "# Calculate the global x and y limits\n",
    "xlim = (embedding[:,0].min()-1, embedding[:,0].max()+1)\n",
    "ylim = (embedding[:,1].min()-1, embedding[:,1].max()+1)\n",
    "\n",
    "for model_number in range(25):\n",
    "    ax = axs[model_number // 5, model_number % 5]\n",
    "\n",
    "    for model_type in ['PT', 'NPT']:\n",
    "        # Get mask for the current trajectory\n",
    "        mask = (np.array(model_numbers) == model_number) & (np.array(PT) == model_type)\n",
    "\n",
    "        # Get the points and corresponding epochs for the current trajectory\n",
    "        trajectory_points = embedding[mask]\n",
    "        trajectory_epochs = np.array(epochs)[mask]\n",
    "\n",
    "        # Sort the points and epochs\n",
    "        sort_indices = np.argsort(trajectory_epochs)\n",
    "        sorted_points = trajectory_points[sort_indices]\n",
    "\n",
    "        # Select the colormap based on model_type\n",
    "        cmap = cmap_pt if model_type == 'PT' else cmap_npt\n",
    "\n",
    "        # Plot the points with color indicating epoch and marker indicating PT/NPT\n",
    "        ax.scatter(sorted_points[:, 0], sorted_points[:, 1], \n",
    "                   c=trajectory_epochs[sort_indices], \n",
    "                   cmap=cmap, \n",
    "                   marker=marker_map[model_type], \n",
    "                   alpha=0.6)\n",
    "\n",
    "        # Plot lines connecting the points of the same model type\n",
    "        ax.plot(sorted_points[:, 0], sorted_points[:, 1], \n",
    "                color='lightgrey' if model_type == 'PT' else 'black', \n",
    "                linestyle='--')\n",
    "\n",
    "    ax.set_aspect('auto')\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "    # Turn off tick labels\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./protein_paths_cancer.png',dpi=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#return final_pred, (out_ehr, pred_proteomics, pred_ehr, out_combined, final_pred, self.final_combine.weight)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = joint_model(RNN_data_codes.shape[2], best_hidden_dim, [best_hidden_dim], best_num_layers, 1,\n",
    "                    proteomics.shape[1]-1, [best_protein_hidden], [], best_dropout).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "input_proteomics = proteomics.merge(patient_indices[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "input_proteomics = scaler.fit_transform(input_proteomics)\n",
    "\n",
    "EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes]  \n",
    "EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "input_proteomics = torch.tensor(input_proteomics).float()\n",
    "input_proteomics = torch.nan_to_num(input_proteomics)\n",
    "outcomes = torch.tensor(RNN_data_outcomes).float()\n",
    "\n",
    "\n",
    "data_set = DataBuilder(input_proteomics, outcomes, scaler)\n",
    "loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "loader_codes = create_dataloaders(EHR_codes, outcomes, RNN_data_lengths, 100000)\n",
    "            \n",
    "\n",
    "model_paths = [f'/mnt/project/cancer_mortality_3yr_LARGER_models_results/both_PT_FT_{i}_epoch{j}.pth' for i in range(100) for j in range(200)]\n",
    "model_number = 0\n",
    "\n",
    "all_outputs = []\n",
    "model_numbers = []\n",
    "epochs = []\n",
    "PT = []\n",
    "\n",
    "for i in tqdm(model_paths):\n",
    "    result = re.search('both_PT_FT_(\\d+)_epoch(\\d+).pth', i)\n",
    "    model_number = int(result.group(1))\n",
    "    epoch_number = int(result.group(2))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            model_state_dict = torch.load(i)\n",
    "            model.load_state_dict(model_state_dict)\n",
    "            for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(loader_codes, loader_proteomics)):\n",
    "                inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                inputs_proteomics = inputs_proteomics.to(device)\n",
    "                outputs = model(inputs_codes, inputs_proteomics, lengths_codes, interpretability=True)[1][3]\n",
    "            all_outputs.append(outputs.squeeze().cpu().numpy())\n",
    "            model_numbers.append(model_number)\n",
    "            epochs.append(epoch_number)\n",
    "            PT.append('PT')\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "model = joint_model(RNN_data_codes_omics.shape[2], best_hidden_dim_omics, [best_hidden_dim_omics], best_num_layers_omics, 1,\n",
    "                    proteomics.shape[1]-1, [best_protein_hidden], [], best_dropout_omics).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "input_proteomics = proteomics.merge(patient_indices_omics[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "input_proteomics = scaler.fit_transform(input_proteomics)\n",
    "\n",
    "EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes_omics]  \n",
    "EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "input_proteomics = torch.tensor(input_proteomics).float()\n",
    "input_proteomics = torch.nan_to_num(input_proteomics)\n",
    "outcomes = torch.tensor(RNN_data_outcomes_omics).float()\n",
    "\n",
    "\n",
    "data_set = DataBuilder(input_proteomics, outcomes, scaler)\n",
    "loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "loader_codes = create_dataloaders(EHR_codes, outcomes, RNN_data_lengths_omics, 100000)\n",
    "            \n",
    "\n",
    "model_paths = [f'/mnt/project/cancer_mortality_3yr_LARGER_models_results/both_omics_only_{i}_epoch{j}.pth' for i in range(100) for j in range(200)]\n",
    "model_number = 0\n",
    "\n",
    "for i in tqdm(model_paths):\n",
    "    result = re.search('both_omics_only_(\\d+)_epoch(\\d+).pth', i)\n",
    "    model_number = int(result.group(1))\n",
    "    epoch_number = int(result.group(2))\n",
    "    if True:\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                model_state_dict = torch.load(i)\n",
    "                model.load_state_dict(model_state_dict)\n",
    "                for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(loader_codes, loader_proteomics)):\n",
    "                    inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                    inputs_proteomics = inputs_proteomics.to(device)\n",
    "                    outputs = model(inputs_codes, inputs_proteomics, lengths_codes, interpretability=True)[1][3]\n",
    "                all_outputs.append(outputs.squeeze().cpu().numpy())\n",
    "                model_numbers.append(model_number)\n",
    "                epochs.append(epoch_number)\n",
    "                PT.append('NPT')\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "epochs = np.array(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create custom lines for the legend\n",
    "line1 = Line2D([0], [0], color='none', marker='o', markersize=10, markerfacecolor='darkred', label='PT')\n",
    "line2 = Line2D([0], [0], color='none', marker='x', markersize=10, markerfacecolor='darkblue', label='NPT')\n",
    "\n",
    "# Create a t-SNE instance and fit_transform the data\n",
    "tsne = TSNE(n_components=2, random_state=42, init='pca', learning_rate='auto')\n",
    "embedding = tsne.fit_transform(np.array(all_outputs))\n",
    "\n",
    "# Create a markers array based on PT array\n",
    "marker_map = {'PT': 'o', 'NPT': 'x'}\n",
    "markers = [marker_map[pt] for pt in PT]\n",
    "\n",
    "# Don't normalize the epochs\n",
    "colors = epochs\n",
    "\n",
    "# Create colormaps that goes from light red to dark red for PT, light blue to dark blue for NPT\n",
    "cmap_pt = LinearSegmentedColormap.from_list(\"mycmap_pt\", [\"lightcoral\", \"darkred\"])\n",
    "cmap_npt = LinearSegmentedColormap.from_list(\"mycmap_npt\", [\"lightblue\", \"darkblue\"])\n",
    "\n",
    "# Plot the results with different markers and colors\n",
    "for marker_type, marker in marker_map.items():\n",
    "    mask = np.array(markers) == marker\n",
    "    cmap = cmap_pt if marker_type == 'PT' else cmap_npt\n",
    "    sc = plt.scatter(embedding[mask, 0], embedding[mask, 1], marker=marker, c=colors[mask], cmap=cmap, alpha = 0.7)\n",
    "\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.gca().set_xticks([])\n",
    "plt.gca().set_yticks([])\n",
    "plt.title('Joint EHR-protein parameter space', fontsize=14)\n",
    "plt.legend(handles=[line1, line2])  # add the custom legend\n",
    "plt.savefig('./joint_params.png', dpi=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform t-SNE dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=42, init='pca', learning_rate='auto')\n",
    "embedding = tsne.fit_transform(np.array(all_outputs))\n",
    "\n",
    "# Create colormaps that go from light red to dark red for PT, and light blue to dark blue for NPT\n",
    "cmap_pt = LinearSegmentedColormap.from_list(\"mycmap_pt\", [\"lightcoral\", \"darkred\"])\n",
    "cmap_npt = LinearSegmentedColormap.from_list(\"mycmap_npt\", [\"lightblue\", \"darkblue\"])\n",
    "\n",
    "fig, axs = plt.subplots(5, 5, figsize=(20, 20))\n",
    "\n",
    "# Calculate the global x and y limits\n",
    "xlim = (embedding[:,0].min()-1, embedding[:,0].max()+1)\n",
    "ylim = (embedding[:,1].min()-1, embedding[:,1].max()+1)\n",
    "\n",
    "for model_number in range(25):\n",
    "    ax = axs[model_number // 5, model_number % 5]\n",
    "\n",
    "    for model_type in ['PT', 'NPT']:\n",
    "        # Get mask for the current trajectory\n",
    "        mask = (np.array(model_numbers) == model_number) & (np.array(PT) == model_type)\n",
    "\n",
    "        # Get the points and corresponding epochs for the current trajectory\n",
    "        trajectory_points = embedding[mask]\n",
    "        trajectory_epochs = np.array(epochs)[mask]\n",
    "\n",
    "        # Sort the points and epochs\n",
    "        sort_indices = np.argsort(trajectory_epochs)\n",
    "        sorted_points = trajectory_points[sort_indices]\n",
    "\n",
    "        # Select the colormap based on model_type\n",
    "        cmap = cmap_pt if model_type == 'PT' else cmap_npt\n",
    "\n",
    "        # Plot the points with color indicating epoch and marker indicating PT/NPT\n",
    "        ax.scatter(sorted_points[:, 0], sorted_points[:, 1], \n",
    "                   c=trajectory_epochs[sort_indices], \n",
    "                   cmap=cmap, \n",
    "                   marker=marker_map[model_type], \n",
    "                   alpha=0.6)\n",
    "\n",
    "        # Plot lines connecting the points of the same model type\n",
    "        ax.plot(sorted_points[:, 0], sorted_points[:, 1], \n",
    "                color='lightgrey' if model_type == 'PT' else 'black', \n",
    "                linestyle='--')\n",
    "\n",
    "    ax.set_aspect('auto')\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "    # Turn off tick labels\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./joint_paths_cancer.png',dpi=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#return final_pred, (out_ehr, pred_proteomics, pred_ehr, out_combined, final_pred, self.final_combine.weight)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = joint_model(RNN_data_codes.shape[2], best_hidden_dim, [best_hidden_dim], best_num_layers, 1,\n",
    "                    proteomics.shape[1]-1, [best_protein_hidden], [], best_dropout).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "input_proteomics = proteomics.merge(patient_indices[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "input_proteomics = scaler.fit_transform(input_proteomics)\n",
    "\n",
    "EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes]  \n",
    "EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "input_proteomics = torch.tensor(input_proteomics).float()\n",
    "input_proteomics = torch.nan_to_num(input_proteomics)\n",
    "outcomes = torch.tensor(RNN_data_outcomes).float()\n",
    "\n",
    "\n",
    "data_set = DataBuilder(input_proteomics, outcomes, scaler)\n",
    "loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "loader_codes = create_dataloaders(EHR_codes, outcomes, RNN_data_lengths, 100000)\n",
    "            \n",
    "\n",
    "model_paths = [f'/mnt/project/cancer_mortality_3yr_LARGER_models_results/both_PT_FT_{i}_epoch{j}.pth' for i in range(100) for j in range(200)]\n",
    "model_number = 0\n",
    "\n",
    "all_outputs = []\n",
    "model_numbers = []\n",
    "epochs = []\n",
    "PT = []\n",
    "\n",
    "for i in tqdm(model_paths):\n",
    "    result = re.search('both_PT_FT_(\\d+)_epoch(\\d+).pth', i)\n",
    "    model_number = int(result.group(1))\n",
    "    epoch_number = int(result.group(2))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            model_state_dict = torch.load(i)\n",
    "            model.load_state_dict(model_state_dict)\n",
    "            for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(loader_codes, loader_proteomics)):\n",
    "                inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                inputs_proteomics = inputs_proteomics.to(device)\n",
    "                outputs = model(inputs_codes, inputs_proteomics, lengths_codes, interpretability=True)[1][2]\n",
    "            all_outputs.append(outputs.squeeze().cpu().numpy())\n",
    "            model_numbers.append(model_number)\n",
    "            epochs.append(epoch_number)\n",
    "            PT.append('PT')\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "model = joint_model(RNN_data_codes_omics.shape[2], best_hidden_dim_omics, [best_hidden_dim_omics], best_num_layers_omics, 1,\n",
    "                    proteomics.shape[1]-1, [best_protein_hidden], [], best_dropout_omics).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "input_proteomics = proteomics.merge(patient_indices_omics[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "input_proteomics = scaler.fit_transform(input_proteomics)\n",
    "\n",
    "EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes_omics]  \n",
    "EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "input_proteomics = torch.tensor(input_proteomics).float()\n",
    "input_proteomics = torch.nan_to_num(input_proteomics)\n",
    "outcomes = torch.tensor(RNN_data_outcomes_omics).float()\n",
    "\n",
    "\n",
    "data_set = DataBuilder(input_proteomics, outcomes, scaler)\n",
    "loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "loader_codes = create_dataloaders(EHR_codes, outcomes, RNN_data_lengths_omics, 100000)\n",
    "            \n",
    "\n",
    "model_paths = [f'/mnt/project/cancer_mortality_3yr_LARGER_models_results/both_omics_only_{i}_epoch{j}.pth' for i in range(100) for j in range(200)]\n",
    "model_number = 0\n",
    "\n",
    "for i in tqdm(model_paths):\n",
    "    result = re.search('both_omics_only_(\\d+)_epoch(\\d+).pth', i)\n",
    "    model_number = int(result.group(1))\n",
    "    epoch_number = int(result.group(2))\n",
    "    if True:\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                model_state_dict = torch.load(i)\n",
    "                model.load_state_dict(model_state_dict)\n",
    "                for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(loader_codes, loader_proteomics)):\n",
    "                    inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                    inputs_proteomics = inputs_proteomics.to(device)\n",
    "                    outputs = model(inputs_codes, inputs_proteomics, lengths_codes, interpretability=True)[1][2]\n",
    "                all_outputs.append(outputs.squeeze().cpu().numpy())\n",
    "                model_numbers.append(model_number)\n",
    "                epochs.append(epoch_number)\n",
    "                PT.append('NPT')\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "epochs = np.array(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create custom lines for the legend\n",
    "line1 = Line2D([0], [0], color='none', marker='o', markersize=10, markerfacecolor='darkred', label='PT')\n",
    "line2 = Line2D([0], [0], color='none', marker='x', markersize=10, markerfacecolor='darkblue', label='NPT')\n",
    "\n",
    "# Create a t-SNE instance and fit_transform the data\n",
    "tsne = TSNE(n_components=2, random_state=42, init='pca', learning_rate='auto')\n",
    "embedding = tsne.fit_transform(np.array(all_outputs))\n",
    "\n",
    "# Create a markers array based on PT array\n",
    "marker_map = {'PT': 'o', 'NPT': 'x'}\n",
    "markers = [marker_map[pt] for pt in PT]\n",
    "\n",
    "# Don't normalize the epochs\n",
    "colors = epochs\n",
    "\n",
    "# Create colormaps that goes from light red to dark red for PT, light blue to dark blue for NPT\n",
    "cmap_pt = LinearSegmentedColormap.from_list(\"mycmap_pt\", [\"lightcoral\", \"darkred\"])\n",
    "cmap_npt = LinearSegmentedColormap.from_list(\"mycmap_npt\", [\"lightblue\", \"darkblue\"])\n",
    "\n",
    "# Plot the results with different markers and colors\n",
    "for marker_type, marker in marker_map.items():\n",
    "    mask = np.array(markers) == marker\n",
    "    cmap = cmap_pt if marker_type == 'PT' else cmap_npt\n",
    "    sc = plt.scatter(embedding[mask, 0], embedding[mask, 1], marker=marker, c=colors[mask], cmap=cmap, alpha = 0.7)\n",
    "\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.gca().set_xticks([])\n",
    "plt.gca().set_yticks([])\n",
    "plt.title('EHR parameter space', fontsize=14)\n",
    "plt.legend(handles=[line1, line2])  # add the custom legend\n",
    "plt.savefig('./EHR_params.png', dpi=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform t-SNE dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=42, init='pca', learning_rate='auto')\n",
    "embedding = tsne.fit_transform(np.array(all_outputs))\n",
    "\n",
    "# Create colormaps that go from light red to dark red for PT, and light blue to dark blue for NPT\n",
    "cmap_pt = LinearSegmentedColormap.from_list(\"mycmap_pt\", [\"lightcoral\", \"darkred\"])\n",
    "cmap_npt = LinearSegmentedColormap.from_list(\"mycmap_npt\", [\"lightblue\", \"darkblue\"])\n",
    "\n",
    "fig, axs = plt.subplots(5, 5, figsize=(20, 20))\n",
    "\n",
    "# Calculate the global x and y limits\n",
    "xlim = (embedding[:,0].min()-1, embedding[:,0].max()+1)\n",
    "ylim = (embedding[:,1].min()-1, embedding[:,1].max()+1)\n",
    "\n",
    "for model_number in range(25):\n",
    "    ax = axs[model_number // 5, model_number % 5]\n",
    "\n",
    "    for model_type in ['PT', 'NPT']:\n",
    "        # Get mask for the current trajectory\n",
    "        mask = (np.array(model_numbers) == model_number) & (np.array(PT) == model_type)\n",
    "\n",
    "        # Get the points and corresponding epochs for the current trajectory\n",
    "        trajectory_points = embedding[mask]\n",
    "        trajectory_epochs = np.array(epochs)[mask]\n",
    "\n",
    "        # Sort the points and epochs\n",
    "        sort_indices = np.argsort(trajectory_epochs)\n",
    "        sorted_points = trajectory_points[sort_indices]\n",
    "\n",
    "        # Select the colormap based on model_type\n",
    "        cmap = cmap_pt if model_type == 'PT' else cmap_npt\n",
    "\n",
    "        # Plot the points with color indicating epoch and marker indicating PT/NPT\n",
    "        ax.scatter(sorted_points[:, 0], sorted_points[:, 1], \n",
    "                   c=trajectory_epochs[sort_indices], \n",
    "                   cmap=cmap, \n",
    "                   marker=marker_map[model_type], \n",
    "                   alpha=0.6)\n",
    "\n",
    "        # Plot lines connecting the points of the same model type\n",
    "        ax.plot(sorted_points[:, 0], sorted_points[:, 1], \n",
    "                color='lightgrey' if model_type == 'PT' else 'black', \n",
    "                linestyle='--')\n",
    "\n",
    "    ax.set_aspect('auto')\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "    # Turn off tick labels\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./EHR_paths_cancer.png',dpi=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#return final_pred, (out_ehr, pred_proteomics, pred_ehr, out_combined, final_pred, self.final_combine.weight)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = joint_model(RNN_data_codes.shape[2], best_hidden_dim, [best_hidden_dim], best_num_layers, 1,\n",
    "                    proteomics.shape[1]-1, [best_protein_hidden], [], best_dropout).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "input_proteomics = proteomics.merge(patient_indices[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "input_proteomics = scaler.fit_transform(input_proteomics)\n",
    "\n",
    "EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes]  \n",
    "EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "input_proteomics = torch.tensor(input_proteomics).float()\n",
    "input_proteomics = torch.nan_to_num(input_proteomics)\n",
    "outcomes = torch.tensor(RNN_data_outcomes).float()\n",
    "\n",
    "\n",
    "data_set = DataBuilder(input_proteomics, outcomes, scaler)\n",
    "loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "loader_codes = create_dataloaders(EHR_codes, outcomes, RNN_data_lengths, 100000)\n",
    "            \n",
    "\n",
    "model_paths = [f'/mnt/project/cancer_mortality_3yr_LARGER_models_results/both_PT_FT_{i}_epoch{j}.pth' for i in range(100) for j in range(200)]\n",
    "model_number = 0\n",
    "\n",
    "all_outputs = []\n",
    "model_numbers = []\n",
    "epochs = []\n",
    "PT = []\n",
    "\n",
    "for i in tqdm(model_paths):\n",
    "    result = re.search('both_PT_FT_(\\d+)_epoch(\\d+).pth', i)\n",
    "    model_number = int(result.group(1))\n",
    "    epoch_number = int(result.group(2))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            model_state_dict = torch.load(i)\n",
    "            model.load_state_dict(model_state_dict)\n",
    "            for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(loader_codes, loader_proteomics)):\n",
    "                inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                inputs_proteomics = inputs_proteomics.to(device)\n",
    "                outputs = model(inputs_codes, inputs_proteomics, lengths_codes, interpretability=True)[1][4]\n",
    "            all_outputs.append(outputs.squeeze().cpu().numpy())\n",
    "            model_numbers.append(model_number)\n",
    "            epochs.append(epoch_number)\n",
    "            PT.append('PT')\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "model = joint_model(RNN_data_codes_omics.shape[2], best_hidden_dim_omics, [best_hidden_dim_omics], best_num_layers_omics, 1,\n",
    "                    proteomics.shape[1]-1, [best_protein_hidden], [], best_dropout_omics).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "input_proteomics = proteomics.merge(patient_indices_omics[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "input_proteomics = scaler.fit_transform(input_proteomics)\n",
    "\n",
    "EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes_omics]  \n",
    "EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "input_proteomics = torch.tensor(input_proteomics).float()\n",
    "input_proteomics = torch.nan_to_num(input_proteomics)\n",
    "outcomes = torch.tensor(RNN_data_outcomes_omics).float()\n",
    "\n",
    "\n",
    "data_set = DataBuilder(input_proteomics, outcomes, scaler)\n",
    "loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "loader_codes = create_dataloaders(EHR_codes, outcomes, RNN_data_lengths_omics, 100000)\n",
    "            \n",
    "\n",
    "model_paths = [f'/mnt/project/cancer_mortality_3yr_LARGER_models_results/both_omics_only_{i}_epoch{j}.pth' for i in range(100) for j in range(200)]\n",
    "model_number = 0\n",
    "\n",
    "for i in tqdm(model_paths):\n",
    "    result = re.search('both_omics_only_(\\d+)_epoch(\\d+).pth', i)\n",
    "    model_number = int(result.group(1))\n",
    "    epoch_number = int(result.group(2))\n",
    "    if True:\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                model_state_dict = torch.load(i)\n",
    "                model.load_state_dict(model_state_dict)\n",
    "                for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(loader_codes, loader_proteomics)):\n",
    "                    inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                    inputs_proteomics = inputs_proteomics.to(device)\n",
    "                    outputs = model(inputs_codes, inputs_proteomics, lengths_codes, interpretability=True)[1][4]\n",
    "                all_outputs.append(outputs.squeeze().cpu().numpy())\n",
    "                model_numbers.append(model_number)\n",
    "                epochs.append(epoch_number)\n",
    "                PT.append('NPT')\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "epochs = np.array(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create custom lines for the legend\n",
    "line1 = Line2D([0], [0], color='none', marker='o', markersize=10, markerfacecolor='darkred', label='PT')\n",
    "line2 = Line2D([0], [0], color='none', marker='x', markersize=10, markerfacecolor='darkblue', label='NPT')\n",
    "\n",
    "# Create a t-SNE instance and fit_transform the data\n",
    "tsne = TSNE(n_components=2, random_state=42, init='pca', learning_rate='auto', perplexity=400)\n",
    "embedding = tsne.fit_transform(np.array(all_outputs))\n",
    "\n",
    "# Create a markers array based on PT array\n",
    "marker_map = {'PT': 'o', 'NPT': 'x'}\n",
    "markers = [marker_map[pt] for pt in PT]\n",
    "\n",
    "# Don't normalize the epochs\n",
    "colors = epochs\n",
    "\n",
    "# Create colormaps that goes from light red to dark red for PT, light blue to dark blue for NPT\n",
    "cmap_pt = LinearSegmentedColormap.from_list(\"mycmap_pt\", [\"lightcoral\", \"darkred\"])\n",
    "cmap_npt = LinearSegmentedColormap.from_list(\"mycmap_npt\", [\"lightblue\", \"darkblue\"])\n",
    "\n",
    "# Plot the results with different markers and colors\n",
    "for marker_type, marker in marker_map.items():\n",
    "    mask = np.array(markers) == marker\n",
    "    cmap = cmap_pt if marker_type == 'PT' else cmap_npt\n",
    "    sc = plt.scatter(embedding[mask, 0], embedding[mask, 1], marker=marker, c=colors[mask], cmap=cmap, alpha = 0.7)\n",
    "\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.gca().set_xticks([])\n",
    "plt.gca().set_yticks([])\n",
    "plt.title('Overall parameter space', fontsize=14)\n",
    "plt.legend(handles=[line1, line2])  # add the custom legend\n",
    "plt.savefig('./overall_params.png', dpi=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform t-SNE dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=42, init='pca', learning_rate='auto')\n",
    "embedding = tsne.fit_transform(np.array(all_outputs))\n",
    "\n",
    "# Create colormaps that go from light red to dark red for PT, and light blue to dark blue for NPT\n",
    "cmap_pt = LinearSegmentedColormap.from_list(\"mycmap_pt\", [\"lightcoral\", \"darkred\"])\n",
    "cmap_npt = LinearSegmentedColormap.from_list(\"mycmap_npt\", [\"lightblue\", \"darkblue\"])\n",
    "\n",
    "fig, axs = plt.subplots(5, 5, figsize=(20, 20))\n",
    "\n",
    "# Calculate the global x and y limits\n",
    "xlim = (embedding[:,0].min()-1, embedding[:,0].max()+1)\n",
    "ylim = (embedding[:,1].min()-1, embedding[:,1].max()+1)\n",
    "\n",
    "for model_number in range(25):\n",
    "    ax = axs[model_number // 5, model_number % 5]\n",
    "\n",
    "    for model_type in ['PT', 'NPT']:\n",
    "        # Get mask for the current trajectory\n",
    "        mask = (np.array(model_numbers) == model_number) & (np.array(PT) == model_type)\n",
    "\n",
    "        # Get the points and corresponding epochs for the current trajectory\n",
    "        trajectory_points = embedding[mask]\n",
    "        trajectory_epochs = np.array(epochs)[mask]\n",
    "\n",
    "        # Sort the points and epochs\n",
    "        sort_indices = np.argsort(trajectory_epochs)\n",
    "        sorted_points = trajectory_points[sort_indices]\n",
    "\n",
    "        # Select the colormap based on model_type\n",
    "        cmap = cmap_pt if model_type == 'PT' else cmap_npt\n",
    "\n",
    "        # Plot the points with color indicating epoch and marker indicating PT/NPT\n",
    "        ax.scatter(sorted_points[:, 0], sorted_points[:, 1], \n",
    "                   c=trajectory_epochs[sort_indices], \n",
    "                   cmap=cmap, \n",
    "                   marker=marker_map[model_type], \n",
    "                   alpha=0.6)\n",
    "\n",
    "        # Plot lines connecting the points of the same model type\n",
    "        ax.plot(sorted_points[:, 0], sorted_points[:, 1], \n",
    "                color='lightgrey' if model_type == 'PT' else 'black', \n",
    "                linestyle='--')\n",
    "\n",
    "    ax.set_aspect('auto')\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "    # Turn off tick labels\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./overall_paths_cancer.png',dpi=1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
