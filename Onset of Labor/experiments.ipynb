{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, cross_val_predict, train_test_split, KFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, roc_curve, balanced_accuracy_score, accuracy_score, mean_squared_error\n",
    "from scipy.stats import pearsonr, ttest_rel\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE, Isomap\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import LinearSegmentedColormap, ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "import loralib as lora\n",
    "import random\n",
    "import umap\n",
    "import re\n",
    "from itertools import product\n",
    "import os\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load proteomics data\n",
    "OOL_proteomics = pd.read_csv('./data/processed_data/ool_proteomics_omop_id.csv').drop(['Unnamed: 0','SampleID','ID','EGA'],axis=1)\n",
    "OOL_proteomics['sample_ID'] = OOL_proteomics['maternal_person_id'].astype(str)+'_'+OOL_proteomics['Timepoint'].astype(str)\n",
    "OOL_proteomics = OOL_proteomics.drop(['Timepoint','maternal_person_id'],axis=1)\n",
    "OOL_proteomics.columns = [str(i)+'_protein' for i in OOL_proteomics.columns]\n",
    "OOL_proteomics = OOL_proteomics.rename(columns={'DOS_protein':'DOS', 'sample_ID_protein':'sample_ID'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load outcomes\n",
    "OOL_outcomes = OOL_proteomics[['sample_ID','DOS']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#omics only architecture \n",
    "class proteomics_net(nn.Module):\n",
    "    def __init__(self, input_size, None, output_size, dropout=0.1):\n",
    "        super(proteomics_net, self).__init__()\n",
    "        \n",
    "        self.proteomics_layers = nn.Sequential(\n",
    "            nn.Linear(input_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proteomics_layers(x)\n",
    "        return x\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        pretrained_dict = torch.load(path, map_location=lambda storage, loc: storage)\n",
    "        model_dict = self.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        self.load_state_dict(model_dict)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EHR architecture\n",
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_size_codes, hidden_size, prediction_module_hidden_sizes, num_layers, output_size, dropout=0.1):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size_codes, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        self.prediction_module = nn.Sequential(\n",
    "            nn.Linear(prediction_module_hidden_sizes[0], output_size)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, lengths, interpretability=False):\n",
    "        device = x.device\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        x_packed = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        out_packed, _ = self.gru(x_packed, h0)\n",
    "        out, _ = pad_packed_sequence(out_packed, batch_first=True)\n",
    "        \n",
    "        out = out[torch.arange(x.size(0)), lengths-1, :]\n",
    "        out_final = self.prediction_module(out)\n",
    "        if interpretability == False:\n",
    "            return out_final\n",
    "        else:\n",
    "            return out_final, out\n",
    "\n",
    "# Prepare the dataset\n",
    "class PatientDataset(Dataset):\n",
    "    def __init__(self, data, labels, lengths):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.lengths = lengths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx], self.lengths[idx]\n",
    "\n",
    "# Custom collate function for DataLoader\n",
    "def collate_fn(batch):\n",
    "    data, labels, lengths = zip(*batch)\n",
    "    data = pad_sequence(data, batch_first=True, padding_value=0)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "    return data, labels, lengths\n",
    "\n",
    "\n",
    "def normalize_dataset(dataset):\n",
    "    for i, sequence in enumerate(dataset):\n",
    "        dataset[i] = (sequence - sequence.mean(dim=0, keepdim=True)) / (sequence.std(dim=0, keepdim=True) + 1e-8)\n",
    "    return dataset\n",
    "\n",
    "def impute_missing_values(dataset):\n",
    "    # Stack all tensors in the dataset along a new dimension, creating a tensor of shape (num_samples, max_seq_length, num_features)\n",
    "    stacked_data = torch.stack(dataset)\n",
    "\n",
    "    # Calculate the mean of each feature across all samples and sequences, ignoring NaN values\n",
    "    feature_means = torch.nanmean(stacked_data, dim=(0, 1))\n",
    "\n",
    "    # Iterate through the dataset (list of tensors)\n",
    "    for i, sequence in enumerate(dataset):\n",
    "        # Create a boolean mask indicating the positions of NaN values in the sequence\n",
    "        mask = torch.isnan(sequence)\n",
    "\n",
    "        # Replace NaN values in the sequence with the corresponding feature means\n",
    "        # 'expand_as' is used to match the dimensions of the mask and the sequence\n",
    "        dataset[i][mask] = feature_means.expand_as(sequence)[mask]\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Create DataLoaders\n",
    "def create_dataloaders(patient_data, patient_outcomes, lengths, batch_size=64, normalize=False):\n",
    "    \n",
    "    X_train = impute_missing_values(patient_data)\n",
    "    y_train = patient_outcomes\n",
    "    \n",
    "    if normalize:\n",
    "        X_train = normalize_dataset(X_train)\n",
    "    \n",
    "    y_train = (y_train-y_train.mean()) / y_train.std()\n",
    "    \n",
    "    train_dataset = PatientDataset(X_train, y_train, lengths)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, worker_init_fn=worker_init_fn)\n",
    "    return train_loader\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "def worker_init_fn(worker_id):\n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full COMET framework architecture\n",
    "class joint_model(nn.Module):\n",
    "    def __init__(self, input_size_codes, hidden_size, prediction_module_hidden_sizes, num_layers, output_size, input_size_proteomics, None, combined_hidden_layers, dropout=0.1):\n",
    "        super(joint_model, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size_codes, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "    \n",
    "        self.prediction_module = nn.Sequential(\n",
    "            nn.Linear(prediction_module_hidden_sizes[0], output_size)\n",
    "        )\n",
    "\n",
    "        self.skip_connect_prot = nn.Linear(input_size_proteomics, output_size)\n",
    "        \n",
    "        self.combined_layers = nn.Sequential(\n",
    "            nn.Linear(input_size_proteomics + hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "        self.final_combine = nn.Linear(3, 1, bias=False)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, x_proteomics, lengths, interpretability=False, better_latent=None, better_ratio=0.5):\n",
    "        device = x.device\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        x_packed = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        out_packed, _ = self.gru(x_packed, h0)\n",
    "        out, _ = pad_packed_sequence(out_packed, batch_first=True)\n",
    "        \n",
    "        out_ehr = out[torch.arange(x.size(0)), lengths-1, :]\n",
    "        \n",
    "        if x_proteomics == None:\n",
    "            return out_ehr\n",
    "\n",
    "        if better_latent is not None:\n",
    "            out_ehr = better_ratio * better_latent + (1-better_ratio) * out_ehr\n",
    "                \n",
    "        out_combined = torch.cat((out_ehr, x_proteomics), 1)\n",
    "        \n",
    "        out_combined = self.combined_layers(out_combined)\n",
    "        \n",
    "        pred_proteomics = self.skip_connect_prot(x_proteomics)\n",
    "        pred_ehr = self.prediction_module(out_ehr)\n",
    "        \n",
    "        final_pred = self.final_combine(torch.cat((pred_proteomics, pred_ehr, out_combined), 1))\n",
    "        \n",
    "        if interpretability == False:\n",
    "            return final_pred\n",
    "        else:\n",
    "            return final_pred, (out_ehr, pred_proteomics, pred_ehr, out_combined, final_pred, self.final_combine.weight)\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        pretrained_dict = torch.load(path, map_location=lambda storage, loc: storage)\n",
    "        model_dict = self.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        self.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBuilder(Dataset):\n",
    "    def __init__(self, x, y, standardizer):\n",
    "        self.x, self.y, self.standardizer = x, y, standardizer\n",
    "        self.len=self.x.shape[0]\n",
    "    def __getitem__(self,index):      \n",
    "        return (self.x[index], self.y[index])\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(EHR_codes, proteomics, patient_indices, outcomes, lengths, experiment_name, lr, lr_decay,\n",
    "                   bs, train_indices=None, val_indices=None, test_indices=None, feature_types='EHR', model_path='', fine_tune=False, seed=42, num_layers=2,hidden_dim=400,\n",
    "                   dropout=0.4, return_preds=False, return_interpretability=False, return_grads=False,\n",
    "                   hyperparam_tuning=False):\n",
    "    \"\"\"\n",
    "    EHR_codes: pre-processed data for codes of shape (num_patients, max_length, embedding_dim)v\n",
    "    EHR_vitals: pre-processed data for codes of shape (num_patients, max_length, embedding_dim)\n",
    "    proteomics: dataframe with proteomics data\n",
    "    patient_indices: dataframe with sample IDs and row numbers in pre-processed matrices\n",
    "    outcomes: array with DOS\n",
    "    lengths: array with lengths (i.e. number of visits) to help with padding\n",
    "    experiment_name: string for file name for models\n",
    "    lr: float for learning rate\n",
    "    lr_decay: float for learning rate decay\n",
    "    bs: int for batch size\n",
    "    feature_types: string either 'EHR', 'metab', 'both'\n",
    "    model_path: string for file path to model if loading a pre-trained model\n",
    "    fine_tune: boolean for whether or not EHR weight should be learned, can only be true if model != ''\n",
    "    seed: int, random_seed for train/test/val split and seeding model \n",
    "    num_layers: number of GRU layers in RNN\n",
    "    hidden_dim: hidden_dim of GRU output\n",
    "    dropout: dropout weight in model\n",
    "    return_preds: setting to control output of function, if True we return the predictions\n",
    "    return_interpretability: setting to control output of function, if True we return some additional data to help with interpretability analysis\n",
    "    return_grads: setting to control output of function, if True we return the gradient\n",
    "    hyperparam_tuning: setting to control whether or not we save the model at each epoch (if True, we do not)\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "    prediction_module_hidden_sizes = [hidden_dim,hidden_dim//2, hidden_dim//4, hidden_dim//8]\n",
    "    \n",
    "    assert feature_types in ['EHR','metab','both']   \n",
    "    if feature_types == 'metab': assert model_path == ''\n",
    "    if (model_path != '') & (feature_types == 'both'): assert fine_tune==True\n",
    "    if hyperparam_tuning == False: assert train_indices == None\n",
    "\n",
    "    if hyperparam_tuning == False:\n",
    "        maternal_IDs = patient_indices['sample_ID'].str[0:7].unique()\n",
    "\n",
    "        train_ratio = 0.70\n",
    "        test_ratio = 0.15\n",
    "        val_ratio = 0.15\n",
    "\n",
    "        # First, split the unique_ids into train and temp (test + validation) sets\n",
    "        train_ids, temp_ids = train_test_split(maternal_IDs, test_size=(test_ratio + val_ratio),random_state=seed)\n",
    "\n",
    "        # Next, split the temp_ids into test and validation sets\n",
    "        test_ids, val_ids = train_test_split(temp_ids, test_size=(val_ratio / (test_ratio + val_ratio)), random_state=seed)\n",
    "        patient_indices['maternal_ID'] = patient_indices['sample_ID'].str[0:7]\n",
    "        patient_indices['maternal_ID_ts'] = patient_indices['maternal_ID'].astype(str)+'_'+patient_indices['sample_ID'].str[-2:]\n",
    "        proteomics = proteomics.merge(patient_indices[['maternal_ID_ts','array_index']], how='left', left_on='sample_ID', right_on='maternal_ID_ts').drop(['sample_ID','maternal_ID_ts','DOS'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "\n",
    "        train_indices = patient_indices[patient_indices['maternal_ID'].isin(train_ids)]['array_index'].values\n",
    "        np.random.shuffle(train_indices)\n",
    "        test_indices = patient_indices[patient_indices['maternal_ID'].isin(test_ids)]['array_index'].values\n",
    "        val_indices = patient_indices[patient_indices['maternal_ID'].isin(val_ids)]['array_index'].values\n",
    "    \n",
    "    #data processing for train data to prepare for input to ML model\n",
    "    train_EHR_codes = EHR_codes[train_indices,:,:]\n",
    "    if feature_types != 'EHR':\n",
    "        train_proteomics = proteomics[train_indices,:]\n",
    "        scaler = StandardScaler()\n",
    "        train_proteomics = scaler.fit_transform(train_proteomics)\n",
    "    train_outcomes = outcomes[train_indices]\n",
    "    mean_train_outcomes = np.mean(train_outcomes)\n",
    "    sd_train_outcomes = np.std(train_outcomes)\n",
    "    train_outcomes = (train_outcomes - np.mean(train_outcomes))/np.std(train_outcomes)\n",
    "    train_lengths = lengths[train_indices]\n",
    "    \n",
    "    #data processing for test data to prepare for input to ML model\n",
    "    test_EHR_codes = EHR_codes[test_indices, :, :]\n",
    "    if feature_types != 'EHR':\n",
    "        test_proteomics = proteomics[test_indices, :]\n",
    "        scaler = StandardScaler()\n",
    "        test_proteomics = scaler.fit_transform(test_proteomics)\n",
    "    test_outcomes = outcomes[test_indices]\n",
    "    mean_test_outcomes = np.mean(test_outcomes)\n",
    "    sd_test_outcomes = np.std(test_outcomes)\n",
    "    test_outcomes = (test_outcomes - np.mean(test_outcomes))/np.std(test_outcomes)\n",
    "    test_lengths = lengths[test_indices]\n",
    "    \n",
    "    #data processing for val data to prepare for input to ML model\n",
    "    val_EHR_codes = EHR_codes[val_indices, :, :]\n",
    "    if feature_types != 'EHR':\n",
    "        val_proteomics = proteomics[val_indices, :]\n",
    "        scaler = StandardScaler()\n",
    "        val_proteomics = scaler.fit_transform(val_proteomics)\n",
    "    val_outcomes = outcomes[val_indices]\n",
    "    mean_val_outcomes = np.mean(val_outcomes)\n",
    "    sd_val_outcomes = np.std(val_outcomes)\n",
    "    val_outcomes = (val_outcomes - np.mean(val_outcomes))/np.std(val_outcomes)\n",
    "    val_lengths = lengths[val_indices]\n",
    "    \n",
    "    #data processing for all data to prepare for input to ML model\n",
    "    all_EHR_codes = EHR_codes\n",
    "    scaler = StandardScaler()\n",
    "    all_outcomes = outcomes\n",
    "    all_outcomes = (all_outcomes - np.mean(all_outcomes))/np.std(all_outcomes)\n",
    "    all_lengths = lengths\n",
    "\n",
    "    #additional training data processesing\n",
    "    train_EHR_codes = [torch.tensor(data).float() for data in train_EHR_codes]  \n",
    "    train_EHR_codes = [torch.nan_to_num(x) for x in train_EHR_codes]\n",
    "    if feature_types != 'EHR':\n",
    "        train_proteomics = torch.tensor(train_proteomics).float()\n",
    "        train_proteomics = torch.nan_to_num(train_proteomics)\n",
    "    train_outcomes = torch.tensor(train_outcomes).float()\n",
    "    \n",
    "    #additional test data processesing\n",
    "    test_EHR_codes = [torch.tensor(data).float() for data in test_EHR_codes]\n",
    "    test_EHR_codes = [torch.nan_to_num(x) for x in test_EHR_codes]\n",
    "    if feature_types != 'EHR':\n",
    "        test_proteomics = torch.tensor(test_proteomics).float()\n",
    "        test_proteomics = torch.nan_to_num(test_proteomics)\n",
    "    test_outcomes = torch.tensor(test_outcomes).float()\n",
    "\n",
    "    #additional validation data processesing\n",
    "    val_EHR_codes = [torch.tensor(data).float() for data in val_EHR_codes]\n",
    "    val_EHR_codes = [torch.nan_to_num(x) for x in val_EHR_codes]\n",
    "    if feature_types != 'EHR':\n",
    "        val_proteomics = torch.tensor(val_proteomics).float()\n",
    "        val_proteomics = torch.nan_to_num(val_proteomics)\n",
    "    val_outcomes = torch.tensor(val_outcomes).float()\n",
    "\n",
    "    #additional all data processesing\n",
    "    all_EHR_codes = [torch.tensor(data).float() for data in all_EHR_codes]\n",
    "    all_EHR_codes = [torch.nan_to_num(x) for x in all_EHR_codes]\n",
    "    all_outcomes = torch.tensor(all_outcomes).float()\n",
    "\n",
    "    #If omics data is used, create a dataloader for omics data\n",
    "    if feature_types != 'EHR':\n",
    "        data_set_train = DataBuilder(train_proteomics, train_outcomes, scaler)\n",
    "        train_loader_proteomics = DataLoader(dataset=data_set_train,batch_size=bs, worker_init_fn=worker_init_fn)\n",
    "    \n",
    "    #Create a dataloader for EHR data\n",
    "    train_loader_codes = create_dataloaders(train_EHR_codes, train_outcomes, train_lengths, bs)\n",
    "\n",
    "    if feature_types != 'EHR':\n",
    "        data_set_test = DataBuilder(test_proteomics, test_outcomes, scaler)\n",
    "        test_loader_proteomics = DataLoader(dataset=data_set_test,batch_size=bs, worker_init_fn=worker_init_fn)\n",
    "    test_loader_codes = create_dataloaders(test_EHR_codes, test_outcomes, test_lengths, bs)\n",
    "\n",
    "    if feature_types != 'EHR':\n",
    "        data_set_val = DataBuilder(val_proteomics, val_outcomes, scaler)\n",
    "        val_loader_proteomics = DataLoader(dataset=data_set_val,batch_size=100*bs, worker_init_fn=worker_init_fn)\n",
    "    val_loader_codes = create_dataloaders(val_EHR_codes, val_outcomes, val_lengths, 100*bs)\n",
    "            \n",
    "    all_loader_codes = create_dataloaders(all_EHR_codes, all_outcomes, all_lengths, 1000)   \n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    #For baseline experiments, initialize the correct type of model based on the features used\n",
    "    if model_path == '':\n",
    "        if feature_types == 'EHR':\n",
    "            model = GRUNet(EHR_codes.shape[2], hidden_dim, prediction_module_hidden_sizes, num_layers, 1, dropout).to(device)\n",
    "        elif feature_types == 'metab':\n",
    "            model = proteomics_net(proteomics.shape[1], None, 1, dropout).to(device)\n",
    "        elif feature_types == 'both':\n",
    "            model = joint_model(EHR_codes.shape[2], hidden_dim, prediction_module_hidden_sizes, num_layers, 1, proteomics.shape[1], [1024, 512, 256, 128], [64, 32, 16, 8], dropout).to(device)\n",
    "    #For COMET experiments\n",
    "    else:\n",
    "        #Load correct model architecture based on whether or not we use omics data\n",
    "        if feature_types == 'EHR':\n",
    "            model = GRUNet(EHR_codes.shape[2], hidden_dim, prediction_module_hidden_sizes, num_layers, 1, dropout)\n",
    "            model_state_dict = torch.load(model_path)\n",
    "            model.load_state_dict(model_state_dict)\n",
    "            model.to(device)\n",
    "            #If no fine tuning, that means we are assessing the pre-trained model on the omics data with no additional training\n",
    "            if fine_tune == False:\n",
    "                model.eval()\n",
    "                criterion = nn.MSELoss()\n",
    "                val_predictions = []\n",
    "                val_true_labels = []\n",
    "                running_loss_val, num_samples_val = 0, 0\n",
    "                with torch.no_grad():\n",
    "                    for (inputs_codes, labels_codes, lengths_codes) in (val_loader_codes):\n",
    "                            inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                            outputs = model(inputs_codes, lengths_codes)\n",
    "                            \n",
    "                            mean_tensor = torch.tensor(mean_val_outcomes, dtype=torch.float32, device=device)\n",
    "                            std_tensor = torch.tensor(sd_val_outcomes, dtype=torch.float32, device=device)\n",
    "\n",
    "                            denormalized_outputs = outputs.squeeze() * std_tensor + mean_tensor\n",
    "                            denormalized_labels = labels * std_tensor + mean_tensor\n",
    "                \n",
    "                            loss = criterion(denormalized_outputs, denormalized_labels)\n",
    "#                             loss = criterion(outputs.squeeze(), labels)\n",
    "                            running_loss_val += (loss.item()*lengths_codes.shape[0])\n",
    "                            num_samples_val += lengths_codes.shape[0]\n",
    "                            val_predictions.extend(outputs.squeeze().tolist())\n",
    "                            val_true_labels.extend(labels.tolist())\n",
    "                    \n",
    "                val_loss = running_loss_val / (num_samples_val)\n",
    "                pearson_corr, _ = pearsonr(val_predictions, val_true_labels)\n",
    "                original_val_outcomes = outcomes[val_indices]\n",
    "                val_outcome_mean, val_outcome_sd = np.mean(original_val_outcomes), np.std(original_val_outcomes)\n",
    "                val_rmse = np.sqrt(mean_squared_error(original_val_outcomes, np.array(val_predictions)*val_outcome_sd+val_outcome_mean))\n",
    "\n",
    "                print(f'Total Loss: {val_loss:.4f}, Pearson R: {pearson_corr:.4f}, RMSE: {val_rmse:.4f}')\n",
    "                if return_preds == True:\n",
    "                    return pearson_corr, val_loss, val_rmse, val_true_labels, val_predictions, val_indices\n",
    "                else:\n",
    "                    return pearson_corr, val_loss, val_rmse\n",
    "            #if we are fine-tuning, freeze the GRU weights\n",
    "            elif fine_tune == True:\n",
    "                model = GRUNet(EHR_codes.shape[2], hidden_dim, prediction_module_hidden_sizes, num_layers, 1, dropout)\n",
    "                model_state_dict = torch.load(model_path)\n",
    "                model.load_state_dict(model_state_dict, strict=False)\n",
    "                model.to(device)\n",
    "                for name, param in model.named_parameters():\n",
    "                    if ('gru' in name):\n",
    "                        param.requires_grad = False\n",
    "        #use the correct model if we include omics data along with the transferred weights from the EHR model\n",
    "        elif feature_types == 'both':\n",
    "            model = joint_model(EHR_codes.shape[2], hidden_dim, prediction_module_hidden_sizes, num_layers, 1, proteomics.shape[1], [1024, 512, 256, 128], [64, 32, 16, 8], dropout).to(device)\n",
    "            model_state_dict = torch.load(model_path)\n",
    "            gru_weights = {}\n",
    "            for k,v in zip(model_state_dict.keys(), model_state_dict.values()):\n",
    "                if ('gru' in k) | ('pred' in k):\n",
    "                    gru_weights[k] = v\n",
    "            model.load_state_dict(gru_weights, strict=False)\n",
    "            model.to(device)\n",
    "            \n",
    "            for name, param in model.named_parameters():\n",
    "                if ('gru' in name):\n",
    "                    param.requires_grad = False\n",
    "    \n",
    "\n",
    "    #prepare to track relevant info during training\n",
    "    epoch_arr = []\n",
    "    num_samples_in_batch = []\n",
    "    gradient_arr = []\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=lr_decay)\n",
    "    num_epochs = 100\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    val_losses = []\n",
    "    best_loss = np.inf\n",
    "    #training loop\n",
    "    for epoch in (range(num_epochs)):\n",
    "        model.train()\n",
    "        #if we're not doing hyperparameter tuning, save the model at each epoch for downstream analysis\n",
    "        if hyperparam_tuning == False:\n",
    "            torch.save(model.state_dict(), './models/predictive_models/{}_epoch{}.pth'.format(experiment_name, epoch))\n",
    "        running_loss_train, num_train_samples = 0, 0\n",
    "        if feature_types == 'EHR':\n",
    "            for (inputs_codes, labels_codes, lengths_codes)in train_loader_codes:\n",
    "                if feature_types != 'metab':\n",
    "                    inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "\n",
    "                outputs = model(inputs_codes, lengths_codes)\n",
    "\n",
    "\n",
    "                mean_tensor = torch.tensor(mean_train_outcomes, dtype=torch.float32, device=device)\n",
    "                std_tensor = torch.tensor(sd_train_outcomes, dtype=torch.float32, device=device)\n",
    "\n",
    "                denormalized_outputs = outputs.squeeze() * std_tensor + mean_tensor\n",
    "                denormalized_labels = labels * std_tensor + mean_tensor\n",
    "\n",
    "                loss = criterion(denormalized_outputs, denormalized_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss_train += (loss.item()*lengths_codes.shape[0])\n",
    "                num_train_samples += lengths_codes.shape[0]\n",
    "        else:\n",
    "            for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(train_loader_codes, train_loader_proteomics)):\n",
    "                if feature_types != 'metab':\n",
    "                    inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                if feature_types != 'EHR':\n",
    "                    inputs_proteomics, labels = inputs_proteomics.to(device), labels_proteomics.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                if feature_types == 'metab':\n",
    "                    outputs = model(inputs_proteomics)\n",
    "                elif feature_types == 'EHR':\n",
    "                    outputs = model(inputs_codes, lengths_codes)\n",
    "                elif feature_types == 'both':\n",
    "                    outputs, interpretability_outputs = model(inputs_codes, inputs_proteomics, lengths_codes, interpretability=True)\n",
    "\n",
    "\n",
    "                mean_tensor = torch.tensor(mean_train_outcomes, dtype=torch.float32, device=device)\n",
    "                std_tensor = torch.tensor(sd_train_outcomes, dtype=torch.float32, device=device)\n",
    "\n",
    "                denormalized_outputs = outputs.squeeze() * std_tensor + mean_tensor\n",
    "                denormalized_labels = labels * std_tensor + mean_tensor\n",
    "\n",
    "                loss = criterion(denormalized_outputs, denormalized_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if return_grads:\n",
    "                    epoch_arr.append(epoch)\n",
    "                    num_samples_in_batch.append(outputs.shape[0])\n",
    "                    gradient_arr.append(model.skip_connect_prot.weight.grad.cpu().numpy()[0])\n",
    "                running_loss_train += (loss.item()*lengths_codes.shape[0])\n",
    "                num_train_samples += lengths_codes.shape[0]\n",
    "\n",
    "        train_loss = running_loss_train / num_train_samples\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        running_loss_test, num_samples_test = 0, 0\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        if feature_types == 'EHR':\n",
    "            with torch.no_grad():\n",
    "                for (inputs_codes, labels_codes, lengths_codes)in test_loader_codes:\n",
    "                    inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "\n",
    "                    outputs = model(inputs_codes, lengths_codes)\n",
    "                    \n",
    "                    mean_tensor = torch.tensor(mean_test_outcomes, dtype=torch.float32, device=device)\n",
    "                    std_tensor = torch.tensor(sd_test_outcomes, dtype=torch.float32, device=device)\n",
    "\n",
    "                    denormalized_outputs = outputs.squeeze() * std_tensor + mean_tensor\n",
    "                    denormalized_labels = labels * std_tensor + mean_tensor\n",
    "                   \n",
    "                    loss = criterion(denormalized_outputs, denormalized_labels)\n",
    "                    running_loss_test += (loss.item()*lengths_codes.shape[0])\n",
    "                    num_samples_test += lengths_codes.shape[0]\n",
    "                    predictions.extend(outputs.squeeze().tolist())\n",
    "                    true_labels.extend(labels.tolist())\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(test_loader_codes, test_loader_proteomics)):\n",
    "                    if feature_types != 'metab':\n",
    "                        inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                    if feature_types != 'EHR':\n",
    "                        inputs_proteomics, labels = inputs_proteomics.to(device), labels_proteomics.to(device)\n",
    "\n",
    "                    if feature_types == 'metab':\n",
    "                        outputs = model(inputs_proteomics)\n",
    "                    elif feature_types == 'EHR':\n",
    "                        outputs = model(inputs_codes, lengths_codes)\n",
    "                    elif feature_types == 'both':\n",
    "                        outputs = model(inputs_codes, inputs_proteomics, lengths_codes)\n",
    "                    \n",
    "                    mean_tensor = torch.tensor(mean_test_outcomes, dtype=torch.float32, device=device)\n",
    "                    std_tensor = torch.tensor(sd_test_outcomes, dtype=torch.float32, device=device)\n",
    "\n",
    "                    denormalized_outputs = outputs.squeeze() * std_tensor + mean_tensor\n",
    "                    denormalized_labels = labels * std_tensor + mean_tensor\n",
    "                    \n",
    "                    loss = criterion(denormalized_outputs, denormalized_labels)\n",
    "                    running_loss_test += (loss.item()*lengths_codes.shape[0])\n",
    "                    num_samples_test += lengths_codes.shape[0]\n",
    "                    predictions.extend(outputs.squeeze().tolist())\n",
    "                    true_labels.extend(labels.tolist())\n",
    "        test_loss = running_loss_test / (num_samples_test)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        patience = 5\n",
    "        pearson_corr, _ = pearsonr(predictions, true_labels)\n",
    "\n",
    "#         print(f'Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Pearson R: {pearson_corr:.4f}')\n",
    "        #check early stopping criteria\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            counter = 0\n",
    "            torch.save(model.state_dict(), './models/predictive_models/{}.pth'.format(experiment_name))\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                break\n",
    "\n",
    "    #when training stops (either due to max epochs or early stopping), load most recent best model per test loss\n",
    "    model.load_state_dict(torch.load('./models/predictive_models/{}.pth'.format(experiment_name)))\n",
    "    running_loss_val, num_samples_val = 0, 0\n",
    "    val_predictions = []\n",
    "    val_true_labels = []\n",
    "    #evaluate model on validation data\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        if feature_types == 'EHR':\n",
    "            for (inputs_codes, labels_codes, lengths_codes) in val_loader_codes:\n",
    "                if feature_types != 'metab':\n",
    "                    inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                if feature_types != 'EHR':\n",
    "                    inputs_proteomics, labels = inputs_proteomics.to(device), labels_proteomics.to(device)\n",
    "\n",
    "                if feature_types == 'metab':\n",
    "                    outputs = model(inputs_proteomics)\n",
    "                elif feature_types == 'EHR':\n",
    "                    outputs = model(inputs_codes, lengths_codes)\n",
    "                elif feature_types == 'both':\n",
    "                    outputs, interpretability_outputs = model(inputs_codes, inputs_proteomics, lengths_codes, interpretability=True)\n",
    "                    \n",
    "                mean_tensor = torch.tensor(mean_val_outcomes, dtype=torch.float32, device=device)\n",
    "                std_tensor = torch.tensor(sd_val_outcomes, dtype=torch.float32, device=device)\n",
    "\n",
    "                denormalized_outputs = outputs.squeeze() * std_tensor + mean_tensor\n",
    "                denormalized_labels = labels * std_tensor + mean_tensor\n",
    "\n",
    "                loss = criterion(denormalized_outputs, denormalized_labels)\n",
    "                running_loss_val += (loss.item()*lengths_codes.shape[0])\n",
    "                num_samples_val += lengths_codes.shape[0]\n",
    "                val_predictions.extend(outputs.squeeze().tolist())\n",
    "                val_true_labels.extend(labels.tolist())\n",
    "        else:\n",
    "            for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(val_loader_codes, val_loader_proteomics)):\n",
    "                if feature_types != 'metab':\n",
    "                    inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                if feature_types != 'EHR':\n",
    "                    inputs_proteomics, labels = inputs_proteomics.to(device), labels_proteomics.to(device)\n",
    "\n",
    "                if feature_types == 'metab':\n",
    "                    outputs = model(inputs_proteomics)\n",
    "                elif feature_types == 'EHR':\n",
    "                    outputs = model(inputs_codes, lengths_codes)\n",
    "                elif feature_types == 'both':\n",
    "                    outputs, interpretability_outputs = model(inputs_codes, inputs_proteomics, lengths_codes, interpretability=True)\n",
    "\n",
    "                mean_tensor = torch.tensor(mean_val_outcomes, dtype=torch.float32, device=device)\n",
    "                std_tensor = torch.tensor(sd_val_outcomes, dtype=torch.float32, device=device)\n",
    "\n",
    "                denormalized_outputs = outputs.squeeze() * std_tensor + mean_tensor\n",
    "                denormalized_labels = labels * std_tensor + mean_tensor\n",
    "\n",
    "                loss = criterion(denormalized_outputs, denormalized_labels)\n",
    "                running_loss_val += (loss.item()*lengths_codes.shape[0])\n",
    "                num_samples_val += lengths_codes.shape[0]\n",
    "                val_predictions.extend(outputs.squeeze().tolist())\n",
    "                val_true_labels.extend(labels.tolist())\n",
    "    val_loss = running_loss_val / (num_samples_val)\n",
    "    val_losses.append(val_loss)\n",
    "    pearson_corr, _ = pearsonr(val_predictions, val_true_labels)\n",
    "    original_val_outcomes = outcomes[val_indices]\n",
    "    val_outcome_mean, val_outcome_sd = np.mean(original_val_outcomes), np.std(original_val_outcomes)\n",
    "    val_rmse = np.sqrt(mean_squared_error(original_val_outcomes, np.array(val_predictions)*val_outcome_sd+val_outcome_mean))\n",
    "\n",
    "    print(f'Epoch: {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Pearson R: {pearson_corr:.4f}, RMSE: {val_rmse:.4f}')\n",
    "    \n",
    "    if hyperparam_tuning == True:\n",
    "        os.remove('./models/predictive_models/{}.pth'.format(experiment_name))\n",
    "    if return_preds == True:\n",
    "        if return_interpretability == False:\n",
    "            if return_grads:\n",
    "                df = pd.DataFrame([epoch_arr, num_samples_in_batch, gradient_arr]).T\n",
    "                df.columns = ['epoch', 'num_samples_in_batch','gradient']\n",
    "                return pearson_corr, val_loss, val_rmse, original_val_outcomes, np.array(val_predictions)*val_outcome_sd+val_outcome_mean, val_indices, df, train_losses, test_losses, val_losses, train_indices, test_indices\n",
    "            else:\n",
    "                return pearson_corr, val_loss, val_rmse, original_val_outcomes, np.array(val_predictions)*val_outcome_sd+val_outcome_mean, val_indices\n",
    "        else:\n",
    "            if return_grads:\n",
    "                df = pd.DataFrame([epoch_arr, num_samples_in_batch, gradient_arr]).T\n",
    "                df.columns = ['epoch', 'num_samples_in_batch','gradient']\n",
    "                return pearson_corr, val_loss, val_rmse, original_val_outcomes, np.array(val_predictions)*val_outcome_sd+val_outcome_mean, val_indices, interpretability_outputs, val_outcome_mean, val_outcome_sd, df, train_losses, test_losses, val_losses, train_indices, test_indices\n",
    "            else:\n",
    "                return pearson_corr, val_loss, val_rmse, original_val_outcomes, np.array(val_predictions)*val_outcome_sd+val_outcome_mean, val_indices, interpretability_outputs, val_outcome_mean, val_outcome_sd\n",
    "    else:\n",
    "        return pearson_corr, val_loss, val_rmse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load omics data with the omics word2vec model\n",
    "RNN_data_codes_OOL = np.load('./data/processed_data/RNN_data_codes_with_obs_word2vec_from_ool.npy')\n",
    "RNN_data_outcomes_OOL = np.load('./data/processed_data/RNN_data_outcomes_with_obs_word2vec_from_ool.npy')\n",
    "RNN_data_lengths_OOL = np.load('./data/processed_data/RNN_data_lengths_with_obs_word2vec_from_ool.npy')\n",
    "patient_indices_OOL = pd.read_csv('./data/processed_data/sampleID_indices_with_obs_word2vec_from_ool.csv').drop('Unnamed: 0',axis=1)\n",
    "patient_indices_OOL.columns = ['sample_ID','array_index']\n",
    "RNN_data_codes_OOL.shape, RNN_data_outcomes_OOL.shape, RNN_data_lengths_OOL.shape, patient_indices_OOL.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load omics data with the pre-trained word2vec model\n",
    "RNN_data_codes = np.load('./data/processed_data/RNN_data_codes_with_obs.npy')\n",
    "RNN_data_outcomes = np.load('./data/processed_data/RNN_data_outcomes_with_obs.npy')\n",
    "RNN_data_lengths = np.load('./data/processed_data/RNN_data_lengths_with_obs.npy')\n",
    "patient_indices = pd.read_csv('./data/processed_data/sampleID_indices_with_obs.csv').drop('Unnamed: 0',axis=1)\n",
    "patient_indices.columns = ['sample_ID','array_index']\n",
    "RNN_data_codes.shape, RNN_data_outcomes.shape, RNN_data_lengths.shape, patient_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load pre-training cohort data\n",
    "RNN_data_codes_PTMODEL = np.load('./data/processed_data/RNN_data_full_EHR_cohort_with_obs_fixed.npy')\n",
    "RNN_data_outcomes_PTMODEL = np.load('./data/processed_data/RNN_data_outcomes_full_EHR_cohort_with_obs_fixed.npy')\n",
    "RNN_data_lengths_PTMODEL = np.load('./data/processed_data/RNN_data_lengths_full_EHR_cohort_with_obs_fixed.npy')\n",
    "patient_indices_PTMODEL = pd.read_csv('./data/processed_data/sampleID_indices_full_cohort_with_obs_fixed.csv').drop('Unnamed: 0',axis=1)\n",
    "patient_indices_PTMODEL.columns = ['sample_ID','array_index']\n",
    "RNN_data_codes_PTMODEL.shape, RNN_data_outcomes_PTMODEL.shape, RNN_data_lengths_PTMODEL.shape, patient_indices_PTMODEL.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_best_params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_list = ['DOS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRIALS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our proteomics data has 12 proteins which are used for QC, all labeled with \"HCE\"\n",
    "#we exclude these proteins from our analysis\n",
    "columns_to_drop = [col for col in OOL_proteomics.columns if \"HCE\" in col]\n",
    "OOL_proteomics = OOL_proteomics.drop(columns=columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter selection for pre-training\n",
    "\n",
    "# Hyperparameter grid\n",
    "grid_search = {\n",
    "    'batch_size': [512],\n",
    "    'lr': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "    'dropout': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'lr_decay': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "    'layers': [2, 4],\n",
    "    'hidden_dim': [400, 800]\n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "all_params = [dict(zip(grid_search.keys(), values)) for values in product(*grid_search.values())]\n",
    "\n",
    "num_layers_arr = []\n",
    "dropout_arr = []\n",
    "lr_arr = []\n",
    "lr_decay_arr = []\n",
    "hidden_dim_arr = []\n",
    "batch_size_arr = []\n",
    "split_num_arr = []\n",
    "loss_arr = []\n",
    "for i in tqdm(outcome_list):\n",
    "    overall_best_params[i] = {'PT': {}}\n",
    "\n",
    "    maternal_IDs = patient_indices_PTMODEL['sample_ID'].str[0:7].unique()\n",
    "    \n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    split_num = 0\n",
    "    for train_index, test_index in kf.split(maternal_IDs):\n",
    "        split_num += 1\n",
    "        results_dict = {}\n",
    "        train_IDs = maternal_IDs[train_index]\n",
    "        test_IDs = maternal_IDs[test_index]\n",
    "        \n",
    "        sample_size = int(0.2 * len(train_index))\n",
    "        random_indices = np.random.choice(train_IDs.shape[0], sample_size, replace=False)\n",
    "        val_IDs = train_IDs[random_indices]\n",
    "        train_IDs = np.delete(train_IDs, random_indices)\n",
    "        \n",
    "        patient_indices_PTMODEL['maternal_ID'] = patient_indices_PTMODEL['sample_ID'].str[0:7]\n",
    "\n",
    "        train_indices = patient_indices_PTMODEL[patient_indices_PTMODEL['maternal_ID'].isin(train_IDs)]['array_index'].values\n",
    "        np.random.shuffle(train_indices)\n",
    "        test_indices = patient_indices_PTMODEL[patient_indices_PTMODEL['maternal_ID'].isin(test_IDs)]['array_index'].values\n",
    "        val_indices = patient_indices_PTMODEL[patient_indices_PTMODEL['maternal_ID'].isin(val_IDs)]['array_index'].values\n",
    "\n",
    "        # Iterate through the hyperparameter combinations\n",
    "        for param_set in tqdm(all_params):\n",
    "            bs = param_set['batch_size']\n",
    "            lr = param_set['lr']\n",
    "            dropout = param_set['dropout']\n",
    "            lr_decay = param_set['lr_decay']\n",
    "            layers = param_set['layers']\n",
    "            hidden_dim = param_set['hidden_dim']\n",
    "            \n",
    "            model_name = 'PT_MODEL_{}_{}_{}_{}_{}_noHCE_with_obs'.format(layers,lr,lr_decay,dropout, hidden_dim)\n",
    "            print(param_set)\n",
    "            val_r, val_loss, val_rmse = run_experiment(RNN_data_codes_PTMODEL, OOL_proteomics,\n",
    "            patient_indices_PTMODEL, RNN_data_outcomes_PTMODEL, RNN_data_lengths_PTMODEL, model_name, \n",
    "            lr, lr_decay, bs, train_indices=train_indices, test_indices=test_indices,\n",
    "                                                       val_indices=val_indices,feature_types='EHR', model_path='', fine_tune=False, seed=42, \n",
    "            hidden_dim=hidden_dim, num_layers=layers, dropout=dropout, hyperparam_tuning=True)\n",
    "            num_layers_arr.append(layers)\n",
    "            dropout_arr.append(dropout)\n",
    "            lr_arr.append(lr)\n",
    "            lr_decay_arr.append(lr_decay)\n",
    "            hidden_dim_arr.append(hidden_dim)\n",
    "            batch_size_arr.append(bs)\n",
    "            split_num_arr.append(split_num)\n",
    "            loss_arr.append(val_loss)\n",
    "\n",
    "        print('PT')\n",
    "        print('outcome {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_df = pd.DataFrame([num_layers_arr, dropout_arr, lr_arr, lr_decay_arr, hidden_dim_arr, batch_size_arr, split_num_arr,loss_arr]).T\n",
    "hyperparam_df.columns = ['num_layers','dropout','lr','lr_decay','hidden_dim','bs','split_num','val_loss']\n",
    "hyperparam_df = hyperparam_df.groupby(['num_layers','dropout','lr','lr_decay','hidden_dim','bs']).mean()\n",
    "num_layers, dropout, lr, lr_decay, hidden_dim, bs = hyperparam_df['val_loss'].idxmin()\n",
    "print(np.min(hyperparam_df['val_loss']))\n",
    "model_name = 'PT_MODEL_{}_{}_{}_{}_{}_noHCE_with_obs'.format(num_layers,lr,lr_decay,dropout, hidden_dim)\n",
    "overall_best_params['DOS']['PT'] = {'num_layers': int(num_layers),'lr': lr,'lr_decay': lr_decay, 'dropout': dropout,\n",
    "                                'hidden_dim': int(hidden_dim), 'batch_size': int(bs), 'model_name': model_name}\n",
    "with open(\"./models/hyperparameters/best_hyperparams_OOL_noHCE_with_obs.pkl\", \"wb\") as f:\n",
    "        pickle.dump(overall_best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_num_layers = overall_best_params['DOS']['PT']['num_layers']\n",
    "best_dropout = overall_best_params['DOS']['PT']['dropout']\n",
    "best_model_name = overall_best_params['DOS']['PT']['model_name']\n",
    "best_hidden_dim = overall_best_params['DOS']['PT']['hidden_dim']\n",
    "best_num_layers, best_dropout, best_hidden_dim, best_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create PT model\n",
    "val_r, val_loss, val_rmse = run_experiment(RNN_data_codes_PTMODEL, OOL_proteomics,\n",
    "    patient_indices_PTMODEL, RNN_data_outcomes_PTMODEL, RNN_data_lengths_PTMODEL, best_model_name, \n",
    "    overall_best_params['DOS']['PT']['lr'], overall_best_params['DOS']['PT']['lr_decay'],\n",
    "       overall_best_params['DOS']['PT']['batch_size'], feature_types='EHR', model_path='', fine_tune=False, seed=42, \n",
    "    hidden_dim=best_hidden_dim, num_layers=best_num_layers, dropout=best_dropout, hyperparam_tuning=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid for EHR baseline hyperparam search\n",
    "grid_search = {\n",
    "    'batch_size': [16],\n",
    "    'lr': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "    'dropout': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'lr_decay': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "    'layers': [2, 4],\n",
    "    'hidden_dim': [400, 800]\n",
    "}\n",
    "all_params = [dict(zip(grid_search.keys(), values)) for values in product(*grid_search.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter search for EHR baseline\n",
    "num_layers_arr = []\n",
    "dropout_arr = []\n",
    "lr_arr = []\n",
    "lr_decay_arr = []\n",
    "hidden_dim_arr = []\n",
    "batch_size_arr = []\n",
    "split_num_arr = []\n",
    "loss_arr = []\n",
    "for i in tqdm(outcome_list):\n",
    "    overall_best_params[i]['exp1'] = {}\n",
    "\n",
    "    maternal_IDs = patient_indices_OOL['sample_ID'].str[0:7].unique()\n",
    "    \n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    split_num = 0\n",
    "    for train_index, test_index in kf.split(maternal_IDs):\n",
    "        split_num += 1\n",
    "        results_dict = {}\n",
    "        train_IDs = maternal_IDs[train_index]\n",
    "        test_IDs = maternal_IDs[test_index]\n",
    "        \n",
    "        sample_size = int(0.2 * len(train_index))\n",
    "        random_indices = np.random.choice(train_IDs.shape[0], sample_size, replace=False)\n",
    "        val_IDs = train_IDs[random_indices]\n",
    "        train_IDs = np.delete(train_IDs, random_indices)\n",
    "        \n",
    "        patient_indices_OOL['maternal_ID'] = patient_indices_OOL['sample_ID'].str[0:7]\n",
    "        patient_indices_OOL['maternal_ID_ts'] = patient_indices_OOL['maternal_ID'].astype(str)+'_'+patient_indices_OOL['sample_ID'].str[-2:]\n",
    "        input_OOL_proteomics = OOL_proteomics.merge(patient_indices_OOL[['maternal_ID_ts','array_index']], how='left', left_on='sample_ID', right_on='maternal_ID_ts').drop(['sample_ID','maternal_ID_ts','DOS'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "\n",
    "        train_indices = patient_indices_OOL[patient_indices_OOL['maternal_ID'].isin(train_IDs)]['array_index'].values\n",
    "        np.random.shuffle(train_indices)\n",
    "        test_indices = patient_indices_OOL[patient_indices_OOL['maternal_ID'].isin(test_IDs)]['array_index'].values\n",
    "        val_indices = patient_indices_OOL[patient_indices_OOL['maternal_ID'].isin(val_IDs)]['array_index'].values\n",
    "\n",
    "        # Iterate through the hyperparameter combinations\n",
    "        for param_set in tqdm(all_params):\n",
    "            bs = param_set['batch_size']\n",
    "            lr = param_set['lr']\n",
    "            dropout = param_set['dropout']\n",
    "            lr_decay = param_set['lr_decay']\n",
    "            layers = param_set['layers']\n",
    "            hidden_dim = param_set['hidden_dim']\n",
    "            print(param_set)\n",
    "            val_r, val_loss, val_rmse = run_experiment(RNN_data_codes_OOL, input_OOL_proteomics,\n",
    "                                                       patient_indices_OOL, RNN_data_outcomes_OOL,\n",
    "                                                       RNN_data_lengths_OOL, 'EHR_OOL_only_noHCE_with_obs',\n",
    "                                                       lr, lr_decay, bs, \n",
    "                                                       train_indices=train_indices, test_indices=test_indices,\n",
    "                                                       val_indices=val_indices, feature_types='EHR', model_path='',\n",
    "                                                       fine_tune=False, seed=42, hidden_dim=hidden_dim,\n",
    "                                                       num_layers=layers, dropout=dropout, hyperparam_tuning=True)\n",
    "\n",
    "            num_layers_arr.append(layers)\n",
    "            dropout_arr.append(dropout)\n",
    "            lr_arr.append(lr)\n",
    "            lr_decay_arr.append(lr_decay)\n",
    "            hidden_dim_arr.append(hidden_dim)\n",
    "            batch_size_arr.append(bs)\n",
    "            split_num_arr.append(split_num)\n",
    "            loss_arr.append(val_loss)\n",
    "\n",
    "        print('experiment 1')\n",
    "        print('outcome {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_df = pd.DataFrame([num_layers_arr, dropout_arr, lr_arr, lr_decay_arr, hidden_dim_arr, batch_size_arr, split_num_arr,loss_arr]).T\n",
    "hyperparam_df.columns = ['num_layers','dropout','lr','lr_decay','hidden_dim','bs','split_num','val_loss']\n",
    "hyperparam_df = hyperparam_df.groupby(['num_layers','dropout','lr','lr_decay','hidden_dim','bs']).mean()\n",
    "num_layers, dropout, lr, lr_decay, hidden_dim, bs = hyperparam_df['val_loss'].idxmin()\n",
    "print(np.min(hyperparam_df['val_loss']))\n",
    "\n",
    "print(num_layers, dropout, lr, lr_decay, hidden_dim, bs)\n",
    "\n",
    "overall_best_params['DOS']['exp1'] = {'num_layers': int(num_layers),'lr': lr,'lr_decay': lr_decay, 'dropout': dropout,\n",
    "                                'hidden_dim': int(hidden_dim), 'batch_size': int(bs)}\n",
    "with open(\"./models/hyperparameters/best_hyperparams_OOL_noHCE_with_obs.pkl\", \"wb\") as f:\n",
    "        pickle.dump(overall_best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_num_layers_OOL = overall_best_params['DOS']['exp1']['num_layers']\n",
    "best_dropout_OOL = overall_best_params['DOS']['exp1']['dropout']\n",
    "best_hidden_dim_OOL = overall_best_params['DOS']['exp1']['hidden_dim']\n",
    "best_num_layers_OOL, best_dropout_OOL, best_hidden_dim_OOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid for omics baseline hyperparam search\n",
    "grid_search = {'batch_size': [16],\n",
    "              'lr': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "              'lr_decay': [1e-1, 1e-2, 1e-3, 1e-4]}\n",
    "\n",
    "all_params = [dict(zip(grid_search.keys(), values)) for values in product(*grid_search.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform omics baseline hyperparam search\n",
    "num_layers_arr = []\n",
    "dropout_arr = []\n",
    "lr_arr = []\n",
    "lr_decay_arr = []\n",
    "hidden_dim_arr = []\n",
    "batch_size_arr = []\n",
    "split_num_arr = []\n",
    "loss_arr = []\n",
    "for i in tqdm(outcome_list):\n",
    "    overall_best_params[i]['exp2'] = {}\n",
    "\n",
    "    maternal_IDs = patient_indices_OOL['sample_ID'].str[0:7].unique()\n",
    "    \n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    split_num = 0\n",
    "    for train_index, test_index in kf.split(maternal_IDs):\n",
    "        split_num += 1\n",
    "        results_dict = {}\n",
    "        train_IDs = maternal_IDs[train_index]\n",
    "        test_IDs = maternal_IDs[test_index]\n",
    "        \n",
    "        sample_size = int(0.2 * len(train_index))\n",
    "        random_indices = np.random.choice(train_IDs.shape[0], sample_size, replace=False)\n",
    "        val_IDs = train_IDs[random_indices]\n",
    "        train_IDs = np.delete(train_IDs, random_indices)\n",
    "        \n",
    "        patient_indices_OOL['maternal_ID'] = patient_indices_OOL['sample_ID'].str[0:7]\n",
    "        patient_indices_OOL['maternal_ID_ts'] = patient_indices_OOL['maternal_ID'].astype(str)+'_'+patient_indices_OOL['sample_ID'].str[-2:]\n",
    "        input_OOL_proteomics = OOL_proteomics.merge(patient_indices_OOL[['maternal_ID_ts','array_index']], how='left', left_on='sample_ID', right_on='maternal_ID_ts').drop(['sample_ID','maternal_ID_ts','DOS'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "\n",
    "        train_indices = patient_indices_OOL[patient_indices_OOL['maternal_ID'].isin(train_IDs)]['array_index'].values\n",
    "        np.random.shuffle(train_indices)\n",
    "        test_indices = patient_indices_OOL[patient_indices_OOL['maternal_ID'].isin(test_IDs)]['array_index'].values\n",
    "        val_indices = patient_indices_OOL[patient_indices_OOL['maternal_ID'].isin(val_IDs)]['array_index'].values\n",
    "\n",
    "        # Iterate through the hyperparameter combinations\n",
    "        for param_set in tqdm(all_params):\n",
    "            bs = param_set['batch_size']\n",
    "            lr = param_set['lr']\n",
    "            dropout = best_dropout_OOL\n",
    "            lr_decay = param_set['lr_decay']\n",
    "            layers = best_num_layers_OOL\n",
    "            hidden_dim = best_hidden_dim_OOL\n",
    "            print(param_set)\n",
    "            val_r, val_loss, val_rmse = run_experiment(RNN_data_codes_OOL, input_OOL_proteomics,\n",
    "                patient_indices_OOL, RNN_data_outcomes_OOL, RNN_data_lengths_OOL, 'proteomics_OOL_only_noHCE_with_obs', \n",
    "                lr, lr_decay, bs, train_indices=train_indices, test_indices=test_indices, val_indices=val_indices,\n",
    "                feature_types='metab', model_path='', fine_tune=False, seed=42, hidden_dim=hidden_dim,\n",
    "                num_layers=layers, dropout=dropout, hyperparam_tuning=True)\n",
    "            \n",
    "            num_layers_arr.append(layers)\n",
    "            dropout_arr.append(dropout)\n",
    "            lr_arr.append(lr)\n",
    "            lr_decay_arr.append(lr_decay)\n",
    "            hidden_dim_arr.append(hidden_dim)\n",
    "            batch_size_arr.append(bs)\n",
    "            split_num_arr.append(split_num)\n",
    "            loss_arr.append(val_loss)\n",
    "\n",
    "        print('experiment 2')\n",
    "        print('outcome {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_df = pd.DataFrame([num_layers_arr, dropout_arr, lr_arr, lr_decay_arr, hidden_dim_arr, batch_size_arr, split_num_arr,loss_arr]).T\n",
    "hyperparam_df.columns = ['num_layers','dropout','lr','lr_decay','hidden_dim','bs','split_num','val_loss']\n",
    "hyperparam_df = hyperparam_df.groupby(['num_layers','dropout','lr','lr_decay','hidden_dim','bs']).mean()\n",
    "num_layers, dropout, lr, lr_decay, hidden_dim, bs = hyperparam_df['val_loss'].idxmin()\n",
    "print(np.min(hyperparam_df['val_loss']))\n",
    "print(num_layers, dropout, lr, lr_decay, hidden_dim, bs)\n",
    "\n",
    "overall_best_params['DOS']['exp2'] = {'num_layers': int(num_layers),'lr': lr,'lr_decay': lr_decay, 'dropout': dropout,\n",
    "                                'hidden_dim': int(hidden_dim), 'batch_size': int(bs)}\n",
    "with open(\"./models/hyperparameters/best_hyperparams_OOL_noHCE_with_obs.pkl\", \"wb\") as f:\n",
    "        pickle.dump(overall_best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid for joint baseline hyperparam search\n",
    "grid_search = {'batch_size': [16],\n",
    "              'lr': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "              'dropout': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "              'lr_decay': [1e-1, 1e-2, 1e-3, 1e-4]}\n",
    "\n",
    "all_params = [dict(zip(grid_search.keys(), values)) for values in product(*grid_search.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joint baseline hyperparam search\n",
    "\n",
    "num_layers_arr = []\n",
    "dropout_arr = []\n",
    "lr_arr = []\n",
    "lr_decay_arr = []\n",
    "hidden_dim_arr = []\n",
    "batch_size_arr = []\n",
    "split_num_arr = []\n",
    "loss_arr = []\n",
    "for i in tqdm(outcome_list):\n",
    "    overall_best_params[i]['exp3'] = {}\n",
    "\n",
    "    maternal_IDs = patient_indices_OOL['sample_ID'].str[0:7].unique()\n",
    "    \n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    split_num = 0\n",
    "    for train_index, test_index in kf.split(maternal_IDs):\n",
    "        split_num += 1\n",
    "        results_dict = {}\n",
    "        train_IDs = maternal_IDs[train_index]\n",
    "        test_IDs = maternal_IDs[test_index]\n",
    "        \n",
    "        sample_size = int(0.2 * len(train_index))\n",
    "        random_indices = np.random.choice(train_IDs.shape[0], sample_size, replace=False)\n",
    "        val_IDs = train_IDs[random_indices]\n",
    "        train_IDs = np.delete(train_IDs, random_indices)\n",
    "        \n",
    "        patient_indices_OOL['maternal_ID'] = patient_indices_OOL['sample_ID'].str[0:7]\n",
    "        patient_indices_OOL['maternal_ID_ts'] = patient_indices_OOL['maternal_ID'].astype(str)+'_'+patient_indices_OOL['sample_ID'].str[-2:]\n",
    "        input_OOL_proteomics = OOL_proteomics.merge(patient_indices_OOL[['maternal_ID_ts','array_index']], how='left', left_on='sample_ID', right_on='maternal_ID_ts').drop(['sample_ID','maternal_ID_ts','DOS'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "\n",
    "        train_indices = patient_indices_OOL[patient_indices_OOL['maternal_ID'].isin(train_IDs)]['array_index'].values\n",
    "        np.random.shuffle(train_indices)\n",
    "        test_indices = patient_indices_OOL[patient_indices_OOL['maternal_ID'].isin(test_IDs)]['array_index'].values\n",
    "        val_indices = patient_indices_OOL[patient_indices_OOL['maternal_ID'].isin(val_IDs)]['array_index'].values\n",
    "\n",
    "        # Iterate through the hyperparameter combinations\n",
    "        for param_set in tqdm(all_params):\n",
    "            bs = param_set['batch_size']\n",
    "            lr = param_set['lr']\n",
    "            dropout = param_set['dropout']\n",
    "            lr_decay = param_set['lr_decay']\n",
    "            layers = best_num_layers_OOL\n",
    "            hidden_dim = best_hidden_dim_OOL\n",
    "            print(param_set)\n",
    "            \n",
    "            val_r, val_loss, val_rmse = run_experiment(RNN_data_codes_OOL, input_OOL_proteomics,\n",
    "                patient_indices_OOL, RNN_data_outcomes_OOL, RNN_data_lengths_OOL, 'both_OOL_only_noHCE_with_obs', \n",
    "                lr, lr_decay, bs, train_indices=train_indices, test_indices=test_indices,\n",
    "                val_indices=val_indices, feature_types='both', model_path='', fine_tune=False, seed=42,\n",
    "                hidden_dim=hidden_dim, num_layers=layers, dropout=dropout, hyperparam_tuning=True)\n",
    "\n",
    "            num_layers_arr.append(layers)\n",
    "            dropout_arr.append(dropout)\n",
    "            lr_arr.append(lr)\n",
    "            lr_decay_arr.append(lr_decay)\n",
    "            hidden_dim_arr.append(hidden_dim)\n",
    "            batch_size_arr.append(bs)\n",
    "            split_num_arr.append(split_num)\n",
    "            loss_arr.append(val_loss)\n",
    "\n",
    "        print('experiment 3')\n",
    "        print('outcome {}'.format(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_df = pd.DataFrame([num_layers_arr, dropout_arr, lr_arr, lr_decay_arr, hidden_dim_arr, batch_size_arr, split_num_arr,loss_arr]).T\n",
    "hyperparam_df.columns = ['num_layers','dropout','lr','lr_decay','hidden_dim','bs','split_num','val_loss']\n",
    "hyperparam_df = hyperparam_df.groupby(['num_layers','dropout','lr','lr_decay','hidden_dim','bs']).mean()\n",
    "num_layers, dropout, lr, lr_decay, hidden_dim, bs = hyperparam_df['val_loss'].idxmin()\n",
    "print(np.min(hyperparam_df['val_loss']))\n",
    "print(num_layers, dropout, lr, lr_decay, hidden_dim, bs)\n",
    "\n",
    "overall_best_params['DOS']['exp3'] = {'num_layers': int(num_layers),'lr': lr,'lr_decay': lr_decay, 'dropout': dropout,\n",
    "                                'hidden_dim': int(hidden_dim), 'batch_size': int(bs)}\n",
    "with open(\"./models/hyperparameters/best_hyperparams_OOL_noHCE_with_obs.pkl\", \"wb\") as f:\n",
    "        pickle.dump(overall_best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test pre-trained EHR model on omics data\n",
    "\n",
    "for i in tqdm(outcome_list):\n",
    "    overall_best_params[i]['exp4'] = {}\n",
    "    results_dict = {}\n",
    "\n",
    "    # set bs large enough to test all data in one pass\n",
    "    # other params don't matter since there is no training here....they are just set arbitrarily and saved to avoid issues with downstream code\n",
    "    bs = 1000\n",
    "    lr = 0.1\n",
    "    dropout = best_dropout\n",
    "    lr_decay = 0.1\n",
    "    layers = best_num_layers\n",
    "    hidden_dim = best_hidden_dim\n",
    "    print(param_set)\n",
    "    val_r, val_loss, val_rmse = run_experiment(RNN_data_codes, OOL_proteomics,\n",
    "            patient_indices, RNN_data_outcomes, RNN_data_lengths, 'EHR_OOL_PT_noHCE_with_obs', \n",
    "            lr, lr_decay, bs, feature_types='EHR', model_path='./models/predictive_models/{}.pth'.format(best_model_name),\n",
    "                                               fine_tune=False, seed=42,\n",
    "                                               hidden_dim=hidden_dim,\n",
    "                                              num_layers=layers, dropout=dropout, hyperparam_tuning=False)\n",
    "\n",
    "    results_dict[val_loss] = {'num_layers': layers,'lr': lr,'lr_decay': lr_decay, 'dropout': dropout,\n",
    "                            'hidden_dim': hidden_dim, 'batch_size': bs}\n",
    "\n",
    "    print('experiment 4')\n",
    "    print('outcome {}'.format(i))\n",
    "    overall_best_params[i]['exp4'] = results_dict[min(results_dict.keys())]\n",
    "    print(results_dict[min(results_dict.keys())])\n",
    "\n",
    "with open(\"./models/hyperparameters/best_hyperparams_OOL_noHCE_with_obs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(overall_best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid search for EHR fine-tuning experiment (only fine-tuning the EHR pre-trained model)\n",
    "grid_search = {'batch_size': [16],\n",
    "              'lr': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "              'lr_decay': [1e-1, 1e-2, 1e-3, 1e-4]}\n",
    "\n",
    "all_params = [dict(zip(grid_search.keys(), values)) for values in product(*grid_search.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actual hyperparam search\n",
    "\n",
    "num_layers_arr = []\n",
    "dropout_arr = []\n",
    "lr_arr = []\n",
    "lr_decay_arr = []\n",
    "hidden_dim_arr = []\n",
    "batch_size_arr = []\n",
    "split_num_arr = []\n",
    "loss_arr = []\n",
    "for i in tqdm(outcome_list):\n",
    "    overall_best_params[i]['exp5'] = {}\n",
    "\n",
    "    maternal_IDs = patient_indices['sample_ID'].str[0:7].unique()\n",
    "    \n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    split_num = 0\n",
    "    for train_index, test_index in kf.split(maternal_IDs):\n",
    "        split_num += 1\n",
    "        results_dict = {}\n",
    "        train_IDs = maternal_IDs[train_index]\n",
    "        test_IDs = maternal_IDs[test_index]\n",
    "        \n",
    "        sample_size = int(0.2 * len(train_index))\n",
    "        random_indices = np.random.choice(train_IDs.shape[0], sample_size, replace=False)\n",
    "        val_IDs = train_IDs[random_indices]\n",
    "        train_IDs = np.delete(train_IDs, random_indices)\n",
    "        \n",
    "        patient_indices['maternal_ID'] = patient_indices['sample_ID'].str[0:7]\n",
    "        patient_indices['maternal_ID_ts'] = patient_indices['maternal_ID'].astype(str)+'_'+patient_indices['sample_ID'].str[-2:]\n",
    "        input_OOL_proteomics = OOL_proteomics.merge(patient_indices[['maternal_ID_ts','array_index']], how='left', left_on='sample_ID', right_on='maternal_ID_ts').drop(['sample_ID','maternal_ID_ts','DOS'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "\n",
    "        train_indices = patient_indices[patient_indices['maternal_ID'].isin(train_IDs)]['array_index'].values\n",
    "        np.random.shuffle(train_indices)\n",
    "        test_indices = patient_indices[patient_indices['maternal_ID'].isin(test_IDs)]['array_index'].values\n",
    "        val_indices = patient_indices[patient_indices['maternal_ID'].isin(val_IDs)]['array_index'].values\n",
    "\n",
    "        # Iterate through the hyperparameter combinations\n",
    "        for param_set in tqdm(all_params):\n",
    "            bs = param_set['batch_size']\n",
    "            lr = param_set['lr']\n",
    "            dropout = best_dropout\n",
    "            lr_decay = param_set['lr_decay']\n",
    "            layers = best_num_layers\n",
    "            hidden_dim = best_hidden_dim\n",
    "            print(param_set)\n",
    "            \n",
    "            val_r, val_loss, val_rmse = run_experiment(RNN_data_codes, input_OOL_proteomics,\n",
    "                patient_indices, RNN_data_outcomes, RNN_data_lengths, 'EHR_OOL_PT_noHCE_with_obs', \n",
    "                lr, lr_decay, bs, train_indices=train_indices, test_indices=test_indices,\n",
    "                val_indices=val_indices, feature_types='EHR',\n",
    "                model_path='./models/predictive_models/{}.pth'.format(best_model_name),\n",
    "                fine_tune=True, seed=42, hidden_dim=hidden_dim, num_layers=layers, dropout=dropout,\n",
    "                                                       hyperparam_tuning=True)\n",
    "            \n",
    "            num_layers_arr.append(layers)\n",
    "            dropout_arr.append(dropout)\n",
    "            lr_arr.append(lr)\n",
    "            lr_decay_arr.append(lr_decay)\n",
    "            hidden_dim_arr.append(hidden_dim)\n",
    "            batch_size_arr.append(bs)\n",
    "            split_num_arr.append(split_num)\n",
    "            loss_arr.append(val_loss)\n",
    "\n",
    "        print('experiment 5')\n",
    "        print('outcome {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_df = pd.DataFrame([num_layers_arr, dropout_arr, lr_arr, lr_decay_arr, hidden_dim_arr, batch_size_arr, split_num_arr,loss_arr]).T\n",
    "hyperparam_df.columns = ['num_layers','dropout','lr','lr_decay','hidden_dim','bs','split_num','val_loss']\n",
    "hyperparam_df = hyperparam_df.groupby(['num_layers','dropout','lr','lr_decay','hidden_dim','bs']).mean()\n",
    "num_layers, dropout, lr, lr_decay, hidden_dim, bs = hyperparam_df['val_loss'].idxmin()\n",
    "print(np.min(hyperparam_df['val_loss']))\n",
    "print(num_layers, dropout, lr, lr_decay, hidden_dim, bs)\n",
    "\n",
    "overall_best_params['DOS']['exp5'] = {'num_layers': int(num_layers),'lr': lr,'lr_decay': lr_decay, 'dropout': dropout,\n",
    "                                'hidden_dim': int(hidden_dim), 'batch_size': int(bs)}\n",
    "with open(\"./models/hyperparameters/best_hyperparams_OOL_noHCE_with_obs.pkl\", \"wb\") as f:\n",
    "        pickle.dump(overall_best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid search for full COMET framework\n",
    "grid_search = {'batch_size': [16],\n",
    "              'lr': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "              'lr_decay': [1e-1, 1e-2, 1e-3, 1e-4]}\n",
    "\n",
    "all_params = [dict(zip(grid_search.keys(), values)) for values in product(*grid_search.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparam search for full COMET framework\n",
    "\n",
    "num_layers_arr = []\n",
    "dropout_arr = []\n",
    "lr_arr = []\n",
    "lr_decay_arr = []\n",
    "hidden_dim_arr = []\n",
    "batch_size_arr = []\n",
    "split_num_arr = []\n",
    "loss_arr = []\n",
    "for i in tqdm(outcome_list):\n",
    "    overall_best_params[i]['exp6'] = {}\n",
    "\n",
    "    maternal_IDs = patient_indices['sample_ID'].str[0:7].unique()\n",
    "    \n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    split_num = 0\n",
    "    for train_index, test_index in kf.split(maternal_IDs):\n",
    "        split_num += 1\n",
    "        results_dict = {}\n",
    "        train_IDs = maternal_IDs[train_index]\n",
    "        test_IDs = maternal_IDs[test_index]\n",
    "        \n",
    "        sample_size = int(0.2 * len(train_index))\n",
    "        random_indices = np.random.choice(train_IDs.shape[0], sample_size, replace=False)\n",
    "        val_IDs = train_IDs[random_indices]\n",
    "        train_IDs = np.delete(train_IDs, random_indices)\n",
    "        \n",
    "        patient_indices['maternal_ID'] = patient_indices['sample_ID'].str[0:7]\n",
    "        patient_indices['maternal_ID_ts'] = patient_indices['maternal_ID'].astype(str)+'_'+patient_indices['sample_ID'].str[-2:]\n",
    "        input_OOL_proteomics = OOL_proteomics.merge(patient_indices[['maternal_ID_ts','array_index']], how='left', left_on='sample_ID', right_on='maternal_ID_ts').drop(['sample_ID','maternal_ID_ts','DOS'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "\n",
    "        train_indices = patient_indices[patient_indices['maternal_ID'].isin(train_IDs)]['array_index'].values\n",
    "        np.random.shuffle(train_indices)\n",
    "        test_indices = patient_indices[patient_indices['maternal_ID'].isin(test_IDs)]['array_index'].values\n",
    "        val_indices = patient_indices[patient_indices['maternal_ID'].isin(val_IDs)]['array_index'].values\n",
    "\n",
    "        # Iterate through the hyperparameter combinations\n",
    "        for param_set in tqdm(all_params):\n",
    "            bs = param_set['batch_size']\n",
    "            lr = param_set['lr']\n",
    "            dropout = best_dropout\n",
    "            lr_decay = param_set['lr_decay']\n",
    "            layers = best_num_layers\n",
    "            hidden_dim = best_hidden_dim\n",
    "            print(param_set)\n",
    "            val_r, val_loss, val_rmse = run_experiment(RNN_data_codes, input_OOL_proteomics,\n",
    "                patient_indices, RNN_data_outcomes, RNN_data_lengths, 'both_PT_noHCE_with_obs', \n",
    "                lr, lr_decay, bs, train_indices=train_indices, test_indices=test_indices,\n",
    "                val_indices=val_indices,feature_types='both',\n",
    "                model_path='./models/predictive_models/{}.pth'.format(best_model_name),\n",
    "                fine_tune=True, seed=42, hidden_dim=hidden_dim, num_layers=layers, dropout=dropout,\n",
    "                                                        hyperparam_tuning=True)\n",
    "            \n",
    "            num_layers_arr.append(layers)\n",
    "            dropout_arr.append(dropout)\n",
    "            lr_arr.append(lr)\n",
    "            lr_decay_arr.append(lr_decay)\n",
    "            hidden_dim_arr.append(hidden_dim)\n",
    "            batch_size_arr.append(bs)\n",
    "            split_num_arr.append(split_num)\n",
    "            loss_arr.append(val_loss)\n",
    "\n",
    "        print('experiment 6')\n",
    "        print('outcome {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_df = pd.DataFrame([num_layers_arr, dropout_arr, lr_arr, lr_decay_arr, hidden_dim_arr, batch_size_arr, split_num_arr,loss_arr]).T\n",
    "hyperparam_df.columns = ['num_layers','dropout','lr','lr_decay','hidden_dim','bs','split_num','val_loss']\n",
    "hyperparam_df = hyperparam_df.groupby(['num_layers','dropout','lr','lr_decay','hidden_dim','bs']).mean()\n",
    "num_layers, dropout, lr, lr_decay, hidden_dim, bs = hyperparam_df['val_loss'].idxmin()\n",
    "\n",
    "print(np.min(hyperparam_df['val_loss']))\n",
    "print(num_layers, dropout, lr, lr_decay, hidden_dim, bs)\n",
    "\n",
    "overall_best_params['DOS']['exp6'] = {'num_layers': int(num_layers),'lr': lr,'lr_decay': lr_decay, 'dropout': dropout,\n",
    "                                'hidden_dim': int(hidden_dim), 'batch_size': int(bs)}\n",
    "with open(\"./models/hyperparameters/best_hyperparams_OOL_noHCE_with_obs.pkl\", \"wb\") as f:\n",
    "        pickle.dump(overall_best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_best_params = pickle.load(open('./models/hyperparameters/best_hyperparams_OOL_noHCE_with_obs.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_num_layers = overall_best_params['DOS']['PT']['num_layers']\n",
    "best_dropout = overall_best_params['DOS']['PT']['dropout']\n",
    "best_model_name = overall_best_params['DOS']['PT']['model_name']\n",
    "best_hidden_dim = overall_best_params['DOS']['PT']['hidden_dim']\n",
    "best_num_layers, best_dropout, best_hidden_dim, best_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_num_layers_OOL = overall_best_params['DOS']['exp1']['num_layers']\n",
    "best_dropout_OOL = overall_best_params['DOS']['exp1']['dropout']\n",
    "best_hidden_dim_OOL = overall_best_params['DOS']['exp1']['hidden_dim']\n",
    "best_num_layers_OOL, best_dropout_OOL, best_hidden_dim_OOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 1 = baseline model EHR features\n",
    "#experiment 2 = baseline model metab features\n",
    "#experiment 3 = baseline model all features\n",
    "#experiment 4 = only pretrained model\n",
    "#experiment 5 = fine tune pretrained model \n",
    "#experiment 6 = add proteomics, fine tune pretrained model w/ frozen GRU weights\n",
    "\n",
    "num_iterations = 25\n",
    "results = {}\n",
    "for i in tqdm(outcome_list):\n",
    "    results[i] = {'exp1':[],'exp2':[],'exp3':[],'exp4':[],'exp5':[],'exp6':[]}\n",
    "    for j in tqdm(range(num_iterations)):\n",
    "        print('experiment 1')\n",
    "        val_auc = run_experiment(RNN_data_codes_OOL, OOL_proteomics,\n",
    "                patient_indices_OOL, RNN_data_outcomes_OOL, RNN_data_lengths_OOL, 'EHR_OOL_only_woHCE_wobs_fixed_{}'.format(j), \n",
    "                overall_best_params[i]['exp1']['lr'], overall_best_params[i]['exp1']['lr_decay'],\n",
    "                overall_best_params[i]['exp1']['batch_size'], feature_types='EHR', model_path='', fine_tune=False, seed=j,\n",
    "                                hidden_dim=overall_best_params[i]['exp1']['hidden_dim'],\n",
    "                                 num_layers=overall_best_params[i]['exp1']['num_layers'],\n",
    "                                 dropout=overall_best_params[i]['exp1']['dropout'], return_preds=True)\n",
    "        results[i]['exp1'].append(val_auc)\n",
    "        \n",
    "        print('experiment 2')\n",
    "        val_auc = run_experiment(RNN_data_codes_OOL, OOL_proteomics,\n",
    "                patient_indices_OOL, RNN_data_outcomes_OOL, RNN_data_lengths_OOL, 'proteomics_OOL_only_woHCE_wobs_fixed_{}'.format(j), \n",
    "                overall_best_params[i]['exp2']['lr'], overall_best_params[i]['exp2']['lr_decay'],\n",
    "                overall_best_params[i]['exp2']['batch_size'], feature_types='metab', model_path='', fine_tune=False, seed=j,\n",
    "                                hidden_dim=best_hidden_dim,num_layers=best_num_layers, dropout=best_dropout, return_preds=True)\n",
    "        results[i]['exp2'].append(val_auc)\n",
    "        \n",
    "        print('experiment 3')\n",
    "        val_auc = run_experiment(RNN_data_codes_OOL, OOL_proteomics,\n",
    "                patient_indices_OOL, RNN_data_outcomes_OOL, RNN_data_lengths_OOL, 'both_OOL_only_woHCE_wobs_fixed_{}'.format(j), \n",
    "                overall_best_params[i]['exp3']['lr'], overall_best_params[i]['exp3']['lr_decay'],\n",
    "                overall_best_params[i]['exp3']['batch_size'], feature_types='both', model_path='', fine_tune=False, seed=j,\n",
    "                                hidden_dim=overall_best_params[i]['exp3']['hidden_dim'],\n",
    "                                 num_layers=overall_best_params[i]['exp3']['num_layers'],\n",
    "                                 dropout=overall_best_params[i]['exp3']['dropout'],\n",
    "                                 return_preds=True, return_interpretability=True, return_grads=True)\n",
    "        results[i]['exp3'].append(val_auc)\n",
    "        \n",
    "        print('experiment 4')\n",
    "        val_auc = run_experiment(RNN_data_codes, OOL_proteomics,\n",
    "                patient_indices, RNN_data_outcomes, RNN_data_lengths, 'EHR_OOL_PT_woHCE_wobs_fixed_{}'.format(j), \n",
    "                overall_best_params[i]['exp4']['lr'], overall_best_params[i]['exp4']['lr_decay'],\n",
    "                overall_best_params[i]['exp4']['batch_size'], feature_types='EHR',\n",
    "                                 model_path='./models/predictive_models/{}.pth'.format(best_model_name),\n",
    "                                 fine_tune=False, seed=j,\n",
    "                                hidden_dim=best_hidden_dim,num_layers=best_num_layers,\n",
    "                                 dropout=best_dropout, return_preds=True)\n",
    "        results[i]['exp4'].append(val_auc)\n",
    "\n",
    "        print('experiment 5')\n",
    "        val_auc = run_experiment(RNN_data_codes, OOL_proteomics,\n",
    "                patient_indices, RNN_data_outcomes, RNN_data_lengths, 'EHR_OOL_PT_FT_woHCE_wobs_fixed_{}'.format(j), \n",
    "                overall_best_params[i]['exp5']['lr'], overall_best_params[i]['exp5']['lr_decay'],\n",
    "                overall_best_params[i]['exp5']['batch_size'], feature_types='EHR',\n",
    "                                 model_path='./models/predictive_models/{}.pth'.format(best_model_name),\n",
    "                                 fine_tune=True, seed=j,\n",
    "                                hidden_dim=best_hidden_dim,num_layers=best_num_layers,\n",
    "                                 dropout=best_dropout, return_preds=True)\n",
    "        results[i]['exp5'].append(val_auc)\n",
    "\n",
    "        print('experiment 6')\n",
    "        val_auc = run_experiment(RNN_data_codes, OOL_proteomics,\n",
    "                patient_indices, RNN_data_outcomes, RNN_data_lengths, 'both_OOL_PT_FT_woHCE_wobs_fixed_{}'.format(j), \n",
    "                overall_best_params[i]['exp6']['lr'], overall_best_params[i]['exp6']['lr_decay'],\n",
    "                overall_best_params[i]['exp6']['batch_size'], feature_types='both',\n",
    "                                model_path='./models/predictive_models/{}.pth'.format(best_model_name),\n",
    "                                 fine_tune=True, seed=j,\n",
    "                                hidden_dim=best_hidden_dim,num_layers=best_num_layers,\n",
    "                                 dropout=best_dropout,\n",
    "                                 return_preds=True, return_interpretability=True, return_grads=True)\n",
    "        results[i]['exp6'].append(val_auc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute performance metrics from predictions in validation set\n",
    "for exp in results['DOS'].keys():\n",
    "    true_outcomes = []\n",
    "    total_preds = []\n",
    "    indices = []\n",
    "    for i in results['DOS'][exp]:\n",
    "        true_outcomes.extend(i[3])\n",
    "        total_preds.extend(i[4])\n",
    "        indices.extend(i[5])\n",
    "    \n",
    "        df = pd.DataFrame([true_outcomes,total_preds,indices]).T\n",
    "        df.columns = ['true_outcome','pred','index']\n",
    "        df = df.groupby('index').mean()\n",
    "    print(exp)\n",
    "    print(pearsonr(df['true_outcome'], df['pred'])[0])\n",
    "    print(np.sqrt(np.sum((df['true_outcome']-df['pred'])**2)/df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE computation for exp4\n",
    "true = []\n",
    "pred = []\n",
    "index = []\n",
    "for i in range(0, NUM_TRIALS):\n",
    "    true.extend(list(np.array(results['DOS']['exp4'][i][3])*results['DOS']['exp3'][i][8]+results['DOS']['exp3'][i][7]))\n",
    "    pred.extend(list(np.array(results['DOS']['exp4'][i][4])*results['DOS']['exp3'][i][8]+results['DOS']['exp3'][i][7]))\n",
    "    index.extend(results['DOS']['exp4'][i][5])\n",
    "    \n",
    "df = pd.DataFrame([true,pred,index]).T\n",
    "df.columns = ['true','pred','index']\n",
    "df = df.groupby('index').mean()\n",
    "np.sqrt(np.mean((df['true']-df['pred'])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./results/OOL_results_noHCE_with_obs.pkl\",\"wb\")\n",
    "\n",
    "# write the python object (dict) to pickle file\n",
    "pickle.dump(results,f)\n",
    "\n",
    "# close file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pickle.load(open('./results/OOL_results_noHCE_with_obs.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Latent Representation of EHR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_data_codes_PTMODEL.shape, RNN_data_outcomes_PTMODEL.shape, RNN_data_lengths_PTMODEL.shape, patient_indices_PTMODEL.shape\n",
    "\n",
    "\n",
    "all_EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes_PTMODEL]\n",
    "all_EHR_codes = [torch.nan_to_num(x) for x in all_EHR_codes]\n",
    "all_outcomes = torch.tensor(RNN_data_outcomes_PTMODEL).float()\n",
    "\n",
    "all_loader_codes = create_dataloaders(all_EHR_codes, all_outcomes, RNN_data_lengths_PTMODEL, 100000)   \n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = GRUNet(RNN_data_codes_PTMODEL.shape[2], best_hidden_dim, [best_hidden_dim], best_num_layers, 1, best_dropout).to(device)\n",
    "\n",
    "model_state_dict = torch.load('./models/predictive_models/{}.pth'.format(best_model_name))\n",
    "model.load_state_dict(model_state_dict)\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "criterion = nn.MSELoss()\n",
    "val_predictions = []\n",
    "val_true_labels = []\n",
    "running_loss_val, num_samples_val = 0, 0\n",
    "with torch.no_grad():\n",
    "    for (inputs_codes, labels_codes, lengths_codes) in (all_loader_codes):\n",
    "            inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "            outputs,interpretability_outputs = model(inputs_codes, lengths_codes, interpretability=True)\n",
    "\n",
    "            mean_tensor = torch.tensor(np.mean(RNN_data_outcomes_PTMODEL), dtype=torch.float32, device=device)\n",
    "            std_tensor = torch.tensor(np.std(RNN_data_outcomes_PTMODEL), dtype=torch.float32, device=device)\n",
    "\n",
    "            denormalized_outputs = outputs.squeeze() * std_tensor + mean_tensor\n",
    "            denormalized_labels = labels * std_tensor + mean_tensor\n",
    "\n",
    "            loss = criterion(denormalized_outputs, denormalized_labels)\n",
    "            running_loss_val += (loss.item()*RNN_data_lengths_PTMODEL.shape[0])\n",
    "            num_samples_val += RNN_data_lengths_PTMODEL.shape[0]\n",
    "            \n",
    "            val_predictions.extend(outputs.squeeze().tolist())\n",
    "            val_true_labels.extend(labels.tolist())\n",
    "\n",
    "val_loss = running_loss_val / (num_samples_val)\n",
    "pearson_corr, _ = pearsonr(val_predictions, val_true_labels)\n",
    "val_outcome_mean, val_outcome_sd = np.mean(RNN_data_outcomes_PTMODEL), np.std(RNN_data_outcomes_PTMODEL)\n",
    "val_rmse = np.sqrt(mean_squared_error(RNN_data_outcomes_PTMODEL, np.array(val_predictions)*val_outcome_sd+val_outcome_mean))\n",
    "\n",
    "\n",
    "output = pearson_corr, val_loss, val_rmse, RNN_data_outcomes_PTMODEL, np.array(val_predictions)*val_outcome_sd+val_outcome_mean, patient_indices_PTMODEL, interpretability_outputs, val_outcome_mean, val_outcome_sd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(output, open('./results/full_cohort_latent_noHCE.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance: Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import DeepLift\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "#If all_data is false, we can use index 5 to compute integrated gradients on only val data, 13 for only train data\n",
    "#If all_data is true, we use train, test, and val data to compute integrated gradients\n",
    "imp_index = 5\n",
    "all_data = True\n",
    "\n",
    "\n",
    "# Path to models\n",
    "model_paths = [f'./models/predictive_models/both_OOL_PT_FT_woHCE_wobs_fixed_{i}.pth' for i in range(25)]\n",
    "\n",
    "# Initialize array to store feature importance\n",
    "importance_proteomics_all = torch.zeros((len(model_paths), OOL_proteomics.shape[1]-2))\n",
    "\n",
    "class ModelWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, proteomics, codes, lengths):\n",
    "        return self.model(codes, proteomics, lengths)\n",
    "\n",
    "for model_idx, model_path in tqdm(enumerate(model_paths)):\n",
    "    \n",
    "    \n",
    "    model = joint_model(RNN_data_codes.shape[2], best_hidden_dim, [best_hidden_dim], best_num_layers, 1,\n",
    "                    OOL_proteomics.shape[1]-2, [], [], best_dropout).to(device)\n",
    "    \n",
    "    proteomics = OOL_proteomics.merge(patient_indices[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID','DOS'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "    if all_data == False:\n",
    "        val_proteomics = proteomics[results['DOS']['exp6'][model_idx][imp_index],:]\n",
    "    else:\n",
    "        val_proteomics = proteomics\n",
    "    scaler = StandardScaler()\n",
    "    val_proteomics = scaler.fit_transform(val_proteomics)\n",
    "\n",
    "    if all_data == False:\n",
    "        EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes[results['DOS']['exp6'][model_idx][imp_index],:,:]]  \n",
    "        EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "        val_proteomics = torch.tensor(val_proteomics).float()\n",
    "        val_proteomics = torch.nan_to_num(val_proteomics)\n",
    "        val_outcomes = torch.tensor(RNN_data_outcomes[results['DOS']['exp6'][model_idx][imp_index]]).float()\n",
    "\n",
    "        outcome_mean = torch.mean(val_outcomes)\n",
    "        outcome_sd = torch.std(val_outcomes)\n",
    "\n",
    "        data_set = DataBuilder(val_proteomics, val_outcomes, scaler)\n",
    "        loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "        loader_codes = create_dataloaders(EHR_codes, val_outcomes, RNN_data_lengths[results['DOS']['exp6'][model_idx][imp_index]], 100000)\n",
    "    else:\n",
    "        EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes]  \n",
    "        EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "        val_proteomics = torch.tensor(val_proteomics).float()\n",
    "        val_proteomics = torch.nan_to_num(val_proteomics)\n",
    "        val_outcomes = torch.tensor(RNN_data_outcomes).float()\n",
    "\n",
    "        outcome_mean = torch.mean(val_outcomes)\n",
    "        outcome_sd = torch.std(val_outcomes)\n",
    "\n",
    "        data_set = DataBuilder(val_proteomics, val_outcomes, scaler)\n",
    "        loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "        loader_codes = create_dataloaders(EHR_codes, val_outcomes, RNN_data_lengths, 100000)\n",
    "\n",
    "    model_state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    model_wrapper = ModelWrapper(model)\n",
    "    ig = IntegratedGradients(model_wrapper)\n",
    "    # Compute the feature importance for this model\n",
    "    for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in zip(loader_codes, loader_proteomics):\n",
    "        inputs_codes, labels_codes = inputs_codes.to(device), labels_codes.to(device)\n",
    "        inputs_proteomics, labels_proteomics = inputs_proteomics.to(device), labels_proteomics.to(device)\n",
    "        \n",
    "        def forward_func(proteomics, codes, lengths):\n",
    "            return model(codes, proteomics, lengths)\n",
    "        \n",
    "        # Compute feature importances using a custom forward function\n",
    "        importance_proteomics = ig.attribute(inputs_proteomics, additional_forward_args=(inputs_codes, lengths_codes))\n",
    "        # Store the feature importance for this model\n",
    "        importance_proteomics_all[model_idx] = importance_proteomics.mean(dim=0).cpu().detach()\n",
    "    \n",
    "# Compute the average feature importance across all 25 models\n",
    "importance_proteomics_avg = torch.mean((importance_proteomics_all), dim=0)*1e11\n",
    "\n",
    "# Create dataframe for feature importances\n",
    "importance_df = pd.DataFrame([importance_proteomics_avg.numpy(), OOL_proteomics.drop(['DOS','sample_ID'],axis=1).columns]).T\n",
    "importance_df.columns = ['importance_PT','name']\n",
    "importance_df.sort_values('importance_PT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import DeepLift\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "# Path to models\n",
    "model_paths = [f'./models/predictive_models/both_OOL_only_woHCE_wobs_fixed_{i}.pth' for i in range(25)]\n",
    "\n",
    "# Initialize array to store feature importance\n",
    "importance_proteomics_all = torch.zeros((len(model_paths), OOL_proteomics.shape[1]-2))\n",
    "\n",
    "class ModelWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, proteomics, codes, lengths):\n",
    "        return self.model(codes, proteomics, lengths)\n",
    "\n",
    "# Iterate over all models\n",
    "for model_idx, model_path in tqdm(enumerate(model_paths)):\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    model = joint_model(RNN_data_codes_OOL.shape[2], overall_best_params['DOS']['exp3']['hidden_dim'], [overall_best_params['DOS']['exp3']['hidden_dim']],\n",
    "                        overall_best_params['DOS']['exp3']['num_layers'], 1,\n",
    "                        OOL_proteomics.shape[1]-2, [], [], overall_best_params['DOS']['exp3']['dropout']).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    if all_data == False:\n",
    "        val_proteomics = proteomics[results['DOS']['exp3'][model_idx][imp_index],:]\n",
    "    else:\n",
    "        val_proteomics = proteomics\n",
    "    scaler = StandardScaler()\n",
    "    val_proteomics = scaler.fit_transform(val_proteomics)\n",
    "\n",
    "    if all_data == False:\n",
    "        EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes_OOL[results['DOS']['exp3'][model_idx][imp_index],:,:]]  \n",
    "        EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "        val_proteomics = torch.tensor(val_proteomics).float()\n",
    "        val_proteomics = torch.nan_to_num(val_proteomics)\n",
    "        val_outcomes = torch.tensor(RNN_data_outcomes_OOL[results['DOS']['exp3'][model_idx][imp_index]]).float()\n",
    "\n",
    "        outcome_mean = torch.mean(val_outcomes)\n",
    "        outcome_sd = torch.std(val_outcomes)\n",
    "\n",
    "        data_set = DataBuilder(val_proteomics, val_outcomes, scaler)\n",
    "        loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "        loader_codes = create_dataloaders(EHR_codes, val_outcomes, RNN_data_lengths_OOL[results['DOS']['exp3'][model_idx][imp_index]], 100000)\n",
    "    if all_data == True:\n",
    "        EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes_OOL]  \n",
    "        EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "        val_proteomics = torch.tensor(val_proteomics).float()\n",
    "        val_proteomics = torch.nan_to_num(val_proteomics)\n",
    "        val_outcomes = torch.tensor(RNN_data_outcomes_OOL).float()\n",
    "\n",
    "        outcome_mean = torch.mean(val_outcomes)\n",
    "        outcome_sd = torch.std(val_outcomes)\n",
    "\n",
    "        data_set = DataBuilder(val_proteomics, val_outcomes, scaler)\n",
    "        loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "        loader_codes = create_dataloaders(EHR_codes, val_outcomes, RNN_data_lengths_OOL, 100000)\n",
    "\n",
    "    # Load the model\n",
    "    model_state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "    model_wrapper = ModelWrapper(model)\n",
    "    ig = IntegratedGradients(model_wrapper)\n",
    "    # Compute the feature importance for this model\n",
    "    for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in zip(loader_codes, loader_proteomics):\n",
    "        inputs_codes, labels_codes = inputs_codes.to(device), labels_codes.to(device)\n",
    "        inputs_proteomics, labels_proteomics = inputs_proteomics.to(device), labels_proteomics.to(device)\n",
    "        \n",
    "        def forward_func(proteomics, codes, lengths):\n",
    "            return model(codes, proteomics, lengths)\n",
    "        \n",
    "        # Compute feature importances using a custom forward function\n",
    "        importance_proteomics = ig.attribute(inputs_proteomics, additional_forward_args=(inputs_codes, lengths_codes))\n",
    "        # Store the feature importance for this model\n",
    "        importance_proteomics_all[model_idx] = importance_proteomics.mean(dim=0).cpu().detach()\n",
    "    \n",
    "# Compute the average feature importance across all models\n",
    "importance_proteomics_avg = torch.mean((importance_proteomics_all), dim=0)*1e11\n",
    "\n",
    "# Create dataframe for feature importances\n",
    "importance_df_OOL = pd.DataFrame([importance_proteomics_avg.numpy(), OOL_proteomics.drop(['DOS','sample_ID'],axis=1).columns]).T\n",
    "importance_df_OOL.columns = ['importance_OOL','name']\n",
    "importance_df_OOL.sort_values('importance_OOL').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot function trajectories by input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final structure is [PT_model1_epoch1, PT_mode1_epoch2,...PT_model2_epoch1....NPT_model1_epoch1...] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = joint_model(RNN_data_codes.shape[2], best_hidden_dim, [best_hidden_dim], best_num_layers, 1,\n",
    "                    OOL_proteomics.shape[1]-2, [], [], best_dropout).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "proteomics = OOL_proteomics.merge(patient_indices[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID','DOS'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "proteomics = scaler.fit_transform(proteomics)\n",
    "\n",
    "EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes]  \n",
    "EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "proteomics = torch.tensor(proteomics).float()\n",
    "proteomics = torch.nan_to_num(proteomics)\n",
    "outcomes = torch.tensor(RNN_data_outcomes).float()\n",
    "\n",
    "\n",
    "data_set = DataBuilder(proteomics, outcomes, scaler)\n",
    "loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "loader_codes = create_dataloaders(EHR_codes, outcomes, RNN_data_lengths, 100000)\n",
    "            \n",
    "    \n",
    "model_paths = [f'./models/predictive_models/both_OOL_PT_FT_woHCE_wobs_fixed_{i}_epoch{j}.pth' for i in range(25) for j in range(200)]\n",
    "model_number = 0\n",
    "\n",
    "all_outputs = []\n",
    "model_numbers = []\n",
    "epochs = []\n",
    "PT = []\n",
    "total_losses = []\n",
    "\n",
    "for i in tqdm(model_paths):\n",
    "    result = re.search('both_OOL_PT_FT_woHCE_wobs_fixed_(\\d+)_epoch(\\d+).pth', i)\n",
    "    model_number = int(result.group(1))\n",
    "    epoch_number = int(result.group(2))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            model_state_dict = torch.load(i)\n",
    "            model.load_state_dict(model_state_dict)\n",
    "            for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(loader_codes, loader_proteomics)):\n",
    "                inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                inputs_proteomics = inputs_proteomics.to(device)\n",
    "                outputs = model(inputs_codes, inputs_proteomics, lengths_codes)\n",
    "                total_losses.append(criterion(outputs.squeeze(), labels).cpu().item())\n",
    "            all_outputs.append(outputs.squeeze().cpu().numpy())\n",
    "            model_numbers.append(model_number)\n",
    "            epochs.append(epoch_number)\n",
    "            PT.append('PT')\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "model = joint_model(RNN_data_codes.shape[2], best_hidden_dim_OOL, [best_hidden_dim_OOL], best_num_layers_OOL, 1,\n",
    "                    OOL_proteomics.shape[1]-2, [], [], best_dropout_OOL).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes_OOL]  \n",
    "EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "proteomics = torch.tensor(proteomics).float()\n",
    "proteomics = torch.nan_to_num(proteomics)\n",
    "outcomes = torch.tensor(RNN_data_outcomes_OOL).float()\n",
    "\n",
    "\n",
    "data_set = DataBuilder(proteomics, outcomes, scaler)\n",
    "loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "loader_codes = create_dataloaders(EHR_codes, outcomes, RNN_data_lengths_OOL, 100000)\n",
    "        \n",
    "model_paths = [f'./models/predictive_models/both_OOL_only_woHCE_wobs_fixed_{i}_epoch{j}.pth' for i in range(25) for j in range(200)]\n",
    "model_number = 0\n",
    "\n",
    "for i in tqdm(model_paths):\n",
    "    result = re.search('both_OOL_only_woHCE_wobs_fixed_(\\d+)_epoch(\\d+).pth', i)\n",
    "    model_number = int(result.group(1))\n",
    "    epoch_number = int(result.group(2))\n",
    "    if True:\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                model_state_dict = torch.load(i)\n",
    "                model.load_state_dict(model_state_dict)\n",
    "                for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(loader_codes, loader_proteomics)):\n",
    "                    inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                    inputs_proteomics = inputs_proteomics.to(device)\n",
    "                    outputs = model(inputs_codes, inputs_proteomics, lengths_codes)\n",
    "                    total_losses.append(criterion(outputs.squeeze(), labels).cpu().item())\n",
    "                all_outputs.append(outputs.squeeze().cpu().numpy())\n",
    "                model_numbers.append(model_number)\n",
    "                epochs.append(epoch_number)\n",
    "                PT.append('NPT')\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(all_outputs).shape, len(model_numbers), len(epochs), len(PT), len(total_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.array(epochs)\n",
    "model_numbers = np.array(model_numbers)\n",
    "all_outputs = np.array(all_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom lines for the legend\n",
    "line1 = Line2D([0], [0], color='none', marker='o', markersize=10, markerfacecolor='darkred', label='PT')\n",
    "line2 = Line2D([0], [0], color='none', marker='x', markersize=10, markerfacecolor='darkblue', label='NPT')\n",
    "\n",
    "# Create a t-SNE instance and fit_transform the data\n",
    "tsne = TSNE(n_components=2, random_state=42, init='pca', learning_rate='auto')\n",
    "embedding = tsne.fit_transform(np.array(all_outputs))\n",
    "\n",
    "# Create a markers array based on PT array\n",
    "marker_map = {'PT': 'o', 'NPT': 'x'}\n",
    "markers = [marker_map[pt] for pt in PT]\n",
    "\n",
    "# Don't normalize the epochs\n",
    "colors = epochs\n",
    "\n",
    "# Create colormaps that goes from light red to dark red for PT, light blue to dark blue for NPT\n",
    "cmap_pt = LinearSegmentedColormap.from_list(\"mycmap_pt\", [\"lightcoral\", \"darkred\"])\n",
    "cmap_npt = LinearSegmentedColormap.from_list(\"mycmap_npt\", [\"lightblue\", \"darkblue\"])\n",
    "\n",
    "# Plot the results with different markers and colors\n",
    "for marker_type, marker in marker_map.items():\n",
    "    mask = np.array(markers) == marker\n",
    "    cmap = cmap_pt if marker_type == 'PT' else cmap_npt\n",
    "    sc = plt.scatter(embedding[mask, 0], embedding[mask, 1], marker=marker, c=colors[mask], cmap=cmap, alpha = 0.7)\n",
    "\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.gca().set_xticks([])\n",
    "plt.gca().set_yticks([])\n",
    "plt.title('Overall parameter space', fontsize=14)\n",
    "plt.legend(handles=[line1, line2])  # add the custom legend\n",
    "plt.savefig('./overall_params.png', dpi=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform t-SNE dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=42, init='pca', learning_rate='auto')\n",
    "embedding = tsne.fit_transform(np.array(all_outputs))\n",
    "\n",
    "# Create colormaps that go from light red to dark red for PT, and light blue to dark blue for NPT\n",
    "cmap_pt = LinearSegmentedColormap.from_list(\"mycmap_pt\", [\"lightcoral\", \"darkred\"])\n",
    "cmap_npt = LinearSegmentedColormap.from_list(\"mycmap_npt\", [\"lightblue\", \"darkblue\"])\n",
    "\n",
    "fig, axs = plt.subplots(5, 5, figsize=(20, 20))\n",
    "\n",
    "# Calculate the global x and y limits\n",
    "xlim = (embedding[:,0].min()-1, embedding[:,0].max()+1)\n",
    "ylim = (embedding[:,1].min()-1, embedding[:,1].max()+1)\n",
    "\n",
    "for model_number in range(25):\n",
    "    ax = axs[model_number // 5, model_number % 5]\n",
    "\n",
    "    for model_type in ['PT', 'NPT']:\n",
    "        # Get mask for the current trajectory\n",
    "        mask = (np.array(model_numbers) == model_number) & (np.array(PT) == model_type)\n",
    "\n",
    "        # Get the points and corresponding epochs for the current trajectory\n",
    "        trajectory_points = embedding[mask]\n",
    "        trajectory_epochs = np.array(epochs)[mask]\n",
    "\n",
    "        # Sort the points and epochs\n",
    "        sort_indices = np.argsort(trajectory_epochs)\n",
    "        sorted_points = trajectory_points[sort_indices]\n",
    "\n",
    "        # Select the colormap based on model_type\n",
    "        cmap = cmap_pt if model_type == 'PT' else cmap_npt\n",
    "\n",
    "        # Plot the points with color indicating epoch and marker indicating PT/NPT\n",
    "        ax.scatter(sorted_points[:, 0], sorted_points[:, 1], \n",
    "                   c=trajectory_epochs[sort_indices], \n",
    "                   cmap=cmap, \n",
    "                   marker=marker_map[model_type], \n",
    "                   alpha=0.6)\n",
    "\n",
    "        # Plot lines connecting the points of the same model type\n",
    "        ax.plot(sorted_points[:, 0], sorted_points[:, 1], \n",
    "                color='lightgrey' if model_type == 'PT' else 'black', \n",
    "                linestyle='--')\n",
    "\n",
    "    ax.set_aspect('auto')\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "    # Turn off tick labels\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./overall_paths_ool.png',dpi=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Trajectories Based on Latent Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I want to do this now with the embedding. The embedding will be (embed_dim, num_patients, num_epochs)\n",
    "#A solution is to just concat embed_dim with num_patients, (embed_dim*num_patients, num_epochs*num_models)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "model = joint_model(RNN_data_codes.shape[2], best_hidden_dim, [best_hidden_dim], best_num_layers, 1,\n",
    "                    OOL_proteomics.shape[1]-2, [], [], best_dropout).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "proteomics = OOL_proteomics.merge(patient_indices[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID','DOS'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes]  \n",
    "EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "proteomics = torch.tensor(proteomics).float()\n",
    "proteomics = torch.nan_to_num(proteomics)\n",
    "outcomes = torch.tensor(RNN_data_outcomes).float()\n",
    "\n",
    "\n",
    "data_set = DataBuilder(proteomics, outcomes, scaler)\n",
    "loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "loader_codes = create_dataloaders(EHR_codes, outcomes, RNN_data_lengths, 100000)\n",
    "        \n",
    "\n",
    "model_paths = [f'./models/predictive_models/both_OOL_PT_FT_woHCE_wobs_fixed_{i}_epoch{j}.pth' for i in range(25) for j in range(200)]\n",
    "model_number = 0\n",
    "\n",
    "all_outputs = []\n",
    "model_numbers = []\n",
    "epochs = []\n",
    "PT = []\n",
    "\n",
    "for i in tqdm(model_paths):\n",
    "    result = re.search('both_OOL_PT_FT_woHCE_wobs_fixed_(\\d+)_epoch(\\d+).pth', i)\n",
    "    model_number = int(result.group(1))\n",
    "    epoch_number = int(result.group(2))\n",
    "    if True:\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                model_state_dict = torch.load(i)\n",
    "                model.load_state_dict(model_state_dict)\n",
    "                for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(loader_codes, loader_proteomics)):\n",
    "                    inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                    inputs_proteomics = inputs_proteomics.to(device)\n",
    "                    outputs = model(inputs_codes, inputs_proteomics, lengths_codes, True)\n",
    "                latent_rep = outputs[1][0].flatten()\n",
    "                all_outputs.append(latent_rep.cpu().numpy())\n",
    "                model_numbers.append(model_number)\n",
    "                epochs.append(epoch_number)\n",
    "                PT.append('PT')\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "\n",
    "\n",
    "model = joint_model(RNN_data_codes_OOL.shape[2], best_hidden_dim_OOL, [best_hidden_dim_OOL], best_num_layers_OOL, 1,\n",
    "                    OOL_proteomics.shape[1]-2, [], [], best_dropout_OOL).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "proteomics = OOL_proteomics.merge(patient_indices_OOL[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID','DOS'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes_OOL]  \n",
    "EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "proteomics = torch.tensor(proteomics).float()\n",
    "proteomics = torch.nan_to_num(proteomics)\n",
    "outcomes = torch.tensor(RNN_data_outcomes_OOL).float()\n",
    "\n",
    "\n",
    "data_set = DataBuilder(proteomics, outcomes, scaler)\n",
    "loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "loader_codes = create_dataloaders(EHR_codes, outcomes, RNN_data_lengths_OOL, 100000)\n",
    "            \n",
    " \n",
    "    \n",
    "model_paths = [f'./models/predictive_models/both_OOL_only_woHCE_wobs_fixed_{i}_epoch{j}.pth' for i in range(25) for j in range(200)]\n",
    "model_number = 0\n",
    "\n",
    "\n",
    "for i in tqdm(model_paths):\n",
    "    result = re.search('both_OOL_only_woHCE_wobs_fixed_(\\d+)_epoch(\\d+).pth', i)\n",
    "    model_number = int(result.group(1))\n",
    "    epoch_number = int(result.group(2))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            model_state_dict = torch.load(i)\n",
    "            model.load_state_dict(model_state_dict)\n",
    "            for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(loader_codes, loader_proteomics)):\n",
    "                inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                inputs_proteomics = inputs_proteomics.to(device)\n",
    "                outputs = model(inputs_codes, inputs_proteomics, lengths_codes, True)\n",
    "            latent_rep = outputs[1][0].flatten()\n",
    "            all_outputs.append(latent_rep.cpu().numpy())\n",
    "            model_numbers.append(model_number)\n",
    "            epochs.append(epoch_number)\n",
    "            PT.append('NPT')\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(all_outputs).shape, len(model_numbers), len(epochs), len(PT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.array(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a t-SNE instance and fit_transform the data\n",
    "tsne = TSNE(n_components=2, random_state=42, init='pca', learning_rate='auto')\n",
    "\n",
    "embedding = tsne.fit_transform(np.array(all_outputs))\n",
    "\n",
    "\n",
    "line1 = Line2D([0], [0], color='none', marker='o', markersize=10, markerfacecolor='darkred', label='PT')\n",
    "line2 = Line2D([0], [0], color='none', marker='x', markersize=10, markerfacecolor='darkblue', label='NPT')\n",
    "\n",
    "# Create a markers array based on PT array\n",
    "marker_map = {'PT': 'o', 'NPT': 'x'}\n",
    "markers = [marker_map[pt] for pt in PT]\n",
    "\n",
    "# Don't normalize the epochs\n",
    "colors = epochs\n",
    "\n",
    "# Create a colormap that goes from light green to dark blue\n",
    "cmap_pt = LinearSegmentedColormap.from_list(\"mycmap_pt\", [\"lightcoral\", \"darkred\"])\n",
    "cmap_npt = LinearSegmentedColormap.from_list(\"mycmap_npt\", [\"lightblue\", \"darkblue\"])\n",
    "\n",
    "# Plot the results with different markers and colors\n",
    "for marker_type, marker in marker_map.items():\n",
    "    mask = np.array(markers) == marker\n",
    "    cmap = cmap_pt if marker_type == 'PT' else cmap_npt\n",
    "    sc = plt.scatter(embedding[mask, 0], embedding[mask, 1], marker=marker, c=colors[mask], cmap=cmap, alpha = 0.7)\n",
    "\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "# plt.colorbar(sc, label='Epochs')\n",
    "plt.legend(handles=[line1, line2])  # add the custom legend\n",
    "plt.gca().set_xticks([])\n",
    "plt.gca().set_yticks([])\n",
    "plt.title('RNN parameter space', fontsize=14)\n",
    "plt.savefig('./RNN_param_space.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot losses by epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "experiments = ['exp3', 'exp6']\n",
    "colors = ['blue', 'orange']\n",
    "mean_colors = ['black', 'red']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "max_epoch = max(max(len(model_data[10]) for model_data in results['DOS'][exp]) for exp in experiments) - 5\n",
    "\n",
    "for experiment, color, mean_color in zip(experiments, colors, mean_colors):\n",
    "    train_losses_all = []\n",
    "    test_losses_all = []\n",
    "    \n",
    "    for model_data in results['DOS'][experiment]:\n",
    "        train_losses = model_data[10][:-5]  # Exclude last 5 epochs\n",
    "        test_losses = model_data[11][:-5]  # Exclude last 5 epochs\n",
    "\n",
    "        train_losses_all.append(train_losses + [np.nan]*(max_epoch-len(train_losses)))\n",
    "        test_losses_all.append(test_losses + [np.nan]*(max_epoch-len(test_losses)))\n",
    "\n",
    "        # Plot each model's losses\n",
    "        plt.plot(train_losses, test_losses, marker='o', linestyle='-', color=color, alpha=0.1)\n",
    "    \n",
    "    # Calculate the mean across each epoch for train and test losses separately\n",
    "    train_losses_mean = np.ma.masked_invalid(train_losses_all).mean(axis=0)\n",
    "    test_losses_mean = np.ma.masked_invalid(test_losses_all).mean(axis=0)\n",
    "\n",
    "    # Sort by training loss\n",
    "    sorted_indices = np.argsort(train_losses_mean)\n",
    "\n",
    "    # Only plot the mean losses when we have at least 5 data points\n",
    "    mask = np.count_nonzero(~np.isnan(train_losses_all), axis=0) >= 5\n",
    "\n",
    "    # Plot the mean losses with the specific color\n",
    "    plt.plot(train_losses_mean[sorted_indices][mask[sorted_indices]], \n",
    "             test_losses_mean[sorted_indices][mask[sorted_indices]], \n",
    "             color=mean_color, linewidth=2.0, label=f\"{experiment} mean\", zorder=100)\n",
    "\n",
    "# Create custom patches for the legend\n",
    "patch1 = mpatches.Patch(color='blue', label='Not PT models')\n",
    "patch2 = mpatches.Patch(color='orange', label='PT models')\n",
    "patch3 = mpatches.Patch(color='black', label='Not PT mean')\n",
    "patch4 = mpatches.Patch(color='red', label='PT mean')\n",
    "\n",
    "plt.gca().set_xscale('log')\n",
    "plt.gca().set_yscale('log')\n",
    "plt.xlabel('Training loss', fontsize=18)\n",
    "plt.ylabel('Test loss', fontsize=18)\n",
    "plt.title('Training losses vs Test losses for different models (log scale)',fontsize=18)\n",
    "plt.legend(handles=[patch1, patch2, patch3, patch4])\n",
    "plt.savefig('./losses.png', dpi=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat the plots above but with proteomics predictions only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return final_pred, (out_ehr, pred_proteomics, pred_ehr, out_combined, final_pred, self.final_combine.weight)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = joint_model(RNN_data_codes.shape[2], best_hidden_dim, [best_hidden_dim], best_num_layers, 1,\n",
    "                    OOL_proteomics.shape[1]-2, [], [], best_dropout).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "proteomics = OOL_proteomics.merge(patient_indices[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID','DOS'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "proteomics = scaler.fit_transform(proteomics)\n",
    "\n",
    "EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes]  \n",
    "EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "proteomics = torch.tensor(proteomics).float()\n",
    "proteomics = torch.nan_to_num(proteomics)\n",
    "outcomes = torch.tensor(RNN_data_outcomes).float()\n",
    "\n",
    "\n",
    "data_set = DataBuilder(proteomics, outcomes, scaler)\n",
    "loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "loader_codes = create_dataloaders(EHR_codes, outcomes, RNN_data_lengths, 100000)\n",
    "            \n",
    "\n",
    "model_paths = [f'./models/predictive_models/both_OOL_PT_FT_woHCE_wobs_fixed_{i}_epoch{j}.pth' for i in range(25) for j in range(200)]\n",
    "model_number = 0\n",
    "\n",
    "all_outputs = []\n",
    "model_numbers = []\n",
    "epochs = []\n",
    "PT = []\n",
    "\n",
    "for i in tqdm(model_paths):\n",
    "    result = re.search('both_OOL_PT_FT_woHCE_wobs_fixed_(\\d+)_epoch(\\d+).pth', i)\n",
    "    model_number = int(result.group(1))\n",
    "    epoch_number = int(result.group(2))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            model_state_dict = torch.load(i)\n",
    "            model.load_state_dict(model_state_dict)\n",
    "            for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(loader_codes, loader_proteomics)):\n",
    "                inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                inputs_proteomics = inputs_proteomics.to(device)\n",
    "                outputs = model(inputs_codes, inputs_proteomics, lengths_codes, interpretability=True)[1][1]\n",
    "            all_outputs.append(outputs.squeeze().cpu().numpy())\n",
    "            model_numbers.append(model_number)\n",
    "            epochs.append(epoch_number)\n",
    "            PT.append('PT')\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "model = joint_model(RNN_data_codes.shape[2], best_hidden_dim_OOL, [best_hidden_dim_OOL], best_num_layers_OOL, 1,\n",
    "                    OOL_proteomics.shape[1]-2, [], [], best_dropout_OOL).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "proteomics = OOL_proteomics.merge(patient_indices[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID','DOS'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "proteomics = scaler.fit_transform(proteomics)\n",
    "\n",
    "EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes_OOL]  \n",
    "EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "proteomics = torch.tensor(proteomics).float()\n",
    "proteomics = torch.nan_to_num(proteomics)\n",
    "outcomes = torch.tensor(RNN_data_outcomes_OOL).float()\n",
    "\n",
    "\n",
    "data_set = DataBuilder(proteomics, outcomes, scaler)\n",
    "loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "loader_codes = create_dataloaders(EHR_codes, outcomes, RNN_data_lengths_OOL, 100000)\n",
    "            \n",
    "\n",
    "model_paths = [f'./models/predictive_models/both_OOL_only_woHCE_wobs_fixed_{i}_epoch{j}.pth' for i in range(25) for j in range(200)]\n",
    "model_number = 0\n",
    "\n",
    "for i in tqdm(model_paths):\n",
    "    result = re.search('both_OOL_only_woHCE_wobs_fixed_(\\d+)_epoch(\\d+).pth', i)\n",
    "    model_number = int(result.group(1))\n",
    "    epoch_number = int(result.group(2))\n",
    "    if True:\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                model_state_dict = torch.load(i)\n",
    "                model.load_state_dict(model_state_dict)\n",
    "                for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(loader_codes, loader_proteomics)):\n",
    "                    inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                    inputs_proteomics = inputs_proteomics.to(device)\n",
    "                    outputs = model(inputs_codes, inputs_proteomics, lengths_codes, interpretability=True)[1][1]\n",
    "                all_outputs.append(outputs.squeeze().cpu().numpy())\n",
    "                model_numbers.append(model_number)\n",
    "                epochs.append(epoch_number)\n",
    "                PT.append('NPT')\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.array(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom lines for the legend\n",
    "line1 = Line2D([0], [0], color='none', marker='o', markersize=10, markerfacecolor='darkred', label='PT')\n",
    "line2 = Line2D([0], [0], color='none', marker='x', markersize=10, markerfacecolor='darkblue', label='NPT')\n",
    "\n",
    "# Create a t-SNE instance and fit_transform the data\n",
    "tsne = TSNE(n_components=2, random_state=42, init='pca', learning_rate='auto')\n",
    "embedding = tsne.fit_transform(np.array(all_outputs))\n",
    "\n",
    "# Create a markers array based on PT array\n",
    "marker_map = {'PT': 'o', 'NPT': 'x'}\n",
    "markers = [marker_map[pt] for pt in PT]\n",
    "\n",
    "# Don't normalize the epochs\n",
    "colors = epochs\n",
    "\n",
    "# Create colormaps that goes from light red to dark red for PT, light blue to dark blue for NPT\n",
    "cmap_pt = LinearSegmentedColormap.from_list(\"mycmap_pt\", [\"lightcoral\", \"darkred\"])\n",
    "cmap_npt = LinearSegmentedColormap.from_list(\"mycmap_npt\", [\"lightblue\", \"darkblue\"])\n",
    "\n",
    "# Plot the results with different markers and colors\n",
    "for marker_type, marker in marker_map.items():\n",
    "    mask = np.array(markers) == marker\n",
    "    cmap = cmap_pt if marker_type == 'PT' else cmap_npt\n",
    "    sc = plt.scatter(embedding[mask, 0], embedding[mask, 1], marker=marker, c=colors[mask], cmap=cmap, alpha = 0.7)\n",
    "\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.gca().set_xticks([])\n",
    "plt.gca().set_yticks([])\n",
    "plt.title('Protein parameter space', fontsize=14)\n",
    "plt.legend(handles=[line1, line2])  # add the custom legend\n",
    "plt.savefig('./protein_params.png', dpi=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform t-SNE dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=42, init='pca', learning_rate='auto')\n",
    "embedding = tsne.fit_transform(np.array(all_outputs))\n",
    "\n",
    "# Create colormaps that go from light red to dark red for PT, and light blue to dark blue for NPT\n",
    "cmap_pt = LinearSegmentedColormap.from_list(\"mycmap_pt\", [\"lightcoral\", \"darkred\"])\n",
    "cmap_npt = LinearSegmentedColormap.from_list(\"mycmap_npt\", [\"lightblue\", \"darkblue\"])\n",
    "\n",
    "fig, axs = plt.subplots(5, 5, figsize=(20, 20))\n",
    "\n",
    "# Calculate the global x and y limits\n",
    "xlim = (embedding[:,0].min()-1, embedding[:,0].max()+1)\n",
    "ylim = (embedding[:,1].min()-1, embedding[:,1].max()+1)\n",
    "\n",
    "for model_number in range(25):\n",
    "    ax = axs[model_number // 5, model_number % 5]\n",
    "\n",
    "    for model_type in ['PT', 'NPT']:\n",
    "        # Get mask for the current trajectory\n",
    "        mask = (np.array(model_numbers) == model_number) & (np.array(PT) == model_type)\n",
    "\n",
    "        # Get the points and corresponding epochs for the current trajectory\n",
    "        trajectory_points = embedding[mask]\n",
    "        trajectory_epochs = np.array(epochs)[mask]\n",
    "\n",
    "        # Sort the points and epochs\n",
    "        sort_indices = np.argsort(trajectory_epochs)\n",
    "        sorted_points = trajectory_points[sort_indices]\n",
    "\n",
    "        # Select the colormap based on model_type\n",
    "        cmap = cmap_pt if model_type == 'PT' else cmap_npt\n",
    "\n",
    "        # Plot the points with color indicating epoch and marker indicating PT/NPT\n",
    "        ax.scatter(sorted_points[:, 0], sorted_points[:, 1], \n",
    "                   c=trajectory_epochs[sort_indices], \n",
    "                   cmap=cmap, \n",
    "                   marker=marker_map[model_type], \n",
    "                   alpha=0.6)\n",
    "\n",
    "        # Plot lines connecting the points of the same model type\n",
    "        ax.plot(sorted_points[:, 0], sorted_points[:, 1], \n",
    "                color='lightgrey' if model_type == 'PT' else 'black', \n",
    "                linestyle='--')\n",
    "\n",
    "    ax.set_aspect('auto')\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "    # Turn off tick labels\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./protein_paths_ool.png',dpi=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Again, with the joint predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return final_pred, (out_ehr, pred_proteomics, pred_ehr, out_combined, final_pred, self.final_combine.weight)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = joint_model(RNN_data_codes.shape[2], best_hidden_dim, [best_hidden_dim], best_num_layers, 1,\n",
    "                    OOL_proteomics.shape[1]-2, [], [], best_dropout).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "proteomics = OOL_proteomics.merge(patient_indices[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID','DOS'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "proteomics = scaler.fit_transform(proteomics)\n",
    "\n",
    "EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes]  \n",
    "EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "proteomics = torch.tensor(proteomics).float()\n",
    "proteomics = torch.nan_to_num(proteomics)\n",
    "outcomes = torch.tensor(RNN_data_outcomes).float()\n",
    "\n",
    "\n",
    "data_set = DataBuilder(proteomics, outcomes, scaler)\n",
    "loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "loader_codes = create_dataloaders(EHR_codes, outcomes, RNN_data_lengths, 100000)\n",
    "            \n",
    "    \n",
    "model_paths = [f'./models/predictive_models/both_OOL_PT_FT_woHCE_wobs_fixed_{i}_epoch{j}.pth' for i in range(25) for j in range(200)]\n",
    "model_number = 0\n",
    "\n",
    "all_outputs = []\n",
    "model_numbers = []\n",
    "epochs = []\n",
    "PT = []\n",
    "\n",
    "for i in tqdm(model_paths):\n",
    "    result = re.search('both_OOL_PT_FT_woHCE_wobs_fixed_(\\d+)_epoch(\\d+).pth', i)\n",
    "    model_number = int(result.group(1))\n",
    "    epoch_number = int(result.group(2))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            model_state_dict = torch.load(i)\n",
    "            model.load_state_dict(model_state_dict)\n",
    "            for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(loader_codes, loader_proteomics)):\n",
    "                inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                inputs_proteomics = inputs_proteomics.to(device)\n",
    "                outputs = model(inputs_codes, inputs_proteomics, lengths_codes, interpretability=True)[1][3]\n",
    "            all_outputs.append(outputs.squeeze().cpu().numpy())\n",
    "            model_numbers.append(model_number)\n",
    "            epochs.append(epoch_number)\n",
    "            PT.append('PT')\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "model = joint_model(RNN_data_codes.shape[2], best_hidden_dim_OOL, [best_hidden_dim_OOL], best_num_layers_OOL, 1,\n",
    "                    OOL_proteomics.shape[1]-2, [], [], best_dropout_OOL).to(device)\n",
    "\n",
    "model.eval()\n",
    "model_paths = [f'./models/predictive_models/both_OOL_only_woHCE_wobs_fixed_{i}_epoch{j}.pth' for i in range(25) for j in range(200)]\n",
    "model_number = 0\n",
    "\n",
    "for i in tqdm(model_paths):\n",
    "    result = re.search('both_OOL_only_woHCE_wobs_fixed_(\\d+)_epoch(\\d+).pth', i)\n",
    "    model_number = int(result.group(1))\n",
    "    epoch_number = int(result.group(2))\n",
    "    if True:\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                model_state_dict = torch.load(i)\n",
    "                model.load_state_dict(model_state_dict)\n",
    "                for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(loader_codes, loader_proteomics)):\n",
    "                    inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                    inputs_proteomics = inputs_proteomics.to(device)\n",
    "                    outputs = model(inputs_codes, inputs_proteomics, lengths_codes, interpretability=True)[1][3]\n",
    "                all_outputs.append(outputs.squeeze().cpu().numpy())\n",
    "                model_numbers.append(model_number)\n",
    "                epochs.append(epoch_number)\n",
    "                PT.append('NPT')\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.array(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom lines for the legend\n",
    "line1 = Line2D([0], [0], color='none', marker='o', markersize=10, markerfacecolor='black', label='PT')\n",
    "line2 = Line2D([0], [0], color='none', marker='x', markersize=10, markerfacecolor='black', label='NPT')\n",
    "\n",
    "# Create a t-SNE instance and fit_transform the data\n",
    "tsne = TSNE(n_components=2, random_state=42, init='pca', learning_rate='auto')\n",
    "embedding = tsne.fit_transform(np.array(all_outputs))\n",
    "\n",
    "# Create a markers array based on PT array\n",
    "marker_map = {'PT': 'o', 'NPT': 'x'}\n",
    "markers = [marker_map[pt] for pt in PT]\n",
    "\n",
    "# Don't normalize the epochs\n",
    "colors = epochs\n",
    "\n",
    "# Create colormaps that goes from light red to dark red for PT, light blue to dark blue for NPT\n",
    "cmap_pt = LinearSegmentedColormap.from_list(\"mycmap_pt\", [\"lightcoral\", \"darkred\"])\n",
    "cmap_npt = LinearSegmentedColormap.from_list(\"mycmap_npt\", [\"lightblue\", \"darkblue\"])\n",
    "\n",
    "# Plot the results with different markers and colors\n",
    "for marker_type, marker in marker_map.items():\n",
    "    mask = np.array(markers) == marker\n",
    "    cmap = cmap_pt if marker_type == 'PT' else cmap_npt\n",
    "    sc = plt.scatter(embedding[mask, 0], embedding[mask, 1], marker=marker, c=colors[mask], cmap=cmap, alpha = 0.7)\n",
    "\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.gca().set_xticks([])\n",
    "plt.gca().set_yticks([])\n",
    "plt.title('Joint EHR-protein parameter space', fontsize=16)\n",
    "plt.legend(handles=[line1, line2])  # add the custom legend\n",
    "plt.savefig('./joint_params.png',dpi=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform t-SNE dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=42, init='pca', learning_rate='auto')\n",
    "embedding = tsne.fit_transform(np.array(all_outputs))\n",
    "\n",
    "# Create colormaps that go from light red to dark red for PT, and light blue to dark blue for NPT\n",
    "cmap_pt = LinearSegmentedColormap.from_list(\"mycmap_pt\", [\"lightcoral\", \"darkred\"])\n",
    "cmap_npt = LinearSegmentedColormap.from_list(\"mycmap_npt\", [\"lightblue\", \"darkblue\"])\n",
    "\n",
    "fig, axs = plt.subplots(5, 5, figsize=(20, 20))\n",
    "\n",
    "# Calculate the global x and y limits\n",
    "xlim = (embedding[:,0].min()-1, embedding[:,0].max()+1)\n",
    "ylim = (embedding[:,1].min()-1, embedding[:,1].max()+1)\n",
    "\n",
    "for model_number in range(25):\n",
    "    ax = axs[model_number // 5, model_number % 5]\n",
    "\n",
    "    for model_type in ['PT', 'NPT']:\n",
    "        # Get mask for the current trajectory\n",
    "        mask = (np.array(model_numbers) == model_number) & (np.array(PT) == model_type)\n",
    "\n",
    "        # Get the points and corresponding epochs for the current trajectory\n",
    "        trajectory_points = embedding[mask]\n",
    "        trajectory_epochs = np.array(epochs)[mask]\n",
    "\n",
    "        # Sort the points and epochs\n",
    "        sort_indices = np.argsort(trajectory_epochs)\n",
    "        sorted_points = trajectory_points[sort_indices]\n",
    "\n",
    "        # Select the colormap based on model_type\n",
    "        cmap = cmap_pt if model_type == 'PT' else cmap_npt\n",
    "\n",
    "        # Plot the points with color indicating epoch and marker indicating PT/NPT\n",
    "        ax.scatter(sorted_points[:, 0], sorted_points[:, 1], \n",
    "                   c=trajectory_epochs[sort_indices], \n",
    "                   cmap=cmap, \n",
    "                   marker=marker_map[model_type], \n",
    "                   alpha=0.6)\n",
    "\n",
    "        # Plot lines connecting the points of the same model type\n",
    "        ax.plot(sorted_points[:, 0], sorted_points[:, 1], \n",
    "                color='lightgrey' if model_type == 'PT' else 'black', \n",
    "                linestyle='--')\n",
    "\n",
    "    ax.set_aspect('auto')\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "    # Turn off tick labels\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./joint_paths_ool.png',dpi=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lastly, EHR predictions (not latent space like above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return final_pred, (out_ehr, pred_proteomics, pred_ehr, out_combined, final_pred, self.final_combine.weight)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = joint_model(RNN_data_codes.shape[2], best_hidden_dim, [best_hidden_dim], best_num_layers, 1,\n",
    "                    OOL_proteomics.shape[1]-2, [], [], best_dropout).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "proteomics = OOL_proteomics.merge(patient_indices[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID','DOS'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "proteomics = scaler.fit_transform(proteomics)\n",
    "\n",
    "EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes]  \n",
    "EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "proteomics = torch.tensor(proteomics).float()\n",
    "proteomics = torch.nan_to_num(proteomics)\n",
    "outcomes = torch.tensor(RNN_data_outcomes).float()\n",
    "\n",
    "\n",
    "data_set = DataBuilder(proteomics, outcomes, scaler)\n",
    "loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "loader_codes = create_dataloaders(EHR_codes, outcomes, RNN_data_lengths, 100000)\n",
    "            \n",
    "model_paths = [f'./models/predictive_models/both_OOL_PT_FT_woHCE_wobs_fixed_{i}_epoch{j}.pth' for i in range(25) for j in range(200)]\n",
    "model_number = 0\n",
    "\n",
    "all_outputs = []\n",
    "model_numbers = []\n",
    "epochs = []\n",
    "PT = []\n",
    "\n",
    "for i in tqdm(model_paths):\n",
    "    result = re.search('both_OOL_PT_FT_woHCE_wobs_fixed_(\\d+)_epoch(\\d+).pth', i)\n",
    "    model_number = int(result.group(1))\n",
    "    epoch_number = int(result.group(2))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            model_state_dict = torch.load(i)\n",
    "            model.load_state_dict(model_state_dict)\n",
    "            for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(loader_codes, loader_proteomics)):\n",
    "                inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                inputs_proteomics = inputs_proteomics.to(device)\n",
    "                outputs = model(inputs_codes, inputs_proteomics, lengths_codes, interpretability=True)[1][2]\n",
    "            all_outputs.append(outputs.squeeze().cpu().numpy())\n",
    "            model_numbers.append(model_number)\n",
    "            epochs.append(epoch_number)\n",
    "            PT.append('PT')\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "model = joint_model(RNN_data_codes.shape[2], best_hidden_dim_OOL, [best_hidden_dim_OOL], best_num_layers_OOL, 1,\n",
    "                    OOL_proteomics.shape[1]-2, [], [], best_dropout_OOL).to(device)\n",
    "\n",
    "model.eval() \n",
    "model_paths = [f'./models/predictive_models/both_OOL_only_woHCE_wobs_fixed_{i}_epoch{j}.pth' for i in range(25) for j in range(200)]\n",
    "model_number = 0\n",
    "\n",
    "for i in tqdm(model_paths):\n",
    "    result = re.search('both_OOL_only_woHCE_wobs_fixed_(\\d+)_epoch(\\d+).pth', i)\n",
    "    model_number = int(result.group(1))\n",
    "    epoch_number = int(result.group(2))\n",
    "    if True:\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                model_state_dict = torch.load(i)\n",
    "                model.load_state_dict(model_state_dict)\n",
    "                for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(loader_codes, loader_proteomics)):\n",
    "                    inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                    inputs_proteomics = inputs_proteomics.to(device)\n",
    "                    outputs = model(inputs_codes, inputs_proteomics, lengths_codes, interpretability=True)[1][2]\n",
    "                all_outputs.append(outputs.squeeze().cpu().numpy())\n",
    "                model_numbers.append(model_number)\n",
    "                epochs.append(epoch_number)\n",
    "                PT.append('NPT')\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.array(epochs)\n",
    "# Create custom lines for the legend\n",
    "line1 = Line2D([0], [0], color='none', marker='o', markersize=10, markerfacecolor='black', label='PT')\n",
    "line2 = Line2D([0], [0], color='none', marker='x', markersize=10, markerfacecolor='black', label='NPT')\n",
    "\n",
    "# Create a t-SNE instance and fit_transform the data\n",
    "tsne = TSNE(n_components=2, random_state=42, init='pca', learning_rate='auto')\n",
    "embedding = tsne.fit_transform(np.array(all_outputs))\n",
    "\n",
    "# Create a markers array based on PT array\n",
    "marker_map = {'PT': 'o', 'NPT': 'x'}\n",
    "markers = [marker_map[pt] for pt in PT]\n",
    "\n",
    "# Don't normalize the epochs\n",
    "colors = epochs\n",
    "\n",
    "# Create colormaps that goes from light red to dark red for PT, light blue to dark blue for NPT\n",
    "cmap_pt = LinearSegmentedColormap.from_list(\"mycmap_pt\", [\"lightcoral\", \"darkred\"])\n",
    "cmap_npt = LinearSegmentedColormap.from_list(\"mycmap_npt\", [\"lightblue\", \"darkblue\"])\n",
    "\n",
    "# Plot the results with different markers and colors\n",
    "for marker_type, marker in marker_map.items():\n",
    "    mask = np.array(markers) == marker\n",
    "    cmap = cmap_pt if marker_type == 'PT' else cmap_npt\n",
    "    sc = plt.scatter(embedding[mask, 0], embedding[mask, 1], marker=marker, c=colors[mask], cmap=cmap, alpha = 0.7)\n",
    "\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.gca().set_xticks([])\n",
    "plt.gca().set_yticks([])\n",
    "plt.title('EHR parameter space', fontsize=14)\n",
    "plt.legend(handles=[line1, line2])  # add the custom legend\n",
    "plt.savefig('./EHR_param_space.png',dpi=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform t-SNE dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=42, init='pca', learning_rate='auto')\n",
    "embedding = tsne.fit_transform(np.array(all_outputs))\n",
    "\n",
    "# Create colormaps that go from light red to dark red for PT, and light blue to dark blue for NPT\n",
    "cmap_pt = LinearSegmentedColormap.from_list(\"mycmap_pt\", [\"lightcoral\", \"darkred\"])\n",
    "cmap_npt = LinearSegmentedColormap.from_list(\"mycmap_npt\", [\"lightblue\", \"darkblue\"])\n",
    "\n",
    "fig, axs = plt.subplots(5, 5, figsize=(20, 20))\n",
    "\n",
    "# Calculate the global x and y limits\n",
    "xlim = (embedding[:,0].min()-1, embedding[:,0].max()+1)\n",
    "ylim = (embedding[:,1].min()-1, embedding[:,1].max()+1)\n",
    "\n",
    "for model_number in range(25):\n",
    "    ax = axs[model_number // 5, model_number % 5]\n",
    "\n",
    "    for model_type in ['PT', 'NPT']:\n",
    "        # Get mask for the current trajectory\n",
    "        mask = (np.array(model_numbers) == model_number) & (np.array(PT) == model_type)\n",
    "\n",
    "        # Get the points and corresponding epochs for the current trajectory\n",
    "        trajectory_points = embedding[mask]\n",
    "        trajectory_epochs = np.array(epochs)[mask]\n",
    "\n",
    "        # Sort the points and epochs\n",
    "        sort_indices = np.argsort(trajectory_epochs)\n",
    "        sorted_points = trajectory_points[sort_indices]\n",
    "\n",
    "        # Select the colormap based on model_type\n",
    "        cmap = cmap_pt if model_type == 'PT' else cmap_npt\n",
    "\n",
    "        # Plot the points with color indicating epoch and marker indicating PT/NPT\n",
    "        ax.scatter(sorted_points[:, 0], sorted_points[:, 1], \n",
    "                   c=trajectory_epochs[sort_indices], \n",
    "                   cmap=cmap, \n",
    "                   marker=marker_map[model_type], \n",
    "                   alpha=0.6)\n",
    "\n",
    "        # Plot lines connecting the points of the same model type\n",
    "        ax.plot(sorted_points[:, 0], sorted_points[:, 1], \n",
    "                color='lightgrey' if model_type == 'PT' else 'black', \n",
    "                linestyle='--')\n",
    "\n",
    "    ax.set_aspect('auto')\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "    # Turn off tick labels\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./EHR_paths_ool.png',dpi=1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
