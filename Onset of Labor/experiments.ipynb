{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c7f02e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, cross_val_predict, train_test_split, KFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, roc_curve, balanced_accuracy_score, accuracy_score, mean_squared_error\n",
    "from scipy.stats import pearsonr, ttest_rel\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE, Isomap\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import LinearSegmentedColormap, ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "import loralib as lora\n",
    "import random\n",
    "import umap\n",
    "import re\n",
    "from itertools import product\n",
    "import os\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0571fbb7",
   "metadata": {},
   "source": [
    "### Initial Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b3fa18b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load proteomics data\n",
    "OOL_proteomics = pd.read_csv('./data/processed_data/ool_proteomics_omop_id.csv')\n",
    "OOL_proteomics['sample_ID'] = OOL_proteomics['maternal_person_id'].astype(str)+'_'+OOL_proteomics['Timepoint'].astype(str)\n",
    "OOL_proteomics = OOL_proteomics.drop(['Timepoint','maternal_person_id'],axis=1)\n",
    "OOL_proteomics.columns = [str(i)+'_protein' for i in OOL_proteomics.columns]\n",
    "OOL_proteomics = OOL_proteomics.rename(columns={'DOS_protein':'DOS', 'sample_ID_protein':'sample_ID'})\n",
    "OOL_proteomics = OOL_proteomics.drop(['child_person_id_protein','sample_date_protein','child_birth_date_protein'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "34fbf087",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load outcomes\n",
    "OOL_outcomes = OOL_proteomics[['sample_ID','DOS']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e55ff2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#omics only architecture \n",
    "class proteomics_net(nn.Module):\n",
    "    def __init__(self, input_size, var_not_used, output_size, dropout=0.1):\n",
    "        super(proteomics_net, self).__init__()\n",
    "        \n",
    "        self.proteomics_layers = nn.Sequential(\n",
    "            nn.Linear(input_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proteomics_layers(x)\n",
    "        return x\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        pretrained_dict = torch.load(path, map_location=lambda storage, loc: storage)\n",
    "        model_dict = self.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        self.load_state_dict(model_dict)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2e93b969",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EHR architecture\n",
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_size_codes, hidden_size, prediction_module_hidden_sizes, num_layers, output_size, dropout=0.1):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size_codes, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        self.prediction_module = nn.Sequential(\n",
    "            nn.Linear(prediction_module_hidden_sizes[0], output_size)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, lengths, interpretability=False):\n",
    "        device = x.device\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        x_packed = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        out_packed, _ = self.gru(x_packed, h0)\n",
    "        out, _ = pad_packed_sequence(out_packed, batch_first=True)\n",
    "        \n",
    "        out = out[torch.arange(x.size(0)), lengths-1, :]\n",
    "        out_final = self.prediction_module(out)\n",
    "        if interpretability == False:\n",
    "            return out_final\n",
    "        else:\n",
    "            return out_final, out\n",
    "\n",
    "# Prepare the dataset\n",
    "class PatientDataset(Dataset):\n",
    "    def __init__(self, data, labels, lengths):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.lengths = lengths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx], self.lengths[idx]\n",
    "\n",
    "# Custom collate function for DataLoader\n",
    "def collate_fn(batch):\n",
    "    data, labels, lengths = zip(*batch)\n",
    "    data = pad_sequence(data, batch_first=True, padding_value=0)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "    return data, labels, lengths\n",
    "\n",
    "\n",
    "def normalize_dataset(dataset):\n",
    "    for i, sequence in enumerate(dataset):\n",
    "        dataset[i] = (sequence - sequence.mean(dim=0, keepdim=True)) / (sequence.std(dim=0, keepdim=True) + 1e-8)\n",
    "    return dataset\n",
    "\n",
    "def impute_missing_values(dataset):\n",
    "    # Stack all tensors in the dataset along a new dimension, creating a tensor of shape (num_samples, max_seq_length, num_features)\n",
    "    stacked_data = torch.stack(dataset)\n",
    "\n",
    "    # Calculate the mean of each feature across all samples and sequences, ignoring NaN values\n",
    "    feature_means = torch.nanmean(stacked_data, dim=(0, 1))\n",
    "\n",
    "    # Iterate through the dataset (list of tensors)\n",
    "    for i, sequence in enumerate(dataset):\n",
    "        # Create a boolean mask indicating the positions of NaN values in the sequence\n",
    "        mask = torch.isnan(sequence)\n",
    "\n",
    "        # Replace NaN values in the sequence with the corresponding feature means\n",
    "        # 'expand_as' is used to match the dimensions of the mask and the sequence\n",
    "        dataset[i][mask] = feature_means.expand_as(sequence)[mask]\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Create DataLoaders\n",
    "def create_dataloaders(patient_data, patient_outcomes, lengths, batch_size=64, normalize=False):\n",
    "    \n",
    "    X_train = impute_missing_values(patient_data)\n",
    "    y_train = patient_outcomes\n",
    "    \n",
    "    if normalize:\n",
    "        X_train = normalize_dataset(X_train)\n",
    "    \n",
    "    y_train = (y_train-y_train.mean()) / y_train.std()\n",
    "    \n",
    "    train_dataset = PatientDataset(X_train, y_train, lengths)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, worker_init_fn=worker_init_fn)\n",
    "    return train_loader\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "def worker_init_fn(worker_id):\n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "60e9793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full COMET framework architecture\n",
    "class joint_model(nn.Module):\n",
    "    def __init__(self, input_size_codes, hidden_size, prediction_module_hidden_sizes, num_layers, output_size, input_size_proteomics, var_not_used, combined_hidden_layers, dropout=0.1):\n",
    "        super(joint_model, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size_codes, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "    \n",
    "        self.prediction_module = nn.Sequential(\n",
    "            nn.Linear(prediction_module_hidden_sizes[0], output_size)\n",
    "        )\n",
    "\n",
    "        self.skip_connect_prot = nn.Linear(input_size_proteomics, output_size)\n",
    "        \n",
    "        self.combined_layers = nn.Sequential(\n",
    "            nn.Linear(input_size_proteomics + hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "        self.final_combine = nn.Linear(3, 1, bias=False)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, x_proteomics, lengths, interpretability=False, better_latent=None, better_ratio=0.5):\n",
    "        device = x.device\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        x_packed = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        out_packed, _ = self.gru(x_packed, h0)\n",
    "        out, _ = pad_packed_sequence(out_packed, batch_first=True)\n",
    "        \n",
    "        out_ehr = out[torch.arange(x.size(0)), lengths-1, :]\n",
    "        \n",
    "        if x_proteomics == None:\n",
    "            return out_ehr\n",
    "\n",
    "        if better_latent is not None:\n",
    "            out_ehr = better_ratio * better_latent + (1-better_ratio) * out_ehr\n",
    "                \n",
    "        out_combined = torch.cat((out_ehr, x_proteomics), 1)\n",
    "        \n",
    "        out_combined = self.combined_layers(out_combined)\n",
    "        \n",
    "        pred_proteomics = self.skip_connect_prot(x_proteomics)\n",
    "        pred_ehr = self.prediction_module(out_ehr)\n",
    "        \n",
    "        final_pred = self.final_combine(torch.cat((pred_proteomics, pred_ehr, out_combined), 1))\n",
    "        \n",
    "        if interpretability == False:\n",
    "            return final_pred\n",
    "        else:\n",
    "            return final_pred, (out_ehr, pred_proteomics, pred_ehr, out_combined, final_pred, self.final_combine.weight)\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        pretrained_dict = torch.load(path, map_location=lambda storage, loc: storage)\n",
    "        model_dict = self.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        self.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a73712b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBuilder(Dataset):\n",
    "    def __init__(self, x, y, standardizer):\n",
    "        self.x, self.y, self.standardizer = x, y, standardizer\n",
    "        self.len=self.x.shape[0]\n",
    "    def __getitem__(self,index):      \n",
    "        return (self.x[index], self.y[index])\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a8372fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(EHR_codes, proteomics, patient_indices, outcomes, lengths, experiment_name, lr, lr_decay,\n",
    "                   bs, train_indices=None, val_indices=None, test_indices=None, feature_types='EHR', model_path='', fine_tune=False, seed=42, num_layers=2,hidden_dim=400,\n",
    "                   dropout=0.4, return_preds=False, return_interpretability=False, return_grads=False,\n",
    "                   hyperparam_tuning=False):\n",
    "    \"\"\"\n",
    "    EHR_codes: pre-processed data for codes of shape (num_patients, max_length, embedding_dim)v\n",
    "    EHR_vitals: pre-processed data for codes of shape (num_patients, max_length, embedding_dim)\n",
    "    proteomics: dataframe with proteomics data\n",
    "    patient_indices: dataframe with sample IDs and row numbers in pre-processed matrices\n",
    "    outcomes: array with DOS\n",
    "    lengths: array with lengths (i.e. number of visits) to help with padding\n",
    "    experiment_name: string for file name for models\n",
    "    lr: float for learning rate\n",
    "    lr_decay: float for learning rate decay\n",
    "    bs: int for batch size\n",
    "    feature_types: string either 'EHR', 'metab', 'both'\n",
    "    model_path: string for file path to model if loading a pre-trained model\n",
    "    fine_tune: boolean for whether or not EHR weight should be learned, can only be true if model != ''\n",
    "    seed: int, random_seed for train/test/val split and seeding model \n",
    "    num_layers: number of GRU layers in RNN\n",
    "    hidden_dim: hidden_dim of GRU output\n",
    "    dropout: dropout weight in model\n",
    "    return_preds: setting to control output of function, if True we return the predictions\n",
    "    return_interpretability: setting to control output of function, if True we return some additional data to help with interpretability analysis\n",
    "    return_grads: setting to control output of function, if True we return the gradient\n",
    "    hyperparam_tuning: setting to control whether or not we save the model at each epoch (if True, we do not)\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "    prediction_module_hidden_sizes = [hidden_dim,hidden_dim//2, hidden_dim//4, hidden_dim//8]\n",
    "    \n",
    "    assert feature_types in ['EHR','metab','both']   \n",
    "    if feature_types == 'metab': assert model_path == ''\n",
    "    if (model_path != '') & (feature_types == 'both'): assert fine_tune==True\n",
    "    if hyperparam_tuning == False: assert train_indices == None\n",
    "\n",
    "    if hyperparam_tuning == False:\n",
    "        maternal_IDs = patient_indices['sample_ID'].str[0:7].unique()\n",
    "\n",
    "        train_ratio = 0.70\n",
    "        test_ratio = 0.15\n",
    "        val_ratio = 0.15\n",
    "\n",
    "        # First, split the unique_ids into train and temp (test + validation) sets\n",
    "        train_ids, temp_ids = train_test_split(maternal_IDs, test_size=(test_ratio + val_ratio),random_state=seed)\n",
    "\n",
    "        # Next, split the temp_ids into test and validation sets\n",
    "        test_ids, val_ids = train_test_split(temp_ids, test_size=(val_ratio / (test_ratio + val_ratio)), random_state=seed)\n",
    "        patient_indices['maternal_ID'] = patient_indices['sample_ID'].str[0:7]\n",
    "        patient_indices['maternal_ID_ts'] = patient_indices['maternal_ID'].astype(str)+'_'+patient_indices['sample_ID'].str[-2:]\n",
    "        proteomics = proteomics.merge(patient_indices[['maternal_ID_ts','array_index']], how='left', left_on='sample_ID', right_on='maternal_ID_ts').drop(['sample_ID','maternal_ID_ts','DOS'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "\n",
    "        train_indices = patient_indices[patient_indices['maternal_ID'].isin(train_ids)]['array_index'].values\n",
    "        np.random.shuffle(train_indices)\n",
    "        test_indices = patient_indices[patient_indices['maternal_ID'].isin(test_ids)]['array_index'].values\n",
    "        val_indices = patient_indices[patient_indices['maternal_ID'].isin(val_ids)]['array_index'].values\n",
    "    \n",
    "    #data processing for train data to prepare for input to ML model\n",
    "    train_EHR_codes = EHR_codes[train_indices,:,:]\n",
    "    if feature_types != 'EHR':\n",
    "        train_proteomics = proteomics[train_indices,:]\n",
    "        scaler = StandardScaler()\n",
    "        train_proteomics = scaler.fit_transform(train_proteomics)\n",
    "    train_outcomes = outcomes[train_indices]\n",
    "    mean_train_outcomes = np.mean(train_outcomes)\n",
    "    sd_train_outcomes = np.std(train_outcomes)\n",
    "    train_outcomes = (train_outcomes - np.mean(train_outcomes))/np.std(train_outcomes)\n",
    "    train_lengths = lengths[train_indices]\n",
    "    \n",
    "    #data processing for test data to prepare for input to ML model\n",
    "    test_EHR_codes = EHR_codes[test_indices, :, :]\n",
    "    if feature_types != 'EHR':\n",
    "        test_proteomics = proteomics[test_indices, :]\n",
    "        scaler = StandardScaler()\n",
    "        test_proteomics = scaler.fit_transform(test_proteomics)\n",
    "    test_outcomes = outcomes[test_indices]\n",
    "    mean_test_outcomes = np.mean(test_outcomes)\n",
    "    sd_test_outcomes = np.std(test_outcomes)\n",
    "    test_outcomes = (test_outcomes - np.mean(test_outcomes))/np.std(test_outcomes)\n",
    "    test_lengths = lengths[test_indices]\n",
    "    \n",
    "    #data processing for val data to prepare for input to ML model\n",
    "    val_EHR_codes = EHR_codes[val_indices, :, :]\n",
    "    if feature_types != 'EHR':\n",
    "        val_proteomics = proteomics[val_indices, :]\n",
    "        scaler = StandardScaler()\n",
    "        val_proteomics = scaler.fit_transform(val_proteomics)\n",
    "    val_outcomes = outcomes[val_indices]\n",
    "    mean_val_outcomes = np.mean(val_outcomes)\n",
    "    sd_val_outcomes = np.std(val_outcomes)\n",
    "    val_outcomes = (val_outcomes - np.mean(val_outcomes))/np.std(val_outcomes)\n",
    "    val_lengths = lengths[val_indices]\n",
    "    \n",
    "    #data processing for all data to prepare for input to ML model\n",
    "    all_EHR_codes = EHR_codes\n",
    "    scaler = StandardScaler()\n",
    "    all_outcomes = outcomes\n",
    "    all_outcomes = (all_outcomes - np.mean(all_outcomes))/np.std(all_outcomes)\n",
    "    all_lengths = lengths\n",
    "\n",
    "    #additional training data processesing\n",
    "    train_EHR_codes = [torch.tensor(data).float() for data in train_EHR_codes]  \n",
    "    train_EHR_codes = [torch.nan_to_num(x) for x in train_EHR_codes]\n",
    "    if feature_types != 'EHR':\n",
    "        train_proteomics = torch.tensor(train_proteomics).float()\n",
    "        train_proteomics = torch.nan_to_num(train_proteomics)\n",
    "    train_outcomes = torch.tensor(train_outcomes).float()\n",
    "    \n",
    "    #additional test data processesing\n",
    "    test_EHR_codes = [torch.tensor(data).float() for data in test_EHR_codes]\n",
    "    test_EHR_codes = [torch.nan_to_num(x) for x in test_EHR_codes]\n",
    "    if feature_types != 'EHR':\n",
    "        test_proteomics = torch.tensor(test_proteomics).float()\n",
    "        test_proteomics = torch.nan_to_num(test_proteomics)\n",
    "    test_outcomes = torch.tensor(test_outcomes).float()\n",
    "\n",
    "    #additional validation data processesing\n",
    "    val_EHR_codes = [torch.tensor(data).float() for data in val_EHR_codes]\n",
    "    val_EHR_codes = [torch.nan_to_num(x) for x in val_EHR_codes]\n",
    "    if feature_types != 'EHR':\n",
    "        val_proteomics = torch.tensor(val_proteomics).float()\n",
    "        val_proteomics = torch.nan_to_num(val_proteomics)\n",
    "    val_outcomes = torch.tensor(val_outcomes).float()\n",
    "\n",
    "    #additional all data processesing\n",
    "    all_EHR_codes = [torch.tensor(data).float() for data in all_EHR_codes]\n",
    "    all_EHR_codes = [torch.nan_to_num(x) for x in all_EHR_codes]\n",
    "    all_outcomes = torch.tensor(all_outcomes).float()\n",
    "\n",
    "    #If omics data is used, create a dataloader for omics data\n",
    "    if feature_types != 'EHR':\n",
    "        data_set_train = DataBuilder(train_proteomics, train_outcomes, scaler)\n",
    "        train_loader_proteomics = DataLoader(dataset=data_set_train,batch_size=bs, worker_init_fn=worker_init_fn)\n",
    "    \n",
    "    #Create a dataloader for EHR data\n",
    "    train_loader_codes = create_dataloaders(train_EHR_codes, train_outcomes, train_lengths, bs)\n",
    "\n",
    "    if feature_types != 'EHR':\n",
    "        data_set_test = DataBuilder(test_proteomics, test_outcomes, scaler)\n",
    "        test_loader_proteomics = DataLoader(dataset=data_set_test,batch_size=bs, worker_init_fn=worker_init_fn)\n",
    "    test_loader_codes = create_dataloaders(test_EHR_codes, test_outcomes, test_lengths, bs)\n",
    "\n",
    "    if feature_types != 'EHR':\n",
    "        data_set_val = DataBuilder(val_proteomics, val_outcomes, scaler)\n",
    "        val_loader_proteomics = DataLoader(dataset=data_set_val,batch_size=100*bs, worker_init_fn=worker_init_fn)\n",
    "    val_loader_codes = create_dataloaders(val_EHR_codes, val_outcomes, val_lengths, 100*bs)\n",
    "            \n",
    "    all_loader_codes = create_dataloaders(all_EHR_codes, all_outcomes, all_lengths, 1000)   \n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    #For baseline experiments, initialize the correct type of model based on the features used\n",
    "    if model_path == '':\n",
    "        if feature_types == 'EHR':\n",
    "            model = GRUNet(EHR_codes.shape[2], hidden_dim, prediction_module_hidden_sizes, num_layers, 1, dropout).to(device)\n",
    "        elif feature_types == 'metab':\n",
    "            model = proteomics_net(proteomics.shape[1], None, 1, dropout).to(device)\n",
    "        elif feature_types == 'both':\n",
    "            model = joint_model(EHR_codes.shape[2], hidden_dim, prediction_module_hidden_sizes, num_layers, 1, proteomics.shape[1], [1024, 512, 256, 128], [64, 32, 16, 8], dropout).to(device)\n",
    "    #For COMET experiments\n",
    "    else:\n",
    "        #Load correct model architecture based on whether or not we use omics data\n",
    "        if feature_types == 'EHR':\n",
    "            model = GRUNet(EHR_codes.shape[2], hidden_dim, prediction_module_hidden_sizes, num_layers, 1, dropout)\n",
    "            model_state_dict = torch.load(model_path)\n",
    "            model.load_state_dict(model_state_dict)\n",
    "            model.to(device)\n",
    "            #If no fine tuning, that means we are assessing the pre-trained model on the omics data with no additional training\n",
    "            if fine_tune == False:\n",
    "                model.eval()\n",
    "                criterion = nn.MSELoss()\n",
    "                val_predictions = []\n",
    "                val_true_labels = []\n",
    "                running_loss_val, num_samples_val = 0, 0\n",
    "                with torch.no_grad():\n",
    "                    for (inputs_codes, labels_codes, lengths_codes) in (val_loader_codes):\n",
    "                            inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                            outputs = model(inputs_codes, lengths_codes)\n",
    "                            \n",
    "                            mean_tensor = torch.tensor(mean_val_outcomes, dtype=torch.float32, device=device)\n",
    "                            std_tensor = torch.tensor(sd_val_outcomes, dtype=torch.float32, device=device)\n",
    "\n",
    "                            denormalized_outputs = outputs.squeeze() * std_tensor + mean_tensor\n",
    "                            denormalized_labels = labels * std_tensor + mean_tensor\n",
    "                \n",
    "                            loss = criterion(denormalized_outputs, denormalized_labels)\n",
    "#                             loss = criterion(outputs.squeeze(), labels)\n",
    "                            running_loss_val += (loss.item()*lengths_codes.shape[0])\n",
    "                            num_samples_val += lengths_codes.shape[0]\n",
    "                            val_predictions.extend(outputs.squeeze().tolist())\n",
    "                            val_true_labels.extend(labels.tolist())\n",
    "                    \n",
    "                val_loss = running_loss_val / (num_samples_val)\n",
    "                pearson_corr, _ = pearsonr(val_predictions, val_true_labels)\n",
    "                original_val_outcomes = outcomes[val_indices]\n",
    "                val_outcome_mean, val_outcome_sd = np.mean(original_val_outcomes), np.std(original_val_outcomes)\n",
    "                val_rmse = np.sqrt(mean_squared_error(original_val_outcomes, np.array(val_predictions)*val_outcome_sd+val_outcome_mean))\n",
    "\n",
    "                print(f'Total Loss: {val_loss:.4f}, Pearson R: {pearson_corr:.4f}, RMSE: {val_rmse:.4f}')\n",
    "                if return_preds == True:\n",
    "                    return pearson_corr, val_loss, val_rmse, val_true_labels, val_predictions, val_indices\n",
    "                else:\n",
    "                    return pearson_corr, val_loss, val_rmse\n",
    "            #if we are fine-tuning, freeze the GRU weights\n",
    "            elif fine_tune == True:\n",
    "                model = GRUNet(EHR_codes.shape[2], hidden_dim, prediction_module_hidden_sizes, num_layers, 1, dropout)\n",
    "                model_state_dict = torch.load(model_path)\n",
    "                model.load_state_dict(model_state_dict, strict=False)\n",
    "                model.to(device)\n",
    "                for name, param in model.named_parameters():\n",
    "                    if ('gru' in name):\n",
    "                        param.requires_grad = False\n",
    "        #use the correct model if we include omics data along with the transferred weights from the EHR model\n",
    "        elif feature_types == 'both':\n",
    "            model = joint_model(EHR_codes.shape[2], hidden_dim, prediction_module_hidden_sizes, num_layers, 1, proteomics.shape[1], [1024, 512, 256, 128], [64, 32, 16, 8], dropout).to(device)\n",
    "            model_state_dict = torch.load(model_path)\n",
    "            gru_weights = {}\n",
    "            for k,v in zip(model_state_dict.keys(), model_state_dict.values()):\n",
    "                if ('gru' in k) | ('pred' in k):\n",
    "                    gru_weights[k] = v\n",
    "            model.load_state_dict(gru_weights, strict=False)\n",
    "            model.to(device)\n",
    "            \n",
    "            for name, param in model.named_parameters():\n",
    "                if ('gru' in name):\n",
    "                    param.requires_grad = False\n",
    "    \n",
    "\n",
    "    #prepare to track relevant info during training\n",
    "    epoch_arr = []\n",
    "    num_samples_in_batch = []\n",
    "    gradient_arr = []\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=lr_decay)\n",
    "    num_epochs = 100\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    val_losses = []\n",
    "    best_loss = np.inf\n",
    "    #training loop\n",
    "    for epoch in (range(num_epochs)):\n",
    "        model.train()\n",
    "        #if we're not doing hyperparameter tuning, save the model at each epoch for downstream analysis\n",
    "        if hyperparam_tuning == False:\n",
    "            torch.save(model.state_dict(), './models/predictive_models/{}_epoch{}.pth'.format(experiment_name, epoch))\n",
    "        running_loss_train, num_train_samples = 0, 0\n",
    "        if feature_types == 'EHR':\n",
    "            for (inputs_codes, labels_codes, lengths_codes)in train_loader_codes:\n",
    "                if feature_types != 'metab':\n",
    "                    inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "\n",
    "                outputs = model(inputs_codes, lengths_codes)\n",
    "\n",
    "\n",
    "                mean_tensor = torch.tensor(mean_train_outcomes, dtype=torch.float32, device=device)\n",
    "                std_tensor = torch.tensor(sd_train_outcomes, dtype=torch.float32, device=device)\n",
    "\n",
    "                denormalized_outputs = outputs.squeeze() * std_tensor + mean_tensor\n",
    "                denormalized_labels = labels * std_tensor + mean_tensor\n",
    "\n",
    "                loss = criterion(denormalized_outputs, denormalized_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss_train += (loss.item()*lengths_codes.shape[0])\n",
    "                num_train_samples += lengths_codes.shape[0]\n",
    "        else:\n",
    "            for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(train_loader_codes, train_loader_proteomics)):\n",
    "                if feature_types != 'metab':\n",
    "                    inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                if feature_types != 'EHR':\n",
    "                    inputs_proteomics, labels = inputs_proteomics.to(device), labels_proteomics.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                if feature_types == 'metab':\n",
    "                    outputs = model(inputs_proteomics)\n",
    "                elif feature_types == 'EHR':\n",
    "                    outputs = model(inputs_codes, lengths_codes)\n",
    "                elif feature_types == 'both':\n",
    "                    outputs, interpretability_outputs = model(inputs_codes, inputs_proteomics, lengths_codes, interpretability=True)\n",
    "\n",
    "\n",
    "                mean_tensor = torch.tensor(mean_train_outcomes, dtype=torch.float32, device=device)\n",
    "                std_tensor = torch.tensor(sd_train_outcomes, dtype=torch.float32, device=device)\n",
    "\n",
    "                denormalized_outputs = outputs.squeeze() * std_tensor + mean_tensor\n",
    "                denormalized_labels = labels * std_tensor + mean_tensor\n",
    "\n",
    "                loss = criterion(denormalized_outputs, denormalized_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if return_grads:\n",
    "                    epoch_arr.append(epoch)\n",
    "                    num_samples_in_batch.append(outputs.shape[0])\n",
    "                    gradient_arr.append(model.skip_connect_prot.weight.grad.cpu().numpy()[0])\n",
    "                running_loss_train += (loss.item()*lengths_codes.shape[0])\n",
    "                num_train_samples += lengths_codes.shape[0]\n",
    "\n",
    "        train_loss = running_loss_train / num_train_samples\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        running_loss_test, num_samples_test = 0, 0\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        if feature_types == 'EHR':\n",
    "            with torch.no_grad():\n",
    "                for (inputs_codes, labels_codes, lengths_codes)in test_loader_codes:\n",
    "                    inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "\n",
    "                    outputs = model(inputs_codes, lengths_codes)\n",
    "                    \n",
    "                    mean_tensor = torch.tensor(mean_test_outcomes, dtype=torch.float32, device=device)\n",
    "                    std_tensor = torch.tensor(sd_test_outcomes, dtype=torch.float32, device=device)\n",
    "\n",
    "                    denormalized_outputs = outputs.squeeze() * std_tensor + mean_tensor\n",
    "                    denormalized_labels = labels * std_tensor + mean_tensor\n",
    "                   \n",
    "                    loss = criterion(denormalized_outputs, denormalized_labels)\n",
    "                    running_loss_test += (loss.item()*lengths_codes.shape[0])\n",
    "                    num_samples_test += lengths_codes.shape[0]\n",
    "                    predictions.extend(outputs.squeeze().tolist())\n",
    "                    true_labels.extend(labels.tolist())\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(test_loader_codes, test_loader_proteomics)):\n",
    "                    if feature_types != 'metab':\n",
    "                        inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                    if feature_types != 'EHR':\n",
    "                        inputs_proteomics, labels = inputs_proteomics.to(device), labels_proteomics.to(device)\n",
    "\n",
    "                    if feature_types == 'metab':\n",
    "                        outputs = model(inputs_proteomics)\n",
    "                    elif feature_types == 'EHR':\n",
    "                        outputs = model(inputs_codes, lengths_codes)\n",
    "                    elif feature_types == 'both':\n",
    "                        outputs = model(inputs_codes, inputs_proteomics, lengths_codes)\n",
    "                    \n",
    "                    mean_tensor = torch.tensor(mean_test_outcomes, dtype=torch.float32, device=device)\n",
    "                    std_tensor = torch.tensor(sd_test_outcomes, dtype=torch.float32, device=device)\n",
    "\n",
    "                    denormalized_outputs = outputs.squeeze() * std_tensor + mean_tensor\n",
    "                    denormalized_labels = labels * std_tensor + mean_tensor\n",
    "                    \n",
    "                    loss = criterion(denormalized_outputs, denormalized_labels)\n",
    "                    running_loss_test += (loss.item()*lengths_codes.shape[0])\n",
    "                    num_samples_test += lengths_codes.shape[0]\n",
    "                    predictions.extend(outputs.squeeze().tolist())\n",
    "                    true_labels.extend(labels.tolist())\n",
    "        test_loss = running_loss_test / (num_samples_test)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        patience = 5\n",
    "        pearson_corr, _ = pearsonr(predictions, true_labels)\n",
    "\n",
    "#         print(f'Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Pearson R: {pearson_corr:.4f}')\n",
    "        #check early stopping criteria\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            counter = 0\n",
    "            torch.save(model.state_dict(), './models/predictive_models/{}.pth'.format(experiment_name))\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                break\n",
    "\n",
    "    #when training stops (either due to max epochs or early stopping), load most recent best model per test loss\n",
    "    model.load_state_dict(torch.load('./models/predictive_models/{}.pth'.format(experiment_name)))\n",
    "    running_loss_val, num_samples_val = 0, 0\n",
    "    val_predictions = []\n",
    "    val_true_labels = []\n",
    "    #evaluate model on validation data\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        if feature_types == 'EHR':\n",
    "            for (inputs_codes, labels_codes, lengths_codes) in val_loader_codes:\n",
    "                if feature_types != 'metab':\n",
    "                    inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                if feature_types != 'EHR':\n",
    "                    inputs_proteomics, labels = inputs_proteomics.to(device), labels_proteomics.to(device)\n",
    "\n",
    "                if feature_types == 'metab':\n",
    "                    outputs = model(inputs_proteomics)\n",
    "                elif feature_types == 'EHR':\n",
    "                    outputs = model(inputs_codes, lengths_codes)\n",
    "                elif feature_types == 'both':\n",
    "                    outputs, interpretability_outputs = model(inputs_codes, inputs_proteomics, lengths_codes, interpretability=True)\n",
    "                    \n",
    "                mean_tensor = torch.tensor(mean_val_outcomes, dtype=torch.float32, device=device)\n",
    "                std_tensor = torch.tensor(sd_val_outcomes, dtype=torch.float32, device=device)\n",
    "\n",
    "                denormalized_outputs = outputs.squeeze() * std_tensor + mean_tensor\n",
    "                denormalized_labels = labels * std_tensor + mean_tensor\n",
    "\n",
    "                loss = criterion(denormalized_outputs, denormalized_labels)\n",
    "                running_loss_val += (loss.item()*lengths_codes.shape[0])\n",
    "                num_samples_val += lengths_codes.shape[0]\n",
    "                val_predictions.extend(outputs.squeeze().tolist())\n",
    "                val_true_labels.extend(labels.tolist())\n",
    "        else:\n",
    "            for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(val_loader_codes, val_loader_proteomics)):\n",
    "                if feature_types != 'metab':\n",
    "                    inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                if feature_types != 'EHR':\n",
    "                    inputs_proteomics, labels = inputs_proteomics.to(device), labels_proteomics.to(device)\n",
    "\n",
    "                if feature_types == 'metab':\n",
    "                    outputs = model(inputs_proteomics)\n",
    "                elif feature_types == 'EHR':\n",
    "                    outputs = model(inputs_codes, lengths_codes)\n",
    "                elif feature_types == 'both':\n",
    "                    outputs, interpretability_outputs = model(inputs_codes, inputs_proteomics, lengths_codes, interpretability=True)\n",
    "\n",
    "                mean_tensor = torch.tensor(mean_val_outcomes, dtype=torch.float32, device=device)\n",
    "                std_tensor = torch.tensor(sd_val_outcomes, dtype=torch.float32, device=device)\n",
    "\n",
    "                denormalized_outputs = outputs.squeeze() * std_tensor + mean_tensor\n",
    "                denormalized_labels = labels * std_tensor + mean_tensor\n",
    "\n",
    "                loss = criterion(denormalized_outputs, denormalized_labels)\n",
    "                running_loss_val += (loss.item()*lengths_codes.shape[0])\n",
    "                num_samples_val += lengths_codes.shape[0]\n",
    "                val_predictions.extend(outputs.squeeze().tolist())\n",
    "                val_true_labels.extend(labels.tolist())\n",
    "    val_loss = running_loss_val / (num_samples_val)\n",
    "    val_losses.append(val_loss)\n",
    "    pearson_corr, _ = pearsonr(val_predictions, val_true_labels)\n",
    "    original_val_outcomes = outcomes[val_indices]\n",
    "    val_outcome_mean, val_outcome_sd = np.mean(original_val_outcomes), np.std(original_val_outcomes)\n",
    "    val_rmse = np.sqrt(mean_squared_error(original_val_outcomes, np.array(val_predictions)*val_outcome_sd+val_outcome_mean))\n",
    "\n",
    "    print(f'Epoch: {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Pearson R: {pearson_corr:.4f}, RMSE: {val_rmse:.4f}')\n",
    "    \n",
    "    if hyperparam_tuning == True:\n",
    "        os.remove('./models/predictive_models/{}.pth'.format(experiment_name))\n",
    "    if return_preds == True:\n",
    "        if return_interpretability == False:\n",
    "            if return_grads:\n",
    "                df = pd.DataFrame([epoch_arr, num_samples_in_batch, gradient_arr]).T\n",
    "                df.columns = ['epoch', 'num_samples_in_batch','gradient']\n",
    "                return pearson_corr, val_loss, val_rmse, original_val_outcomes, np.array(val_predictions)*val_outcome_sd+val_outcome_mean, val_indices, df, train_losses, test_losses, val_losses, train_indices, test_indices\n",
    "            else:\n",
    "                return pearson_corr, val_loss, val_rmse, original_val_outcomes, np.array(val_predictions)*val_outcome_sd+val_outcome_mean, val_indices\n",
    "        else:\n",
    "            if return_grads:\n",
    "                df = pd.DataFrame([epoch_arr, num_samples_in_batch, gradient_arr]).T\n",
    "                df.columns = ['epoch', 'num_samples_in_batch','gradient']\n",
    "                return pearson_corr, val_loss, val_rmse, original_val_outcomes, np.array(val_predictions)*val_outcome_sd+val_outcome_mean, val_indices, interpretability_outputs, val_outcome_mean, val_outcome_sd, df, train_losses, test_losses, val_losses, train_indices, test_indices\n",
    "            else:\n",
    "                return pearson_corr, val_loss, val_rmse, original_val_outcomes, np.array(val_predictions)*val_outcome_sd+val_outcome_mean, val_indices, interpretability_outputs, val_outcome_mean, val_outcome_sd\n",
    "    else:\n",
    "        return pearson_corr, val_loss, val_rmse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ea5e72ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42, 32, 400), (42,), (42,), (42, 2))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load omics data with the omics word2vec model\n",
    "RNN_data_codes_OOL = np.load('./data/processed_data/RNN_data_codes_with_obs_word2vec_from_ool.npy')\n",
    "RNN_data_outcomes_OOL = np.load('./data/processed_data/RNN_data_outcomes_with_obs_word2vec_from_ool.npy')\n",
    "RNN_data_lengths_OOL = np.load('./data/processed_data/RNN_data_lengths_with_obs_word2vec_from_ool.npy')\n",
    "patient_indices_OOL = pd.read_csv('./data/processed_data/sampleID_indices_with_obs_word2vec_from_ool.csv').drop('Unnamed: 0',axis=1)\n",
    "patient_indices_OOL.columns = ['sample_ID','array_index']\n",
    "RNN_data_codes_OOL.shape, RNN_data_outcomes_OOL.shape, RNN_data_lengths_OOL.shape, patient_indices_OOL.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c40bbad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42, 32, 400), (42,), (42,), (42, 2))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load omics data with the pre-trained word2vec model\n",
    "RNN_data_codes = np.load('./data/processed_data/RNN_data_codes_with_obs.npy')\n",
    "RNN_data_outcomes = np.load('./data/processed_data/RNN_data_outcomes_with_obs.npy')\n",
    "RNN_data_lengths = np.load('./data/processed_data/RNN_data_lengths_with_obs.npy')\n",
    "patient_indices = pd.read_csv('./data/processed_data/sampleID_indices_with_obs.csv').drop('Unnamed: 0',axis=1)\n",
    "patient_indices.columns = ['sample_ID','array_index']\n",
    "RNN_data_codes.shape, RNN_data_outcomes.shape, RNN_data_lengths.shape, patient_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "db45d270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((113, 32, 400), (113,), (113,), (113, 2))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load pre-training cohort data\n",
    "RNN_data_codes_PTMODEL = np.load('./data/processed_data/RNN_data_full_EHR_cohort_with_obs_fixed.npy')\n",
    "RNN_data_outcomes_PTMODEL = np.load('./data/processed_data/RNN_data_outcomes_full_EHR_cohort_with_obs_fixed.npy')\n",
    "RNN_data_lengths_PTMODEL = np.load('./data/processed_data/RNN_data_lengths_full_EHR_cohort_with_obs_fixed.npy')\n",
    "patient_indices_PTMODEL = pd.read_csv('./data/processed_data/sampleID_indices_full_cohort_with_obs_fixed.csv').drop('Unnamed: 0',axis=1)\n",
    "patient_indices_PTMODEL.columns = ['sample_ID','array_index']\n",
    "RNN_data_codes_PTMODEL.shape, RNN_data_outcomes_PTMODEL.shape, RNN_data_lengths_PTMODEL.shape, patient_indices_PTMODEL.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d7af171a",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_best_params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f6dc316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_list = ['DOS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7a28a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRIALS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "467b6e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#our proteomics data has 12 proteins which are used for QC, all labeled with \"HCE\"\n",
    "#we exclude these proteins from our analysis\n",
    "columns_to_drop = [col for col in OOL_proteomics.columns if \"HCE\" in col]\n",
    "OOL_proteomics = OOL_proteomics.drop(columns=columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a164efdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c849860ac64af098a4113e6dc3cc48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba98c3abef648aebdd4f6d4de4adb3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.1, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 8437.8896, Pearson R: -0.5368, RMSE: 92.0400\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.1, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 6/100, Val Loss: 50250.7617, Pearson R: 0.4055, RMSE: 224.2142\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.1, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 18/100, Val Loss: 825.4804, Pearson R: -0.5524, RMSE: 29.4382\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.1, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samsonm/miniconda3/envs/multi_modal_DL/lib/python3.10/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/100, Val Loss: 970.9093, Pearson R: -0.2926, RMSE: 31.6785\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.1, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 4169.0571, Pearson R: -0.1962, RMSE: 64.8280\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.1, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 6/100, Val Loss: 11136.9336, Pearson R: 0.1466, RMSE: 105.6407\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.1, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samsonm/miniconda3/envs/multi_modal_DL/lib/python3.10/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/100, Val Loss: 1045.8884, Pearson R: 0.3546, RMSE: 32.8365\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.1, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 14/100, Val Loss: 504.4065, Pearson R: -0.5875, RMSE: 23.1685\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.1, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 15/100, Val Loss: 445.0491, Pearson R: 0.1512, RMSE: 21.8307\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.1, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 12/100, Val Loss: 658.5264, Pearson R: -0.0644, RMSE: 26.3042\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.1, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 3239.6821, Pearson R: -0.3264, RMSE: 57.2695\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.1, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 22/100, Val Loss: 433.0971, Pearson R: 0.3658, RMSE: 21.5565\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.1, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 488.7037, Pearson R: -0.1223, RMSE: 22.8348\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.1, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 14/100, Val Loss: 617.3350, Pearson R: -0.0750, RMSE: 25.5055\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.1, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 33/100, Val Loss: 453.2528, Pearson R: 0.3332, RMSE: 22.0369\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.1, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 30/100, Val Loss: 488.9949, Pearson R: -0.5232, RMSE: 22.8458\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.2, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samsonm/miniconda3/envs/multi_modal_DL/lib/python3.10/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24/100, Val Loss: 541.1212, Pearson R: -0.2062, RMSE: 23.9477\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.2, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 16/100, Val Loss: 1613.7904, Pearson R: -0.1247, RMSE: 40.5738\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.2, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 9530.4902, Pearson R: 0.6312, RMSE: 97.7827\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.2, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 6/100, Val Loss: 20875.3027, Pearson R: -0.4230, RMSE: 144.6472\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.2, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 7/100, Val Loss: 835.0297, Pearson R: 0.6385, RMSE: 29.2610\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.2, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 6/100, Val Loss: 45087.6523, Pearson R: -0.2300, RMSE: 212.4171\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.2, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samsonm/miniconda3/envs/multi_modal_DL/lib/python3.10/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21/100, Val Loss: 453.2511, Pearson R: 0.2547, RMSE: 22.0369\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.2, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 17/100, Val Loss: 469.8111, Pearson R: -0.6274, RMSE: 22.4222\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.2, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 16/100, Val Loss: 535.4394, Pearson R: -0.0307, RMSE: 23.8289\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.2, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 6/100, Val Loss: 23572.7559, Pearson R: 0.0392, RMSE: 153.6300\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.2, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 7/100, Val Loss: 486.9024, Pearson R: -0.2458, RMSE: 22.8044\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.2, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 17/100, Val Loss: 547.9350, Pearson R: -0.0564, RMSE: 24.0896\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.2, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 19/100, Val Loss: 623.9905, Pearson R: 0.4035, RMSE: 25.5933\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.2, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 24/100, Val Loss: 474.5541, Pearson R: 0.2543, RMSE: 22.4910\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.2, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 1443.8679, Pearson R: 0.1833, RMSE: 38.3010\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.2, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 20/100, Val Loss: 451.9588, Pearson R: 0.0922, RMSE: 22.0042\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.3, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 15/100, Val Loss: 458.1228, Pearson R: -0.0406, RMSE: 22.1492\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.3, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 6/100, Val Loss: 18934.9551, Pearson R: 0.3817, RMSE: 137.7173\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.3, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 10/100, Val Loss: 538.5724, Pearson R: -0.0886, RMSE: 23.8960\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.3, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 6/100, Val Loss: 3615.7026, Pearson R: -0.0504, RMSE: 60.4014\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.3, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 6550.0264, Pearson R: 0.3416, RMSE: 81.1076\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.3, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samsonm/miniconda3/envs/multi_modal_DL/lib/python3.10/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/100, Val Loss: 1205.5797, Pearson R: nan, RMSE: 35.1846\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.3, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 15/100, Val Loss: 462.4360, Pearson R: 0.2569, RMSE: 22.2424\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.3, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 9/100, Val Loss: 455.4230, Pearson R: 0.2501, RMSE: 22.0799\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.3, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 10/100, Val Loss: 413.9368, Pearson R: 0.3292, RMSE: 21.0779\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.3, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 19/100, Val Loss: 444.4659, Pearson R: 0.2149, RMSE: 21.8217\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.3, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 22/100, Val Loss: 626.9938, Pearson R: 0.5613, RMSE: 25.6776\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.3, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 6/100, Val Loss: 2938.8289, Pearson R: 0.4238, RMSE: 54.4889\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.3, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 968.0900, Pearson R: -0.1629, RMSE: 31.6303\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.3, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 11/100, Val Loss: 3852.7939, Pearson R: -0.2089, RMSE: 62.4120\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.3, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 10/100, Val Loss: 446.1160, Pearson R: 0.3260, RMSE: 21.8676\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.3, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 12/100, Val Loss: 481.6989, Pearson R: -0.1111, RMSE: 22.6769\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.4, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 15/100, Val Loss: 1651.4202, Pearson R: 0.3366, RMSE: 41.0331\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.4, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 6/100, Val Loss: 5780.3013, Pearson R: 0.1581, RMSE: 76.1924\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.4, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 9/100, Val Loss: 655.4220, Pearson R: -0.1147, RMSE: 26.2278\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.4, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 11/100, Val Loss: 699.4504, Pearson R: 0.1920, RMSE: 27.0522\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.4, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 15/100, Val Loss: 454.6987, Pearson R: 0.0275, RMSE: 22.0688\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.4, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samsonm/miniconda3/envs/multi_modal_DL/lib/python3.10/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/100, Val Loss: 3270.6379, Pearson R: -0.3088, RMSE: 57.4718\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.4, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 494.3636, Pearson R: 0.1717, RMSE: 22.9502\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.4, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 23/100, Val Loss: 457.1763, Pearson R: -0.4293, RMSE: 22.1281\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.4, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 9/100, Val Loss: 478.1115, Pearson R: 0.5628, RMSE: 22.1810\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.4, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 28/100, Val Loss: 560.9001, Pearson R: 0.0909, RMSE: 24.3505\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.4, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 32/100, Val Loss: 457.0846, Pearson R: -0.4923, RMSE: 22.1254\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.4, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 16/100, Val Loss: 468.9607, Pearson R: 0.5579, RMSE: 22.3895\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.4, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 7/100, Val Loss: 5315.3389, Pearson R: 0.0818, RMSE: 73.0937\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.4, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 17/100, Val Loss: 625.9097, Pearson R: -0.2884, RMSE: 25.7247\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.4, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 15/100, Val Loss: 478.5926, Pearson R: -0.6105, RMSE: 22.6051\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.4, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 11/100, Val Loss: 522.9255, Pearson R: 0.1553, RMSE: 23.5628\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.5, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 705.0984, Pearson R: -0.1339, RMSE: 27.1845\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.5, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 6/100, Val Loss: 12559.2559, Pearson R: 0.5134, RMSE: 112.1770\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.5, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 948.2303, Pearson R: -0.3960, RMSE: 31.3409\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.5, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 9/100, Val Loss: 4393.1348, Pearson R: -0.3409, RMSE: 66.5329\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.5, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 15/100, Val Loss: 456.1021, Pearson R: -0.1224, RMSE: 22.1019\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.5, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samsonm/miniconda3/envs/multi_modal_DL/lib/python3.10/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/100, Val Loss: 455.9653, Pearson R: -0.3116, RMSE: 22.0984\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.5, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 13/100, Val Loss: 558.6435, Pearson R: 0.4477, RMSE: 24.3080\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.5, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 13/100, Val Loss: 460.7817, Pearson R: -0.2577, RMSE: 22.2124\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.5, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 16/100, Val Loss: 494.5883, Pearson R: -0.3159, RMSE: 22.9561\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.5, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 7/100, Val Loss: 5347.6138, Pearson R: -0.2098, RMSE: 73.3659\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.5, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 12/100, Val Loss: 524.7698, Pearson R: -0.3036, RMSE: 23.6048\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.5, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 6/100, Val Loss: 24563.3496, Pearson R: 0.3506, RMSE: 156.8111\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.5, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 2200.5710, Pearson R: 0.0798, RMSE: 47.2376\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.5, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 17/100, Val Loss: 1114.0594, Pearson R: 0.2348, RMSE: 33.8529\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.5, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 16/100, Val Loss: 485.9595, Pearson R: -0.6531, RMSE: 22.7791\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.5, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 20/100, Val Loss: 546.5632, Pearson R: 0.2908, RMSE: 24.0457\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.1, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 15/100, Val Loss: 507.3078, Pearson R: -0.2017, RMSE: 23.2463\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.1, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 29/100, Val Loss: 478.8866, Pearson R: 0.0276, RMSE: 22.6059\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.1, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 57/100, Val Loss: 417.3208, Pearson R: 0.3827, RMSE: 21.1517\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.1, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 11/100, Val Loss: 480.9476, Pearson R: 0.1462, RMSE: 22.6550\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.1, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 19/100, Val Loss: 501.7510, Pearson R: 0.1096, RMSE: 23.0769\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.1, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 14/100, Val Loss: 547.9496, Pearson R: -0.4152, RMSE: 24.0942\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.1, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 37/100, Val Loss: 310.9099, Pearson R: 0.6514, RMSE: 18.3363\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.1, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 12/100, Val Loss: 735.4048, Pearson R: 0.4288, RMSE: 27.6999\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.1, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 14/100, Val Loss: 505.8985, Pearson R: -0.3640, RMSE: 23.2270\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.1, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 10/100, Val Loss: 611.2922, Pearson R: 0.4612, RMSE: 25.0365\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.1, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 21/100, Val Loss: 361.2506, Pearson R: 0.5325, RMSE: 19.7320\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.1, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 13/100, Val Loss: 447.2134, Pearson R: 0.3523, RMSE: 21.8882\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.1, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 20/100, Val Loss: 441.4227, Pearson R: 0.1895, RMSE: 21.7268\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.1, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 10/100, Val Loss: 481.5425, Pearson R: 0.1545, RMSE: 22.6358\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.1, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 14/100, Val Loss: 426.6466, Pearson R: 0.3207, RMSE: 21.3973\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.1, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 50/100, Val Loss: 427.1958, Pearson R: 0.3042, RMSE: 21.4089\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.2, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 8/100, Val Loss: 441.5809, Pearson R: 0.2679, RMSE: 21.7565\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.2, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 11/100, Val Loss: 577.5809, Pearson R: 0.4309, RMSE: 24.4569\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.2, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 24/100, Val Loss: 395.1213, Pearson R: 0.4601, RMSE: 20.5950\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.2, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 14/100, Val Loss: 434.3658, Pearson R: 0.2733, RMSE: 21.5832\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.2, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 453.3806, Pearson R: 0.3880, RMSE: 22.0136\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.2, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 11/100, Val Loss: 493.6397, Pearson R: 0.3293, RMSE: 22.8465\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.2, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 550.3218, Pearson R: 0.3846, RMSE: 24.1162\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.2, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 14/100, Val Loss: 494.6412, Pearson R: 0.4338, RMSE: 22.9560\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.2, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 437.5366, Pearson R: 0.3795, RMSE: 21.6609\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.2, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 26/100, Val Loss: 432.0705, Pearson R: 0.2472, RMSE: 21.5222\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.2, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 19/100, Val Loss: 434.1255, Pearson R: 0.7596, RMSE: 21.5824\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.2, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 27/100, Val Loss: 454.8749, Pearson R: 0.4289, RMSE: 22.0732\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.2, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 442.2250, Pearson R: 0.3756, RMSE: 21.7704\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.2, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 10/100, Val Loss: 617.2469, Pearson R: 0.4745, RMSE: 25.4461\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.2, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 18/100, Val Loss: 451.4891, Pearson R: 0.2520, RMSE: 21.9927\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.2, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 13/100, Val Loss: 452.1400, Pearson R: 0.1514, RMSE: 22.0107\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.3, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 624.9808, Pearson R: -0.5765, RMSE: 25.7283\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.3, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 13/100, Val Loss: 467.0503, Pearson R: -0.0022, RMSE: 22.3479\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.3, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 445.6729, Pearson R: 0.4429, RMSE: 21.8549\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.3, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 23/100, Val Loss: 457.2502, Pearson R: 0.1344, RMSE: 22.1253\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.3, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 675.6477, Pearson R: -0.4689, RMSE: 26.6987\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.3, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 10/100, Val Loss: 378.5576, Pearson R: 0.4216, RMSE: 20.1615\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.3, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 16/100, Val Loss: 447.8228, Pearson R: 0.1604, RMSE: 21.8879\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.3, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/100, Val Loss: 467.9374, Pearson R: -0.3775, RMSE: 22.3768\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.3, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 467.5665, Pearson R: 0.3629, RMSE: 22.3330\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.3, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 12/100, Val Loss: 462.9291, Pearson R: -0.5490, RMSE: 22.2616\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.3, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 17/100, Val Loss: 448.3062, Pearson R: 0.3705, RMSE: 21.9202\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.3, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 13/100, Val Loss: 441.2811, Pearson R: 0.3340, RMSE: 21.7497\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.3, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 573.2796, Pearson R: -0.3308, RMSE: 24.6590\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.3, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 16/100, Val Loss: 391.7516, Pearson R: 0.5484, RMSE: 20.5339\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.3, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 25/100, Val Loss: 396.8260, Pearson R: 0.6320, RMSE: 20.6637\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.3, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 9/100, Val Loss: 468.6651, Pearson R: -0.4770, RMSE: 22.3864\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.4, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 515.7698, Pearson R: -0.5997, RMSE: 23.4543\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.4, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 15/100, Val Loss: 479.0605, Pearson R: -0.6107, RMSE: 22.6323\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.4, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 29/100, Val Loss: 409.4035, Pearson R: 0.3408, RMSE: 20.9483\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.4, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 22/100, Val Loss: 448.4927, Pearson R: 0.1099, RMSE: 21.9220\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.4, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 8/100, Val Loss: 445.7796, Pearson R: 0.1913, RMSE: 21.8565\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.4, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 20/100, Val Loss: 453.5392, Pearson R: -0.0801, RMSE: 22.0435\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.4, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 13/100, Val Loss: 482.8039, Pearson R: -0.6199, RMSE: 22.7180\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.4, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 9/100, Val Loss: 468.4345, Pearson R: 0.0491, RMSE: 22.3755\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.4, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 599.9887, Pearson R: -0.4961, RMSE: 25.2210\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.4, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 15/100, Val Loss: 492.3723, Pearson R: 0.2740, RMSE: 22.9034\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.4, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 453.6612, Pearson R: 0.4246, RMSE: 22.0379\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.4, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 28/100, Val Loss: 453.8103, Pearson R: -0.0589, RMSE: 22.0497\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.4, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 439.9579, Pearson R: 0.3760, RMSE: 21.7216\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.4, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 14/100, Val Loss: 410.6882, Pearson R: 0.3653, RMSE: 21.0028\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.4, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 450.6638, Pearson R: 0.5864, RMSE: 21.9730\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.4, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 12/100, Val Loss: 449.0631, Pearson R: 0.4490, RMSE: 21.9372\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.5, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 521.0464, Pearson R: -0.5624, RMSE: 23.5685\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.5, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 33/100, Val Loss: 432.7451, Pearson R: 0.4139, RMSE: 21.5479\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.5, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 588.1490, Pearson R: 0.3664, RMSE: 24.8774\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.5, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 12/100, Val Loss: 533.8743, Pearson R: 0.2926, RMSE: 23.7944\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.5, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 682.5433, Pearson R: -0.4643, RMSE: 26.8370\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.5, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 10/100, Val Loss: 442.8418, Pearson R: 0.4473, RMSE: 21.7727\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.5, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 464.5417, Pearson R: 0.3526, RMSE: 22.2651\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.5, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 38/100, Val Loss: 440.3669, Pearson R: 0.3189, RMSE: 21.7314\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.5, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 584.3869, Pearson R: -0.2841, RMSE: 24.8880\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.5, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 12/100, Val Loss: 456.6927, Pearson R: 0.1424, RMSE: 22.0972\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.5, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 9/100, Val Loss: 478.2014, Pearson R: -0.2599, RMSE: 22.6042\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.5, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 11/100, Val Loss: 740.1166, Pearson R: -0.4117, RMSE: 27.8837\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.5, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 624.1237, Pearson R: -0.3997, RMSE: 25.6960\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.5, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 31/100, Val Loss: 445.2784, Pearson R: 0.1774, RMSE: 21.8340\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.5, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 9/100, Val Loss: 460.8936, Pearson R: -0.2944, RMSE: 22.2152\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.5, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 18/100, Val Loss: 468.0260, Pearson R: -0.3108, RMSE: 22.3723\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.1, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 13/100, Val Loss: 449.9231, Pearson R: 0.1549, RMSE: 21.9322\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.1, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 23/100, Val Loss: 469.6683, Pearson R: -0.0114, RMSE: 22.4074\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.1, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 20/100, Val Loss: 460.4019, Pearson R: 0.1310, RMSE: 22.1821\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.1, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 22/100, Val Loss: 463.0651, Pearson R: 0.1309, RMSE: 22.2365\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.1, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 13/100, Val Loss: 481.1867, Pearson R: 0.1067, RMSE: 22.6433\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.1, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 23/100, Val Loss: 485.2473, Pearson R: -0.0991, RMSE: 22.7599\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.1, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 24/100, Val Loss: 474.9226, Pearson R: 0.0906, RMSE: 22.5065\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.1, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 21/100, Val Loss: 461.7668, Pearson R: 0.1222, RMSE: 22.2115\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.1, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 437.5050, Pearson R: 0.1876, RMSE: 21.6490\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.1, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36/100, Val Loss: 471.5922, Pearson R: -0.0132, RMSE: 22.4505\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.1, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 27/100, Val Loss: 468.6995, Pearson R: 0.0900, RMSE: 22.3704\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.1, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 21/100, Val Loss: 460.8708, Pearson R: 0.1365, RMSE: 22.1915\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.1, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 438.4577, Pearson R: 0.1923, RMSE: 21.6763\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.1, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 37/100, Val Loss: 479.6605, Pearson R: -0.0194, RMSE: 22.6309\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.1, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 13/100, Val Loss: 441.6904, Pearson R: 0.1685, RMSE: 21.7579\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.1, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 21/100, Val Loss: 462.5048, Pearson R: 0.1244, RMSE: 22.2289\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.2, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 13/100, Val Loss: 486.4361, Pearson R: 0.1271, RMSE: 22.7575\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.2, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 22/100, Val Loss: 474.1781, Pearson R: -0.0173, RMSE: 22.5080\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.2, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 19/100, Val Loss: 458.9015, Pearson R: 0.1351, RMSE: 22.1416\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.2, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 18/100, Val Loss: 437.0344, Pearson R: 0.2187, RMSE: 21.6436\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.2, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 445.3130, Pearson R: 0.1682, RMSE: 21.8295\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.2, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 21/100, Val Loss: 464.6183, Pearson R: -0.0054, RMSE: 22.2938\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.2, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 14/100, Val Loss: 448.2953, Pearson R: 0.1496, RMSE: 21.9040\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.2, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 18/100, Val Loss: 440.0151, Pearson R: 0.2115, RMSE: 21.7154\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.2, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 437.1097, Pearson R: 0.1900, RMSE: 21.6410\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.2, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 34/100, Val Loss: 477.9234, Pearson R: -0.0167, RMSE: 22.5916\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.2, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 19/100, Val Loss: 468.2729, Pearson R: 0.1107, RMSE: 22.3578\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.2, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 18/100, Val Loss: 439.6617, Pearson R: 0.2134, RMSE: 21.7078\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.2, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 438.9782, Pearson R: 0.1830, RMSE: 21.6829\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.2, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 22/100, Val Loss: 478.8795, Pearson R: -0.0532, RMSE: 22.6165\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.2, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 14/100, Val Loss: 448.3076, Pearson R: 0.1452, RMSE: 21.9041\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.2, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 18/100, Val Loss: 442.8479, Pearson R: 0.1815, RMSE: 21.7795\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.3, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 26/100, Val Loss: 449.4439, Pearson R: 0.1582, RMSE: 21.9254\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.3, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 22/100, Val Loss: 475.9967, Pearson R: -0.0052, RMSE: 22.5476\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.3, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 15/100, Val Loss: 441.9386, Pearson R: 0.1832, RMSE: 21.7513\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.3, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 19/100, Val Loss: 439.7666, Pearson R: 0.1788, RMSE: 21.7000\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.3, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 438.2379, Pearson R: 0.1918, RMSE: 21.6700\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.3, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 20/100, Val Loss: 467.3439, Pearson R: -0.0017, RMSE: 22.3545\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.3, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 21/100, Val Loss: 454.3557, Pearson R: 0.1618, RMSE: 22.0385\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.3, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 18/100, Val Loss: 442.7256, Pearson R: 0.1960, RMSE: 21.7795\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.3, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 25/100, Val Loss: 466.6139, Pearson R: 0.1186, RMSE: 22.3190\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.3, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 26/100, Val Loss: 473.9059, Pearson R: -0.0235, RMSE: 22.5033\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.3, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 24/100, Val Loss: 462.9895, Pearson R: 0.1385, RMSE: 22.2355\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.3, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 19/100, Val Loss: 443.3194, Pearson R: 0.1622, RMSE: 21.7886\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.3, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 439.9531, Pearson R: 0.1778, RMSE: 21.7064\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.3, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 20/100, Val Loss: 463.3359, Pearson R: 0.0178, RMSE: 22.2627\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.3, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 13/100, Val Loss: 458.8888, Pearson R: 0.1412, RMSE: 22.1442\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.3, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 21/100, Val Loss: 459.7790, Pearson R: 0.1311, RMSE: 22.1682\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.4, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 12/100, Val Loss: 435.9393, Pearson R: 0.1980, RMSE: 21.6122\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.4, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 25/100, Val Loss: 462.2136, Pearson R: 0.0574, RMSE: 22.2338\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.4, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 24/100, Val Loss: 465.4574, Pearson R: 0.1470, RMSE: 22.2866\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.4, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 19/100, Val Loss: 440.7401, Pearson R: 0.1905, RMSE: 21.7304\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.4, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 12/100, Val Loss: 453.1115, Pearson R: 0.1424, RMSE: 22.0085\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.4, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 19/100, Val Loss: 467.9661, Pearson R: 0.0200, RMSE: 22.3655\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.4, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 18/100, Val Loss: 455.3513, Pearson R: 0.1628, RMSE: 22.0591\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.4, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 23/100, Val Loss: 454.1260, Pearson R: 0.1397, RMSE: 22.0382\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.4, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 12/100, Val Loss: 458.4911, Pearson R: 0.1343, RMSE: 22.1316\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.4, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 29/100, Val Loss: 513.3322, Pearson R: -0.0683, RMSE: 23.3707\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.4, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/100, Val Loss: 436.3838, Pearson R: 0.1994, RMSE: 21.6271\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.4, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 19/100, Val Loss: 443.3569, Pearson R: 0.1759, RMSE: 21.7929\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.4, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 436.1730, Pearson R: 0.2017, RMSE: 21.6234\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.4, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 22/100, Val Loss: 500.2692, Pearson R: -0.0379, RMSE: 23.0840\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.4, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 15/100, Val Loss: 436.1657, Pearson R: 0.1960, RMSE: 21.6155\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.4, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 22/100, Val Loss: 479.3839, Pearson R: 0.1073, RMSE: 22.6080\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.5, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 12/100, Val Loss: 435.0700, Pearson R: 0.2059, RMSE: 21.5947\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.5, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 17/100, Val Loss: 443.0395, Pearson R: 0.1645, RMSE: 21.7866\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.5, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 12/100, Val Loss: 453.2697, Pearson R: 0.1840, RMSE: 22.0112\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.5, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 19/100, Val Loss: 449.0905, Pearson R: 0.1509, RMSE: 21.9268\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.5, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 12/100, Val Loss: 446.6180, Pearson R: 0.1595, RMSE: 21.8638\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.5, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 16/100, Val Loss: 443.6842, Pearson R: 0.1738, RMSE: 21.8087\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.5, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 13/100, Val Loss: 468.7534, Pearson R: 0.1461, RMSE: 22.3588\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.5, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 19/100, Val Loss: 441.3452, Pearson R: 0.1725, RMSE: 21.7487\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.5, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 12/100, Val Loss: 440.3901, Pearson R: 0.1758, RMSE: 21.7178\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.5, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 18/100, Val Loss: 475.0055, Pearson R: 0.0272, RMSE: 22.5213\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.5, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 443.2854, Pearson R: 0.1721, RMSE: 21.7798\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.5, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 18/100, Val Loss: 440.4485, Pearson R: 0.1848, RMSE: 21.7289\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.5, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 12/100, Val Loss: 441.6545, Pearson R: 0.1671, RMSE: 21.7471\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.5, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 21/100, Val Loss: 479.0104, Pearson R: 0.0306, RMSE: 22.6089\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.5, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 17/100, Val Loss: 463.2407, Pearson R: 0.1573, RMSE: 22.2368\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.5, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 19/100, Val Loss: 461.5655, Pearson R: 0.1031, RMSE: 22.2109\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.1, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 46/100, Val Loss: 441.7535, Pearson R: 0.1743, RMSE: 21.7475\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.1, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 32/100, Val Loss: 448.8642, Pearson R: 0.1525, RMSE: 21.9113\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.1, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 32/100, Val Loss: 449.1782, Pearson R: 0.1598, RMSE: 21.9163\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.1, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 20/100, Val Loss: 438.4108, Pearson R: 0.1868, RMSE: 21.6701\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.1, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 44/100, Val Loss: 445.1861, Pearson R: 0.1587, RMSE: 21.8278\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.1, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 30/100, Val Loss: 448.4549, Pearson R: 0.1456, RMSE: 21.9037\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.1, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 30/100, Val Loss: 443.6926, Pearson R: 0.1653, RMSE: 21.7923\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.1, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 19/100, Val Loss: 439.3701, Pearson R: 0.1810, RMSE: 21.6949\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.1, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 44/100, Val Loss: 447.9914, Pearson R: 0.1503, RMSE: 21.8925\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.1, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 29/100, Val Loss: 442.3071, Pearson R: 0.1629, RMSE: 21.7631\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.1, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 30/100, Val Loss: 445.4616, Pearson R: 0.1610, RMSE: 21.8330\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.1, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 19/100, Val Loss: 439.9938, Pearson R: 0.1779, RMSE: 21.7100\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.1, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 44/100, Val Loss: 447.7915, Pearson R: 0.1508, RMSE: 21.8881\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.1, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 29/100, Val Loss: 443.1434, Pearson R: 0.1600, RMSE: 21.7823\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.1, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 30/100, Val Loss: 445.9761, Pearson R: 0.1595, RMSE: 21.8449\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.1, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 19/100, Val Loss: 440.6866, Pearson R: 0.1751, RMSE: 21.7256\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.2, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 49/100, Val Loss: 443.3737, Pearson R: 0.1659, RMSE: 21.7858\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.2, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 33/100, Val Loss: 446.2912, Pearson R: 0.1554, RMSE: 21.8533\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.2, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 34/100, Val Loss: 444.0489, Pearson R: 0.1672, RMSE: 21.8006\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.2, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 22/100, Val Loss: 441.9426, Pearson R: 0.1723, RMSE: 21.7514\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.2, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 47/100, Val Loss: 448.2443, Pearson R: 0.1466, RMSE: 21.8993\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.2, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 31/100, Val Loss: 446.9941, Pearson R: 0.1468, RMSE: 21.8711\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.2, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 33/100, Val Loss: 449.4213, Pearson R: 0.1488, RMSE: 21.9246\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.2, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 21/100, Val Loss: 443.1299, Pearson R: 0.1658, RMSE: 21.7806\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.2, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 46/100, Val Loss: 444.4847, Pearson R: 0.1563, RMSE: 21.8136\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.2, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 30/100, Val Loss: 441.9802, Pearson R: 0.1627, RMSE: 21.7567\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.2, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 33/100, Val Loss: 446.9554, Pearson R: 0.1534, RMSE: 21.8683\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.2, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/100, Val Loss: 440.4622, Pearson R: 0.1749, RMSE: 21.7215\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.2, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 46/100, Val Loss: 444.9764, Pearson R: 0.1549, RMSE: 21.8247\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.2, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 30/100, Val Loss: 442.9865, Pearson R: 0.1587, RMSE: 21.7797\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.2, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 33/100, Val Loss: 449.5898, Pearson R: 0.1478, RMSE: 21.9284\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.2, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 21/100, Val Loss: 444.7280, Pearson R: 0.1598, RMSE: 21.8179\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.3, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 52/100, Val Loss: 444.8956, Pearson R: 0.1585, RMSE: 21.8218\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.3, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 35/100, Val Loss: 451.2454, Pearson R: 0.1417, RMSE: 21.9674\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.3, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 38/100, Val Loss: 449.9972, Pearson R: 0.1517, RMSE: 21.9369\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.3, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 23/100, Val Loss: 440.6258, Pearson R: 0.1766, RMSE: 21.7224\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.3, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 50/100, Val Loss: 448.3438, Pearson R: 0.1431, RMSE: 21.9024\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.3, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 32/100, Val Loss: 443.7319, Pearson R: 0.1556, RMSE: 21.7970\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.3, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 36/100, Val Loss: 444.6989, Pearson R: 0.1583, RMSE: 21.8170\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.3, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 22/100, Val Loss: 440.3387, Pearson R: 0.1737, RMSE: 21.7178\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.3, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 49/100, Val Loss: 447.8151, Pearson R: 0.1440, RMSE: 21.8906\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.3, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 31/100, Val Loss: 441.8185, Pearson R: 0.1626, RMSE: 21.7536\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.3, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 35/100, Val Loss: 445.0598, Pearson R: 0.1564, RMSE: 21.8262\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.3, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 22/100, Val Loss: 443.6010, Pearson R: 0.1633, RMSE: 21.7931\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.3, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 49/100, Val Loss: 447.6827, Pearson R: 0.1442, RMSE: 21.8876\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.3, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 31/100, Val Loss: 441.9427, Pearson R: 0.1617, RMSE: 21.7565\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.3, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 35/100, Val Loss: 444.6562, Pearson R: 0.1577, RMSE: 21.8166\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.3, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 22/100, Val Loss: 442.1384, Pearson R: 0.1670, RMSE: 21.7594\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.4, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 54/100, Val Loss: 443.5598, Pearson R: 0.1609, RMSE: 21.7918\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.4, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 36/100, Val Loss: 446.3278, Pearson R: 0.1507, RMSE: 21.8554\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.4, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 40/100, Val Loss: 444.2279, Pearson R: 0.1636, RMSE: 21.8055\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.4, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 25/100, Val Loss: 443.0752, Pearson R: 0.1665, RMSE: 21.7800\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.4, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 52/100, Val Loss: 447.1565, Pearson R: 0.1442, RMSE: 21.8762\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.4, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 33/100, Val Loss: 443.2308, Pearson R: 0.1564, RMSE: 21.7865\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.4, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 38/100, Val Loss: 443.4809, Pearson R: 0.1599, RMSE: 21.7906\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.4, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 24/100, Val Loss: 444.3938, Pearson R: 0.1588, RMSE: 21.8105\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.4, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 51/100, Val Loss: 445.6043, Pearson R: 0.1482, RMSE: 21.8411\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.4, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 32/100, Val Loss: 441.1275, Pearson R: 0.1652, RMSE: 21.7388\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.4, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 40/100, Val Loss: 451.2995, Pearson R: 0.1399, RMSE: 21.9687\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.4, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 24/100, Val Loss: 445.6164, Pearson R: 0.1557, RMSE: 21.8388\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.4, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 51/100, Val Loss: 444.4095, Pearson R: 0.1521, RMSE: 21.8136\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.4, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 33/100, Val Loss: 445.2282, Pearson R: 0.1476, RMSE: 21.8325\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.4, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 40/100, Val Loss: 453.7405, Pearson R: 0.1339, RMSE: 22.0251\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.4, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 24/100, Val Loss: 441.7861, Pearson R: 0.1667, RMSE: 21.7512\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.5, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 58/100, Val Loss: 445.7869, Pearson R: 0.1517, RMSE: 21.8438\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.5, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 37/100, Val Loss: 443.1486, Pearson R: 0.1601, RMSE: 21.7831\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.5, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 42/100, Val Loss: 439.2771, Pearson R: 0.1782, RMSE: 21.6930\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.5, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 28/100, Val Loss: 446.5687, Pearson R: 0.1562, RMSE: 21.8591\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.5, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 55/100, Val Loss: 445.6927, Pearson R: 0.1475, RMSE: 21.8425\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.5, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 35/100, Val Loss: 445.0667, Pearson R: 0.1478, RMSE: 21.8290\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.5, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 43/100, Val Loss: 444.4511, Pearson R: 0.1557, RMSE: 21.8124\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.5, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 27/100, Val Loss: 447.0998, Pearson R: 0.1491, RMSE: 21.8730\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.5, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 55/100, Val Loss: 447.1718, Pearson R: 0.1408, RMSE: 21.8771\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.5, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 34/100, Val Loss: 442.8000, Pearson R: 0.1560, RMSE: 21.7773\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.5, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 43/100, Val Loss: 451.2818, Pearson R: 0.1380, RMSE: 21.9698\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.5, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 26/100, Val Loss: 443.0492, Pearson R: 0.1613, RMSE: 21.7805\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.5, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55/100, Val Loss: 447.9550, Pearson R: 0.1393, RMSE: 21.8952\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.5, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 35/100, Val Loss: 445.6165, Pearson R: 0.1447, RMSE: 21.8418\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.5, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 41/100, Val Loss: 442.0065, Pearson R: 0.1639, RMSE: 21.7572\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.5, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 26/100, Val Loss: 443.2164, Pearson R: 0.1607, RMSE: 21.7846\n",
      "PT\n",
      "outcome DOS\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783f5feb1ea44459a5227b24df451a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.1, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 281.4726, Pearson R: 0.4128, RMSE: 17.2939\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.1, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 6/100, Val Loss: 39546.5664, Pearson R: 0.3473, RMSE: 198.9062\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.1, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 10/100, Val Loss: 314.9005, Pearson R: 0.3374, RMSE: 18.3686\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.1, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 15/100, Val Loss: 315.3676, Pearson R: 0.1822, RMSE: 18.3817\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.1, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 363.5270, Pearson R: 0.4267, RMSE: 19.6052\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.1, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 24/100, Val Loss: 321.4516, Pearson R: -0.0877, RMSE: 18.5466\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.1, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 324.2560, Pearson R: -0.4282, RMSE: 18.6301\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.1, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 8/100, Val Loss: 410.8380, Pearson R: -0.1082, RMSE: 20.8284\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.1, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 326.7079, Pearson R: -0.0112, RMSE: 18.6878\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.1, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 16/100, Val Loss: 5937.0649, Pearson R: -0.1878, RMSE: 77.3037\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.1, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 9/100, Val Loss: 336.6242, Pearson R: 0.2426, RMSE: 18.8817\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.1, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 18/100, Val Loss: 343.4288, Pearson R: -0.3794, RMSE: 19.1509\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.1, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 22/100, Val Loss: 308.5335, Pearson R: 0.1953, RMSE: 18.1837\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.1, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 24/100, Val Loss: 267.2839, Pearson R: 0.4822, RMSE: 16.8232\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.1, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 21/100, Val Loss: 335.3217, Pearson R: 0.3442, RMSE: 18.9168\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.1, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 14/100, Val Loss: 334.7935, Pearson R: 0.0763, RMSE: 18.9022\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.2, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 15/100, Val Loss: 777.0424, Pearson R: -0.0923, RMSE: 28.2766\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.2, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samsonm/miniconda3/envs/multi_modal_DL/lib/python3.10/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/100, Val Loss: 393.0155, Pearson R: 0.2590, RMSE: 20.3811\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.2, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 15/100, Val Loss: 315.3691, Pearson R: 0.4051, RMSE: 18.3818\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.2, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 6/100, Val Loss: 2693.9883, Pearson R: -0.4781, RMSE: 52.2673\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.2, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 9/100, Val Loss: 738.7805, Pearson R: -0.1162, RMSE: 27.6128\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.2, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 14/100, Val Loss: 336.9508, Pearson R: 0.1172, RMSE: 18.9568\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.2, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 687.6359, Pearson R: 0.2710, RMSE: 26.5661\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.2, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 14/100, Val Loss: 532.1388, Pearson R: -0.1359, RMSE: 23.5513\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.2, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 7/100, Val Loss: 304.9281, Pearson R: 0.3662, RMSE: 17.9896\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.2, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 9/100, Val Loss: 24782.4375, Pearson R: 0.1693, RMSE: 157.3927\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.2, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 7/100, Val Loss: 387.8672, Pearson R: -0.0042, RMSE: 20.2592\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.2, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 21/100, Val Loss: 306.5547, Pearson R: 0.3250, RMSE: 18.1313\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.2, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 8/100, Val Loss: 398.1434, Pearson R: -0.1995, RMSE: 20.5420\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.2, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 26/100, Val Loss: 1047.1355, Pearson R: 0.0446, RMSE: 32.6820\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.2, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 23/100, Val Loss: 316.7159, Pearson R: 0.1113, RMSE: 18.4154\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.2, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 12/100, Val Loss: 576.1236, Pearson R: -0.1510, RMSE: 24.4682\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.3, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 8252.8223, Pearson R: -0.2916, RMSE: 90.9761\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.3, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 13/100, Val Loss: 376.6084, Pearson R: 0.4363, RMSE: 19.9776\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.3, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 13/100, Val Loss: 322.7702, Pearson R: -0.1341, RMSE: 18.5836\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.3, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samsonm/miniconda3/envs/multi_modal_DL/lib/python3.10/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33/100, Val Loss: 382.4832, Pearson R: nan, RMSE: 20.1248\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.3, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 18/100, Val Loss: 416.4019, Pearson R: -0.3233, RMSE: 21.0066\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.3, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 8/100, Val Loss: 465.1404, Pearson R: -0.0684, RMSE: 22.1003\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.3, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 22/100, Val Loss: 318.1547, Pearson R: 0.3723, RMSE: 18.4575\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.3, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samsonm/miniconda3/envs/multi_modal_DL/lib/python3.10/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/100, Val Loss: 407.4049, Pearson R: -0.3957, RMSE: 20.7485\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.3, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 13188.4863, Pearson R: -0.2110, RMSE: 114.9865\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.3, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 7/100, Val Loss: 9756.0879, Pearson R: 0.3496, RMSE: 98.8746\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.3, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 15/100, Val Loss: 322.3967, Pearson R: -0.0645, RMSE: 18.5730\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.3, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 29/100, Val Loss: 315.6788, Pearson R: 0.4913, RMSE: 18.3900\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.3, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 20/100, Val Loss: 315.5017, Pearson R: -0.0018, RMSE: 18.3855\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.3, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 12/100, Val Loss: 537.1265, Pearson R: -0.0822, RMSE: 23.6737\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.3, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 2586.9060, Pearson R: -0.2119, RMSE: 51.1117\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.3, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 17/100, Val Loss: 315.7733, Pearson R: -0.0323, RMSE: 18.3930\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.4, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 9/100, Val Loss: 3485.8750, Pearson R: -0.0206, RMSE: 59.2324\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.4, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 16/100, Val Loss: 336.3928, Pearson R: 0.4352, RMSE: 18.9451\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.4, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 9/100, Val Loss: 4471.0640, Pearson R: 0.1432, RMSE: 67.0337\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.4, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 6/100, Val Loss: 8249.9766, Pearson R: -0.3492, RMSE: 90.9715\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.4, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 2084.0889, Pearson R: -0.4733, RMSE: 46.0163\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.4, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 28/100, Val Loss: 314.8175, Pearson R: 0.1891, RMSE: 18.3663\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.4, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 7/100, Val Loss: 855.9916, Pearson R: -0.0865, RMSE: 29.6792\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.4, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 12/100, Val Loss: 513.1821, Pearson R: 0.2247, RMSE: 23.1376\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.4, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 5535.3345, Pearson R: -0.2562, RMSE: 74.5863\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.4, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 15/100, Val Loss: 341.6703, Pearson R: 0.1885, RMSE: 19.0838\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.4, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 471.9688, Pearson R: 0.0979, RMSE: 22.2273\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.4, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 7/100, Val Loss: 558.8867, Pearson R: 0.4332, RMSE: 24.0704\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.4, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 394.5997, Pearson R: -0.2798, RMSE: 20.4551\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.4, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 14/100, Val Loss: 451.3677, Pearson R: -0.0421, RMSE: 21.7755\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.4, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 13/100, Val Loss: 316.7605, Pearson R: 0.2055, RMSE: 18.4190\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.4, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 6/100, Val Loss: 2336.1516, Pearson R: 0.3535, RMSE: 48.3889\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.5, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 19/100, Val Loss: 420.0957, Pearson R: 0.1545, RMSE: 21.0360\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.5, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 9/100, Val Loss: 320.7839, Pearson R: 0.3411, RMSE: 18.5220\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.5, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 14/100, Val Loss: 303.7565, Pearson R: 0.2717, RMSE: 18.0501\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.5, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 7/100, Val Loss: 308.2883, Pearson R: 0.2197, RMSE: 18.1441\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.5, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 13/100, Val Loss: 315.9436, Pearson R: 0.3961, RMSE: 18.3964\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.5, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 14/100, Val Loss: 1019.0883, Pearson R: -0.1048, RMSE: 32.2740\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.5, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 10/100, Val Loss: 338.9219, Pearson R: -0.2493, RMSE: 19.0278\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.5, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 14/100, Val Loss: 327.9269, Pearson R: -0.3487, RMSE: 18.7311\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.5, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 21/100, Val Loss: 319.8631, Pearson R: -0.1289, RMSE: 18.5037\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.5, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 9/100, Val Loss: 3113.4514, Pearson R: 0.4889, RMSE: 55.9857\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.5, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 20/100, Val Loss: 300.6584, Pearson R: 0.4059, RMSE: 17.9605\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.5, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 6/100, Val Loss: 51264.4961, Pearson R: 0.0800, RMSE: 226.4657\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.5, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 16/100, Val Loss: 388.7024, Pearson R: -0.1274, RMSE: 20.2789\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.5, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 10/100, Val Loss: 863.6423, Pearson R: 0.3649, RMSE: 29.7618\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.5, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 9/100, Val Loss: 384.0560, Pearson R: -0.4000, RMSE: 20.2060\n",
      "{'batch_size': 512, 'lr': 0.1, 'dropout': 0.5, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 12/100, Val Loss: 315.3361, Pearson R: 0.1447, RMSE: 18.3809\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.1, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 13/100, Val Loss: 278.2048, Pearson R: 0.4195, RMSE: 17.2627\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.1, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 11/100, Val Loss: 769.2916, Pearson R: -0.3299, RMSE: 28.1648\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.1, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 14/100, Val Loss: 378.5645, Pearson R: -0.2943, RMSE: 20.0592\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.1, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 16/100, Val Loss: 316.7076, Pearson R: -0.2022, RMSE: 18.4191\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.1, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 318.5279, Pearson R: 0.3728, RMSE: 18.4457\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.1, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 11/100, Val Loss: 385.3233, Pearson R: 0.2955, RMSE: 20.1904\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.1, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 14/100, Val Loss: 323.2191, Pearson R: -0.3927, RMSE: 18.5976\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.1, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 13/100, Val Loss: 537.1253, Pearson R: 0.3155, RMSE: 23.6526\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.1, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 10/100, Val Loss: 483.7061, Pearson R: -0.1826, RMSE: 22.5468\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.1, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/100, Val Loss: 396.9817, Pearson R: -0.3126, RMSE: 20.5078\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.1, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 19/100, Val Loss: 321.2773, Pearson R: -0.0850, RMSE: 18.5437\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.1, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 12/100, Val Loss: 422.0460, Pearson R: -0.2404, RMSE: 21.0858\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.1, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 10/100, Val Loss: 384.2600, Pearson R: -0.1896, RMSE: 20.2000\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.1, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 9/100, Val Loss: 534.0505, Pearson R: -0.1709, RMSE: 23.6462\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.1, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 302.5802, Pearson R: 0.4067, RMSE: 18.0171\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.1, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 13/100, Val Loss: 325.8644, Pearson R: 0.3536, RMSE: 18.6536\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.2, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 10/100, Val Loss: 434.0511, Pearson R: -0.1849, RMSE: 21.4077\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.2, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 22/100, Val Loss: 525.9633, Pearson R: -0.0189, RMSE: 23.4269\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.2, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 25/100, Val Loss: 351.4719, Pearson R: 0.1602, RMSE: 19.2982\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.2, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 25/100, Val Loss: 300.4094, Pearson R: 0.2372, RMSE: 17.9391\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.2, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 13/100, Val Loss: 283.5003, Pearson R: 0.3754, RMSE: 17.4177\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.2, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 15/100, Val Loss: 319.9576, Pearson R: 0.2962, RMSE: 18.4953\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.2, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 17/100, Val Loss: 316.4568, Pearson R: -0.0013, RMSE: 18.4114\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.2, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 12/100, Val Loss: 322.9755, Pearson R: -0.2969, RMSE: 18.5909\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.2, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 10/100, Val Loss: 500.1708, Pearson R: -0.1888, RMSE: 22.9133\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.2, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 16/100, Val Loss: 294.4304, Pearson R: 0.4896, RMSE: 17.7664\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.2, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 19/100, Val Loss: 333.8978, Pearson R: -0.0658, RMSE: 18.8859\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.2, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 12/100, Val Loss: 331.1210, Pearson R: 0.0606, RMSE: 18.8046\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.2, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 327.6665, Pearson R: 0.3876, RMSE: 18.6877\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.2, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 22/100, Val Loss: 448.3972, Pearson R: -0.2073, RMSE: 21.7470\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.2, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 409.7052, Pearson R: 0.3465, RMSE: 20.7706\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.2, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 12/100, Val Loss: 367.9672, Pearson R: 0.0377, RMSE: 19.7607\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.3, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 14/100, Val Loss: 403.9131, Pearson R: 0.1998, RMSE: 20.5897\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.3, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 10/100, Val Loss: 345.1942, Pearson R: 0.0901, RMSE: 19.1590\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.3, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 17/100, Val Loss: 320.3770, Pearson R: 0.1853, RMSE: 18.5139\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.3, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 11/100, Val Loss: 359.8985, Pearson R: 0.1372, RMSE: 19.5492\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.3, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 13/100, Val Loss: 516.4149, Pearson R: 0.1651, RMSE: 23.1518\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.3, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 10/100, Val Loss: 317.8388, Pearson R: 0.1038, RMSE: 18.4455\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.3, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 17/100, Val Loss: 274.4288, Pearson R: 0.5099, RMSE: 17.1823\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.3, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 28/100, Val Loss: 287.5150, Pearson R: 0.3989, RMSE: 17.5736\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.3, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 13/100, Val Loss: 335.2832, Pearson R: -0.0041, RMSE: 18.9164\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.3, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 10/100, Val Loss: 307.6522, Pearson R: 0.2131, RMSE: 18.1551\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.3, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 26/100, Val Loss: 363.1011, Pearson R: -0.3970, RMSE: 19.6686\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.3, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 13/100, Val Loss: 414.1842, Pearson R: 0.5676, RMSE: 20.8824\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.3, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 10/100, Val Loss: 415.3325, Pearson R: -0.1711, RMSE: 20.9595\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.3, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 33/100, Val Loss: 261.8215, Pearson R: 0.4202, RMSE: 16.7670\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.3, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 9/100, Val Loss: 320.3928, Pearson R: 0.4624, RMSE: 18.4735\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.3, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 16/100, Val Loss: 325.1678, Pearson R: -0.1316, RMSE: 18.6465\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.4, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 10/100, Val Loss: 426.5025, Pearson R: -0.1066, RMSE: 21.2150\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.4, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 13/100, Val Loss: 343.2921, Pearson R: 0.2601, RMSE: 19.1173\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.4, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 25/100, Val Loss: 235.7095, Pearson R: 0.5392, RMSE: 15.9222\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.4, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 12/100, Val Loss: 315.1303, Pearson R: 0.2198, RMSE: 18.3751\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.4, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 15/100, Val Loss: 368.3389, Pearson R: 0.2476, RMSE: 19.6722\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.4, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 16/100, Val Loss: 323.3122, Pearson R: -0.1142, RMSE: 18.6021\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.4, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 527.2744, Pearson R: -0.3478, RMSE: 23.5269\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.4, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 14/100, Val Loss: 341.1091, Pearson R: -0.2843, RMSE: 19.0887\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.4, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 13/100, Val Loss: 404.7034, Pearson R: 0.1511, RMSE: 20.6194\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.4, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 13/100, Val Loss: 353.1983, Pearson R: -0.1755, RMSE: 19.4027\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.4, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 8/100, Val Loss: 292.5168, Pearson R: 0.4066, RMSE: 17.6582\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.4, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21/100, Val Loss: 332.8034, Pearson R: -0.1315, RMSE: 18.8501\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.4, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 13/100, Val Loss: 302.7122, Pearson R: 0.2782, RMSE: 17.9803\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.4, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 18/100, Val Loss: 343.0262, Pearson R: 0.2540, RMSE: 19.0383\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.4, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 448.2290, Pearson R: 0.3426, RMSE: 21.6768\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.4, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 15/100, Val Loss: 329.9649, Pearson R: 0.0329, RMSE: 18.7747\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.5, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 15/100, Val Loss: 332.6356, Pearson R: -0.0117, RMSE: 18.8467\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.5, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 27/100, Val Loss: 304.5054, Pearson R: 0.2613, RMSE: 18.0703\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.5, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 304.1892, Pearson R: 0.3275, RMSE: 18.0466\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.5, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 14/100, Val Loss: 318.8460, Pearson R: 0.1109, RMSE: 18.4735\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.5, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 10/100, Val Loss: 516.4483, Pearson R: -0.2668, RMSE: 23.2827\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.5, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 16/100, Val Loss: 296.3168, Pearson R: 0.2584, RMSE: 17.8270\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.5, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 44/100, Val Loss: 324.7170, Pearson R: 0.2461, RMSE: 18.5552\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.5, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 18/100, Val Loss: 312.3279, Pearson R: 0.4878, RMSE: 18.2952\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.5, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 10/100, Val Loss: 402.2213, Pearson R: -0.2203, RMSE: 20.6473\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.5, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 16/100, Val Loss: 291.9301, Pearson R: 0.3376, RMSE: 17.7035\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.5, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 306.2142, Pearson R: 0.3530, RMSE: 18.1194\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.5, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 12/100, Val Loss: 289.0052, Pearson R: 0.4337, RMSE: 17.6172\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.5, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 10/100, Val Loss: 365.9958, Pearson R: -0.2697, RMSE: 19.7431\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.5, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 19/100, Val Loss: 316.0031, Pearson R: 0.1902, RMSE: 18.3582\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.5, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 6/100, Val Loss: 407.3502, Pearson R: 0.3980, RMSE: 20.6974\n",
      "{'batch_size': 512, 'lr': 0.01, 'dropout': 0.5, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 12/100, Val Loss: 371.5270, Pearson R: 0.4298, RMSE: 19.8331\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.1, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 278.2170, Pearson R: 0.4281, RMSE: 17.2771\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.1, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 24/100, Val Loss: 277.3124, Pearson R: 0.3896, RMSE: 17.2433\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.1, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 24/100, Val Loss: 272.3378, Pearson R: 0.4234, RMSE: 17.0748\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.1, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 18/100, Val Loss: 286.8599, Pearson R: 0.4180, RMSE: 17.5048\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.1, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 12/100, Val Loss: 283.6033, Pearson R: 0.4230, RMSE: 17.4178\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.1, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 25/100, Val Loss: 287.1207, Pearson R: 0.3991, RMSE: 17.4987\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.1, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 14/100, Val Loss: 282.3149, Pearson R: 0.4193, RMSE: 17.3942\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.1, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 18/100, Val Loss: 275.9947, Pearson R: 0.4176, RMSE: 17.2246\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.1, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 279.6442, Pearson R: 0.4289, RMSE: 17.3369\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.1, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 25/100, Val Loss: 287.7126, Pearson R: 0.4019, RMSE: 17.5129\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.1, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 13/100, Val Loss: 287.9817, Pearson R: 0.4258, RMSE: 17.5887\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.1, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 18/100, Val Loss: 273.8746, Pearson R: 0.4155, RMSE: 17.1511\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.1, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 276.7355, Pearson R: 0.4266, RMSE: 17.2447\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.1, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 21/100, Val Loss: 284.4774, Pearson R: 0.4071, RMSE: 17.4583\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.1, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 13/100, Val Loss: 283.9212, Pearson R: 0.4222, RMSE: 17.4628\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.1, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 18/100, Val Loss: 275.3919, Pearson R: 0.4156, RMSE: 17.1961\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.2, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 278.3102, Pearson R: 0.4287, RMSE: 17.2897\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.2, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 23/100, Val Loss: 282.2218, Pearson R: 0.3910, RMSE: 17.3684\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.2, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 21/100, Val Loss: 275.2460, Pearson R: 0.4250, RMSE: 17.1804\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.2, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 23/100, Val Loss: 281.5213, Pearson R: 0.4083, RMSE: 17.3895\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.2, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 278.6342, Pearson R: 0.4238, RMSE: 17.2711\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.2, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 21/100, Val Loss: 280.9067, Pearson R: 0.4118, RMSE: 17.3724\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.2, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 14/100, Val Loss: 279.9471, Pearson R: 0.4156, RMSE: 17.3132\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.2, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 23/100, Val Loss: 284.9468, Pearson R: 0.4032, RMSE: 17.4964\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.2, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 275.7356, Pearson R: 0.4274, RMSE: 17.2120\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.2, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 23/100, Val Loss: 274.1876, Pearson R: 0.3978, RMSE: 17.1556\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.2, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 23/100, Val Loss: 272.2020, Pearson R: 0.4190, RMSE: 17.0924\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.2, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 18/100, Val Loss: 280.2643, Pearson R: 0.4169, RMSE: 17.3565\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.2, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 276.1505, Pearson R: 0.4265, RMSE: 17.2205\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.2, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23/100, Val Loss: 273.2960, Pearson R: 0.3998, RMSE: 17.1303\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.2, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 14/100, Val Loss: 269.2851, Pearson R: 0.4128, RMSE: 17.0093\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.2, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 21/100, Val Loss: 274.1030, Pearson R: 0.4098, RMSE: 17.1429\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.3, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 12/100, Val Loss: 277.7704, Pearson R: 0.4238, RMSE: 17.2457\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.3, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 20/100, Val Loss: 278.5379, Pearson R: 0.4073, RMSE: 17.3004\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.3, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 22/100, Val Loss: 272.1257, Pearson R: 0.4261, RMSE: 17.0914\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.3, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 19/100, Val Loss: 278.4856, Pearson R: 0.4179, RMSE: 17.2789\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.3, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 281.0443, Pearson R: 0.4289, RMSE: 17.3753\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.3, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 24/100, Val Loss: 276.3028, Pearson R: 0.4055, RMSE: 17.2060\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.3, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 283.4103, Pearson R: 0.4310, RMSE: 17.4525\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.3, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 19/100, Val Loss: 276.5965, Pearson R: 0.4143, RMSE: 17.2428\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.3, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 276.4832, Pearson R: 0.4244, RMSE: 17.2180\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.3, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 22/100, Val Loss: 273.0374, Pearson R: 0.4031, RMSE: 17.1184\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.3, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 12/100, Val Loss: 284.1558, Pearson R: 0.4278, RMSE: 17.4569\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.3, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 18/100, Val Loss: 273.6084, Pearson R: 0.4110, RMSE: 17.1442\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.3, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 12/100, Val Loss: 280.7053, Pearson R: 0.4218, RMSE: 17.3287\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.3, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 20/100, Val Loss: 284.4779, Pearson R: 0.4069, RMSE: 17.4511\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.3, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 16/100, Val Loss: 275.8196, Pearson R: 0.4237, RMSE: 17.1872\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.3, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 18/100, Val Loss: 277.0380, Pearson R: 0.4162, RMSE: 17.2570\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.4, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 12/100, Val Loss: 277.5394, Pearson R: 0.4244, RMSE: 17.2507\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.4, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 22/100, Val Loss: 273.6376, Pearson R: 0.3980, RMSE: 17.1198\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.4, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 13/100, Val Loss: 271.2555, Pearson R: 0.4298, RMSE: 17.0718\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.4, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 19/100, Val Loss: 274.1805, Pearson R: 0.4182, RMSE: 17.1633\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.4, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 12/100, Val Loss: 281.6341, Pearson R: 0.4206, RMSE: 17.3516\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.4, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 24/100, Val Loss: 272.8881, Pearson R: 0.4069, RMSE: 17.1225\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.4, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 292.6282, Pearson R: 0.4276, RMSE: 17.6786\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.4, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 21/100, Val Loss: 272.3481, Pearson R: 0.4126, RMSE: 17.1039\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.4, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 12/100, Val Loss: 275.4328, Pearson R: 0.4173, RMSE: 17.1683\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.4, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 22/100, Val Loss: 277.9579, Pearson R: 0.3971, RMSE: 17.2485\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.4, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 282.8362, Pearson R: 0.4294, RMSE: 17.4138\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.4, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 18/100, Val Loss: 274.8883, Pearson R: 0.4135, RMSE: 17.1878\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.4, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 11/100, Val Loss: 279.3534, Pearson R: 0.4284, RMSE: 17.3209\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.4, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 23/100, Val Loss: 271.6404, Pearson R: 0.4085, RMSE: 17.0775\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.4, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 16/100, Val Loss: 273.0074, Pearson R: 0.4250, RMSE: 17.1171\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.4, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 18/100, Val Loss: 274.7725, Pearson R: 0.4155, RMSE: 17.1820\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.5, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 12/100, Val Loss: 276.3791, Pearson R: 0.4274, RMSE: 17.2317\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.5, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 17/100, Val Loss: 280.8063, Pearson R: 0.4082, RMSE: 17.3306\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.5, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 12/100, Val Loss: 270.9454, Pearson R: 0.4284, RMSE: 17.0545\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.5, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 19/100, Val Loss: 285.3300, Pearson R: 0.4152, RMSE: 17.4714\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.5, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 12/100, Val Loss: 276.3613, Pearson R: 0.4175, RMSE: 17.1994\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.5, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 22/100, Val Loss: 279.7837, Pearson R: 0.4045, RMSE: 17.2846\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.5, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 9/100, Val Loss: 287.0070, Pearson R: 0.4319, RMSE: 17.5525\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.5, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 27/100, Val Loss: 277.2557, Pearson R: 0.4014, RMSE: 17.2527\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.5, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 12/100, Val Loss: 274.8828, Pearson R: 0.4234, RMSE: 17.1760\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.5, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 21/100, Val Loss: 280.3961, Pearson R: 0.4046, RMSE: 17.3083\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.5, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 19/100, Val Loss: 276.0481, Pearson R: 0.4246, RMSE: 17.1992\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.5, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 22/100, Val Loss: 287.3777, Pearson R: 0.4135, RMSE: 17.5342\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.5, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 12/100, Val Loss: 276.4808, Pearson R: 0.4264, RMSE: 17.2330\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.5, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 23/100, Val Loss: 271.1775, Pearson R: 0.4078, RMSE: 17.0611\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.5, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/100, Val Loss: 286.7818, Pearson R: 0.4290, RMSE: 17.5190\n",
      "{'batch_size': 512, 'lr': 0.001, 'dropout': 0.5, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 21/100, Val Loss: 273.1457, Pearson R: 0.4130, RMSE: 17.1218\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.1, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 45/100, Val Loss: 274.1297, Pearson R: 0.4230, RMSE: 17.1520\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.1, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 31/100, Val Loss: 273.2817, Pearson R: 0.4193, RMSE: 17.1215\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.1, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 30/100, Val Loss: 274.4193, Pearson R: 0.4272, RMSE: 17.1618\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.1, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 20/100, Val Loss: 274.7588, Pearson R: 0.4275, RMSE: 17.1714\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.1, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 43/100, Val Loss: 275.1967, Pearson R: 0.4223, RMSE: 17.1862\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.1, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 29/100, Val Loss: 276.3490, Pearson R: 0.4199, RMSE: 17.2170\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.1, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 29/100, Val Loss: 275.4708, Pearson R: 0.4256, RMSE: 17.1939\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.1, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 20/100, Val Loss: 274.2613, Pearson R: 0.4230, RMSE: 17.1471\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.1, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 43/100, Val Loss: 274.4471, Pearson R: 0.4210, RMSE: 17.1614\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.1, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 29/100, Val Loss: 275.7987, Pearson R: 0.4184, RMSE: 17.1992\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.1, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 29/100, Val Loss: 275.3846, Pearson R: 0.4246, RMSE: 17.1901\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.1, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 19/100, Val Loss: 275.5209, Pearson R: 0.4254, RMSE: 17.1974\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.1, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 43/100, Val Loss: 274.7795, Pearson R: 0.4212, RMSE: 17.1728\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.1, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 29/100, Val Loss: 275.7538, Pearson R: 0.4181, RMSE: 17.1976\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.1, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 29/100, Val Loss: 275.0935, Pearson R: 0.4243, RMSE: 17.1806\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.1, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 19/100, Val Loss: 274.7384, Pearson R: 0.4256, RMSE: 17.1756\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.2, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 48/100, Val Loss: 274.1610, Pearson R: 0.4212, RMSE: 17.1514\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.2, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 32/100, Val Loss: 274.6435, Pearson R: 0.4193, RMSE: 17.1640\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.2, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 33/100, Val Loss: 275.4324, Pearson R: 0.4256, RMSE: 17.1889\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.2, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 21/100, Val Loss: 274.3520, Pearson R: 0.4271, RMSE: 17.1633\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.2, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 46/100, Val Loss: 274.9213, Pearson R: 0.4197, RMSE: 17.1759\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.2, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 31/100, Val Loss: 275.0074, Pearson R: 0.4155, RMSE: 17.1705\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.2, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 32/100, Val Loss: 275.4124, Pearson R: 0.4233, RMSE: 17.1879\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.2, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 21/100, Val Loss: 273.6271, Pearson R: 0.4231, RMSE: 17.1351\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.2, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 46/100, Val Loss: 274.1868, Pearson R: 0.4181, RMSE: 17.1514\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.2, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 30/100, Val Loss: 276.4763, Pearson R: 0.4179, RMSE: 17.2203\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.2, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 32/100, Val Loss: 274.8569, Pearson R: 0.4226, RMSE: 17.1720\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.2, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 21/100, Val Loss: 273.9728, Pearson R: 0.4220, RMSE: 17.1428\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.2, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 46/100, Val Loss: 274.8864, Pearson R: 0.4183, RMSE: 17.1729\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.2, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 30/100, Val Loss: 276.5520, Pearson R: 0.4188, RMSE: 17.2248\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.2, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 32/100, Val Loss: 275.0814, Pearson R: 0.4224, RMSE: 17.1765\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.2, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 21/100, Val Loss: 273.8388, Pearson R: 0.4224, RMSE: 17.1415\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.3, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 51/100, Val Loss: 274.0492, Pearson R: 0.4191, RMSE: 17.1468\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.3, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 33/100, Val Loss: 275.6357, Pearson R: 0.4195, RMSE: 17.1939\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.3, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 36/100, Val Loss: 274.9018, Pearson R: 0.4246, RMSE: 17.1748\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.3, 'lr_decay': 0.1, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 23/100, Val Loss: 274.3187, Pearson R: 0.4254, RMSE: 17.1586\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.3, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 49/100, Val Loss: 275.1703, Pearson R: 0.4174, RMSE: 17.1815\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.3, 'lr_decay': 0.01, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 32/100, Val Loss: 276.3835, Pearson R: 0.4167, RMSE: 17.2157\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.3, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 34/100, Val Loss: 276.1950, Pearson R: 0.4237, RMSE: 17.2160\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.3, 'lr_decay': 0.01, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 22/100, Val Loss: 275.7328, Pearson R: 0.4248, RMSE: 17.2020\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.3, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 48/100, Val Loss: 276.1130, Pearson R: 0.4186, RMSE: 17.2118\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.3, 'lr_decay': 0.001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 32/100, Val Loss: 275.7520, Pearson R: 0.4147, RMSE: 17.1949\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.3, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 35/100, Val Loss: 275.0309, Pearson R: 0.4221, RMSE: 17.1799\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.3, 'lr_decay': 0.001, 'layers': 4, 'hidden_dim': 800}\n",
      "Epoch: 22/100, Val Loss: 276.3317, Pearson R: 0.4246, RMSE: 17.2217\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.3, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 48/100, Val Loss: 275.2765, Pearson R: 0.4186, RMSE: 17.1875\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.3, 'lr_decay': 0.0001, 'layers': 2, 'hidden_dim': 800}\n",
      "Epoch: 32/100, Val Loss: 275.4596, Pearson R: 0.4140, RMSE: 17.1849\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.3, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 400}\n",
      "Epoch: 35/100, Val Loss: 275.2048, Pearson R: 0.4214, RMSE: 17.1823\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.3, 'lr_decay': 0.0001, 'layers': 4, 'hidden_dim': 800}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22/100, Val Loss: 275.2436, Pearson R: 0.4238, RMSE: 17.1888\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.4, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 400}\n",
      "Epoch: 54/100, Val Loss: 274.0506, Pearson R: 0.4183, RMSE: 17.1476\n",
      "{'batch_size': 512, 'lr': 0.0001, 'dropout': 0.4, 'lr_decay': 0.1, 'layers': 2, 'hidden_dim': 800}\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameter selection for pre-training\n",
    "\n",
    "# Hyperparameter grid\n",
    "grid_search = {\n",
    "    'batch_size': [512],\n",
    "    'lr': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "    'dropout': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'lr_decay': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "    'layers': [2, 4],\n",
    "    'hidden_dim': [400, 800]\n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "all_params = [dict(zip(grid_search.keys(), values)) for values in product(*grid_search.values())]\n",
    "\n",
    "num_layers_arr = []\n",
    "dropout_arr = []\n",
    "lr_arr = []\n",
    "lr_decay_arr = []\n",
    "hidden_dim_arr = []\n",
    "batch_size_arr = []\n",
    "split_num_arr = []\n",
    "loss_arr = []\n",
    "for i in tqdm(outcome_list):\n",
    "    overall_best_params[i] = {'PT': {}}\n",
    "\n",
    "    maternal_IDs = patient_indices_PTMODEL['sample_ID'].str[0:7].unique()\n",
    "    \n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    split_num = 0\n",
    "    for train_index, test_index in kf.split(maternal_IDs):\n",
    "        split_num += 1\n",
    "        results_dict = {}\n",
    "        train_IDs = maternal_IDs[train_index]\n",
    "        test_IDs = maternal_IDs[test_index]\n",
    "        \n",
    "        sample_size = int(0.2 * len(train_index))\n",
    "        random_indices = np.random.choice(train_IDs.shape[0], sample_size, replace=False)\n",
    "        val_IDs = train_IDs[random_indices]\n",
    "        train_IDs = np.delete(train_IDs, random_indices)\n",
    "        \n",
    "        patient_indices_PTMODEL['maternal_ID'] = patient_indices_PTMODEL['sample_ID'].str[0:7]\n",
    "\n",
    "        train_indices = patient_indices_PTMODEL[patient_indices_PTMODEL['maternal_ID'].isin(train_IDs)]['array_index'].values\n",
    "        np.random.shuffle(train_indices)\n",
    "        test_indices = patient_indices_PTMODEL[patient_indices_PTMODEL['maternal_ID'].isin(test_IDs)]['array_index'].values\n",
    "        val_indices = patient_indices_PTMODEL[patient_indices_PTMODEL['maternal_ID'].isin(val_IDs)]['array_index'].values\n",
    "\n",
    "        # Iterate through the hyperparameter combinations\n",
    "        for param_set in tqdm(all_params):\n",
    "            bs = param_set['batch_size']\n",
    "            lr = param_set['lr']\n",
    "            dropout = param_set['dropout']\n",
    "            lr_decay = param_set['lr_decay']\n",
    "            layers = param_set['layers']\n",
    "            hidden_dim = param_set['hidden_dim']\n",
    "            \n",
    "            model_name = 'PT_MODEL_{}_{}_{}_{}_{}_noHCE_with_obs'.format(layers,lr,lr_decay,dropout, hidden_dim)\n",
    "            print(param_set)\n",
    "            val_r, val_loss, val_rmse = run_experiment(RNN_data_codes_PTMODEL, OOL_proteomics,\n",
    "            patient_indices_PTMODEL, RNN_data_outcomes_PTMODEL, RNN_data_lengths_PTMODEL, model_name, \n",
    "            lr, lr_decay, bs, train_indices=train_indices, test_indices=test_indices,\n",
    "                                                       val_indices=val_indices,feature_types='EHR', model_path='', fine_tune=False, seed=42, \n",
    "            hidden_dim=hidden_dim, num_layers=layers, dropout=dropout, hyperparam_tuning=True)\n",
    "            num_layers_arr.append(layers)\n",
    "            dropout_arr.append(dropout)\n",
    "            lr_arr.append(lr)\n",
    "            lr_decay_arr.append(lr_decay)\n",
    "            hidden_dim_arr.append(hidden_dim)\n",
    "            batch_size_arr.append(bs)\n",
    "            split_num_arr.append(split_num)\n",
    "            loss_arr.append(val_loss)\n",
    "\n",
    "        print('PT')\n",
    "        print('outcome {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4fb602",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_df = pd.DataFrame([num_layers_arr, dropout_arr, lr_arr, lr_decay_arr, hidden_dim_arr, batch_size_arr, split_num_arr,loss_arr]).T\n",
    "hyperparam_df.columns = ['num_layers','dropout','lr','lr_decay','hidden_dim','bs','split_num','val_loss']\n",
    "hyperparam_df = hyperparam_df.groupby(['num_layers','dropout','lr','lr_decay','hidden_dim','bs']).mean()\n",
    "num_layers, dropout, lr, lr_decay, hidden_dim, bs = hyperparam_df['val_loss'].idxmin()\n",
    "print(np.min(hyperparam_df['val_loss']))\n",
    "model_name = 'PT_MODEL_{}_{}_{}_{}_{}_noHCE_with_obs'.format(num_layers,lr,lr_decay,dropout, hidden_dim)\n",
    "overall_best_params['DOS']['PT'] = {'num_layers': int(num_layers),'lr': lr,'lr_decay': lr_decay, 'dropout': dropout,\n",
    "                                'hidden_dim': int(hidden_dim), 'batch_size': int(bs), 'model_name': model_name}\n",
    "with open(\"./models/hyperparameters/best_hyperparams_OOL_noHCE_with_obs.pkl\", \"wb\") as f:\n",
    "        pickle.dump(overall_best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da9fb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_num_layers = overall_best_params['DOS']['PT']['num_layers']\n",
    "best_dropout = overall_best_params['DOS']['PT']['dropout']\n",
    "best_model_name = overall_best_params['DOS']['PT']['model_name']\n",
    "best_hidden_dim = overall_best_params['DOS']['PT']['hidden_dim']\n",
    "best_num_layers, best_dropout, best_hidden_dim, best_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a656d193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create PT model\n",
    "val_r, val_loss, val_rmse = run_experiment(RNN_data_codes_PTMODEL, OOL_proteomics,\n",
    "    patient_indices_PTMODEL, RNN_data_outcomes_PTMODEL, RNN_data_lengths_PTMODEL, best_model_name, \n",
    "    overall_best_params['DOS']['PT']['lr'], overall_best_params['DOS']['PT']['lr_decay'],\n",
    "       overall_best_params['DOS']['PT']['batch_size'], feature_types='EHR', model_path='', fine_tune=False, seed=42, \n",
    "    hidden_dim=best_hidden_dim, num_layers=best_num_layers, dropout=best_dropout, hyperparam_tuning=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64caed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid for EHR baseline hyperparam search\n",
    "grid_search = {\n",
    "    'batch_size': [16],\n",
    "    'lr': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "    'dropout': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'lr_decay': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "    'layers': [2, 4],\n",
    "    'hidden_dim': [400, 800]\n",
    "}\n",
    "all_params = [dict(zip(grid_search.keys(), values)) for values in product(*grid_search.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e09b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter search for EHR baseline\n",
    "num_layers_arr = []\n",
    "dropout_arr = []\n",
    "lr_arr = []\n",
    "lr_decay_arr = []\n",
    "hidden_dim_arr = []\n",
    "batch_size_arr = []\n",
    "split_num_arr = []\n",
    "loss_arr = []\n",
    "for i in tqdm(outcome_list):\n",
    "    overall_best_params[i]['exp1'] = {}\n",
    "\n",
    "    maternal_IDs = patient_indices_OOL['sample_ID'].str[0:7].unique()\n",
    "    \n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    split_num = 0\n",
    "    for train_index, test_index in kf.split(maternal_IDs):\n",
    "        split_num += 1\n",
    "        results_dict = {}\n",
    "        train_IDs = maternal_IDs[train_index]\n",
    "        test_IDs = maternal_IDs[test_index]\n",
    "        \n",
    "        sample_size = int(0.2 * len(train_index))\n",
    "        random_indices = np.random.choice(train_IDs.shape[0], sample_size, replace=False)\n",
    "        val_IDs = train_IDs[random_indices]\n",
    "        train_IDs = np.delete(train_IDs, random_indices)\n",
    "        \n",
    "        patient_indices_OOL['maternal_ID'] = patient_indices_OOL['sample_ID'].str[0:7]\n",
    "        patient_indices_OOL['maternal_ID_ts'] = patient_indices_OOL['maternal_ID'].astype(str)+'_'+patient_indices_OOL['sample_ID'].str[-2:]\n",
    "        input_OOL_proteomics = OOL_proteomics.merge(patient_indices_OOL[['maternal_ID_ts','array_index']], how='left', left_on='sample_ID', right_on='maternal_ID_ts').drop(['sample_ID','maternal_ID_ts','DOS'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "\n",
    "        train_indices = patient_indices_OOL[patient_indices_OOL['maternal_ID'].isin(train_IDs)]['array_index'].values\n",
    "        np.random.shuffle(train_indices)\n",
    "        test_indices = patient_indices_OOL[patient_indices_OOL['maternal_ID'].isin(test_IDs)]['array_index'].values\n",
    "        val_indices = patient_indices_OOL[patient_indices_OOL['maternal_ID'].isin(val_IDs)]['array_index'].values\n",
    "\n",
    "        # Iterate through the hyperparameter combinations\n",
    "        for param_set in tqdm(all_params):\n",
    "            bs = param_set['batch_size']\n",
    "            lr = param_set['lr']\n",
    "            dropout = param_set['dropout']\n",
    "            lr_decay = param_set['lr_decay']\n",
    "            layers = param_set['layers']\n",
    "            hidden_dim = param_set['hidden_dim']\n",
    "            print(param_set)\n",
    "            val_r, val_loss, val_rmse = run_experiment(RNN_data_codes_OOL, input_OOL_proteomics,\n",
    "                                                       patient_indices_OOL, RNN_data_outcomes_OOL,\n",
    "                                                       RNN_data_lengths_OOL, 'EHR_OOL_only_noHCE_with_obs',\n",
    "                                                       lr, lr_decay, bs, \n",
    "                                                       train_indices=train_indices, test_indices=test_indices,\n",
    "                                                       val_indices=val_indices, feature_types='EHR', model_path='',\n",
    "                                                       fine_tune=False, seed=42, hidden_dim=hidden_dim,\n",
    "                                                       num_layers=layers, dropout=dropout, hyperparam_tuning=True)\n",
    "\n",
    "            num_layers_arr.append(layers)\n",
    "            dropout_arr.append(dropout)\n",
    "            lr_arr.append(lr)\n",
    "            lr_decay_arr.append(lr_decay)\n",
    "            hidden_dim_arr.append(hidden_dim)\n",
    "            batch_size_arr.append(bs)\n",
    "            split_num_arr.append(split_num)\n",
    "            loss_arr.append(val_loss)\n",
    "\n",
    "        print('experiment 1')\n",
    "        print('outcome {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60683ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_df = pd.DataFrame([num_layers_arr, dropout_arr, lr_arr, lr_decay_arr, hidden_dim_arr, batch_size_arr, split_num_arr,loss_arr]).T\n",
    "hyperparam_df.columns = ['num_layers','dropout','lr','lr_decay','hidden_dim','bs','split_num','val_loss']\n",
    "hyperparam_df = hyperparam_df.groupby(['num_layers','dropout','lr','lr_decay','hidden_dim','bs']).mean()\n",
    "num_layers, dropout, lr, lr_decay, hidden_dim, bs = hyperparam_df['val_loss'].idxmin()\n",
    "print(np.min(hyperparam_df['val_loss']))\n",
    "\n",
    "print(num_layers, dropout, lr, lr_decay, hidden_dim, bs)\n",
    "\n",
    "overall_best_params['DOS']['exp1'] = {'num_layers': int(num_layers),'lr': lr,'lr_decay': lr_decay, 'dropout': dropout,\n",
    "                                'hidden_dim': int(hidden_dim), 'batch_size': int(bs)}\n",
    "with open(\"./models/hyperparameters/best_hyperparams_OOL_noHCE_with_obs.pkl\", \"wb\") as f:\n",
    "        pickle.dump(overall_best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f5f295",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_num_layers_OOL = overall_best_params['DOS']['exp1']['num_layers']\n",
    "best_dropout_OOL = overall_best_params['DOS']['exp1']['dropout']\n",
    "best_hidden_dim_OOL = overall_best_params['DOS']['exp1']['hidden_dim']\n",
    "best_num_layers_OOL, best_dropout_OOL, best_hidden_dim_OOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaddc719",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid for omics baseline hyperparam search\n",
    "grid_search = {'batch_size': [16],\n",
    "              'lr': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "              'lr_decay': [1e-1, 1e-2, 1e-3, 1e-4]}\n",
    "\n",
    "all_params = [dict(zip(grid_search.keys(), values)) for values in product(*grid_search.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c951eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367aa962",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform omics baseline hyperparam search\n",
    "num_layers_arr = []\n",
    "dropout_arr = []\n",
    "lr_arr = []\n",
    "lr_decay_arr = []\n",
    "hidden_dim_arr = []\n",
    "batch_size_arr = []\n",
    "split_num_arr = []\n",
    "loss_arr = []\n",
    "for i in tqdm(outcome_list):\n",
    "    overall_best_params[i]['exp2'] = {}\n",
    "\n",
    "    maternal_IDs = patient_indices_OOL['sample_ID'].str[0:7].unique()\n",
    "    \n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    split_num = 0\n",
    "    for train_index, test_index in kf.split(maternal_IDs):\n",
    "        split_num += 1\n",
    "        results_dict = {}\n",
    "        train_IDs = maternal_IDs[train_index]\n",
    "        test_IDs = maternal_IDs[test_index]\n",
    "        \n",
    "        sample_size = int(0.2 * len(train_index))\n",
    "        random_indices = np.random.choice(train_IDs.shape[0], sample_size, replace=False)\n",
    "        val_IDs = train_IDs[random_indices]\n",
    "        train_IDs = np.delete(train_IDs, random_indices)\n",
    "        \n",
    "        patient_indices_OOL['maternal_ID'] = patient_indices_OOL['sample_ID'].str[0:7]\n",
    "        patient_indices_OOL['maternal_ID_ts'] = patient_indices_OOL['maternal_ID'].astype(str)+'_'+patient_indices_OOL['sample_ID'].str[-2:]\n",
    "        input_OOL_proteomics = OOL_proteomics.merge(patient_indices_OOL[['maternal_ID_ts','array_index']], how='left', left_on='sample_ID', right_on='maternal_ID_ts').drop(['sample_ID','maternal_ID_ts','DOS'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "\n",
    "        train_indices = patient_indices_OOL[patient_indices_OOL['maternal_ID'].isin(train_IDs)]['array_index'].values\n",
    "        np.random.shuffle(train_indices)\n",
    "        test_indices = patient_indices_OOL[patient_indices_OOL['maternal_ID'].isin(test_IDs)]['array_index'].values\n",
    "        val_indices = patient_indices_OOL[patient_indices_OOL['maternal_ID'].isin(val_IDs)]['array_index'].values\n",
    "\n",
    "        # Iterate through the hyperparameter combinations\n",
    "        for param_set in tqdm(all_params):\n",
    "            bs = param_set['batch_size']\n",
    "            lr = param_set['lr']\n",
    "            dropout = best_dropout_OOL\n",
    "            lr_decay = param_set['lr_decay']\n",
    "            layers = best_num_layers_OOL\n",
    "            hidden_dim = best_hidden_dim_OOL\n",
    "            print(param_set)\n",
    "            val_r, val_loss, val_rmse = run_experiment(RNN_data_codes_OOL, input_OOL_proteomics,\n",
    "                patient_indices_OOL, RNN_data_outcomes_OOL, RNN_data_lengths_OOL, 'proteomics_OOL_only_noHCE_with_obs', \n",
    "                lr, lr_decay, bs, train_indices=train_indices, test_indices=test_indices, val_indices=val_indices,\n",
    "                feature_types='metab', model_path='', fine_tune=False, seed=42, hidden_dim=hidden_dim,\n",
    "                num_layers=layers, dropout=dropout, hyperparam_tuning=True)\n",
    "            \n",
    "            num_layers_arr.append(layers)\n",
    "            dropout_arr.append(dropout)\n",
    "            lr_arr.append(lr)\n",
    "            lr_decay_arr.append(lr_decay)\n",
    "            hidden_dim_arr.append(hidden_dim)\n",
    "            batch_size_arr.append(bs)\n",
    "            split_num_arr.append(split_num)\n",
    "            loss_arr.append(val_loss)\n",
    "\n",
    "        print('experiment 2')\n",
    "        print('outcome {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75917d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_df = pd.DataFrame([num_layers_arr, dropout_arr, lr_arr, lr_decay_arr, hidden_dim_arr, batch_size_arr, split_num_arr,loss_arr]).T\n",
    "hyperparam_df.columns = ['num_layers','dropout','lr','lr_decay','hidden_dim','bs','split_num','val_loss']\n",
    "hyperparam_df = hyperparam_df.groupby(['num_layers','dropout','lr','lr_decay','hidden_dim','bs']).mean()\n",
    "num_layers, dropout, lr, lr_decay, hidden_dim, bs = hyperparam_df['val_loss'].idxmin()\n",
    "print(np.min(hyperparam_df['val_loss']))\n",
    "print(num_layers, dropout, lr, lr_decay, hidden_dim, bs)\n",
    "\n",
    "overall_best_params['DOS']['exp2'] = {'num_layers': int(num_layers),'lr': lr,'lr_decay': lr_decay, 'dropout': dropout,\n",
    "                                'hidden_dim': int(hidden_dim), 'batch_size': int(bs)}\n",
    "with open(\"./models/hyperparameters/best_hyperparams_OOL_noHCE_with_obs.pkl\", \"wb\") as f:\n",
    "        pickle.dump(overall_best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904881c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid for joint baseline hyperparam search\n",
    "grid_search = {'batch_size': [16],\n",
    "              'lr': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "              'dropout': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "              'lr_decay': [1e-1, 1e-2, 1e-3, 1e-4]}\n",
    "\n",
    "all_params = [dict(zip(grid_search.keys(), values)) for values in product(*grid_search.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819f7dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#joint baseline hyperparam search\n",
    "\n",
    "num_layers_arr = []\n",
    "dropout_arr = []\n",
    "lr_arr = []\n",
    "lr_decay_arr = []\n",
    "hidden_dim_arr = []\n",
    "batch_size_arr = []\n",
    "split_num_arr = []\n",
    "loss_arr = []\n",
    "for i in tqdm(outcome_list):\n",
    "    overall_best_params[i]['exp3'] = {}\n",
    "\n",
    "    maternal_IDs = patient_indices_OOL['sample_ID'].str[0:7].unique()\n",
    "    \n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    split_num = 0\n",
    "    for train_index, test_index in kf.split(maternal_IDs):\n",
    "        split_num += 1\n",
    "        results_dict = {}\n",
    "        train_IDs = maternal_IDs[train_index]\n",
    "        test_IDs = maternal_IDs[test_index]\n",
    "        \n",
    "        sample_size = int(0.2 * len(train_index))\n",
    "        random_indices = np.random.choice(train_IDs.shape[0], sample_size, replace=False)\n",
    "        val_IDs = train_IDs[random_indices]\n",
    "        train_IDs = np.delete(train_IDs, random_indices)\n",
    "        \n",
    "        patient_indices_OOL['maternal_ID'] = patient_indices_OOL['sample_ID'].str[0:7]\n",
    "        patient_indices_OOL['maternal_ID_ts'] = patient_indices_OOL['maternal_ID'].astype(str)+'_'+patient_indices_OOL['sample_ID'].str[-2:]\n",
    "        input_OOL_proteomics = OOL_proteomics.merge(patient_indices_OOL[['maternal_ID_ts','array_index']], how='left', left_on='sample_ID', right_on='maternal_ID_ts').drop(['sample_ID','maternal_ID_ts','DOS'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "\n",
    "        train_indices = patient_indices_OOL[patient_indices_OOL['maternal_ID'].isin(train_IDs)]['array_index'].values\n",
    "        np.random.shuffle(train_indices)\n",
    "        test_indices = patient_indices_OOL[patient_indices_OOL['maternal_ID'].isin(test_IDs)]['array_index'].values\n",
    "        val_indices = patient_indices_OOL[patient_indices_OOL['maternal_ID'].isin(val_IDs)]['array_index'].values\n",
    "\n",
    "        # Iterate through the hyperparameter combinations\n",
    "        for param_set in tqdm(all_params):\n",
    "            bs = param_set['batch_size']\n",
    "            lr = param_set['lr']\n",
    "            dropout = param_set['dropout']\n",
    "            lr_decay = param_set['lr_decay']\n",
    "            layers = best_num_layers_OOL\n",
    "            hidden_dim = best_hidden_dim_OOL\n",
    "            print(param_set)\n",
    "            \n",
    "            val_r, val_loss, val_rmse = run_experiment(RNN_data_codes_OOL, input_OOL_proteomics,\n",
    "                patient_indices_OOL, RNN_data_outcomes_OOL, RNN_data_lengths_OOL, 'both_OOL_only_noHCE_with_obs', \n",
    "                lr, lr_decay, bs, train_indices=train_indices, test_indices=test_indices,\n",
    "                val_indices=val_indices, feature_types='both', model_path='', fine_tune=False, seed=42,\n",
    "                hidden_dim=hidden_dim, num_layers=layers, dropout=dropout, hyperparam_tuning=True)\n",
    "\n",
    "            num_layers_arr.append(layers)\n",
    "            dropout_arr.append(dropout)\n",
    "            lr_arr.append(lr)\n",
    "            lr_decay_arr.append(lr_decay)\n",
    "            hidden_dim_arr.append(hidden_dim)\n",
    "            batch_size_arr.append(bs)\n",
    "            split_num_arr.append(split_num)\n",
    "            loss_arr.append(val_loss)\n",
    "\n",
    "        print('experiment 3')\n",
    "        print('outcome {}'.format(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecf6885",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_df = pd.DataFrame([num_layers_arr, dropout_arr, lr_arr, lr_decay_arr, hidden_dim_arr, batch_size_arr, split_num_arr,loss_arr]).T\n",
    "hyperparam_df.columns = ['num_layers','dropout','lr','lr_decay','hidden_dim','bs','split_num','val_loss']\n",
    "hyperparam_df = hyperparam_df.groupby(['num_layers','dropout','lr','lr_decay','hidden_dim','bs']).mean()\n",
    "num_layers, dropout, lr, lr_decay, hidden_dim, bs = hyperparam_df['val_loss'].idxmin()\n",
    "print(np.min(hyperparam_df['val_loss']))\n",
    "print(num_layers, dropout, lr, lr_decay, hidden_dim, bs)\n",
    "\n",
    "overall_best_params['DOS']['exp3'] = {'num_layers': int(num_layers),'lr': lr,'lr_decay': lr_decay, 'dropout': dropout,\n",
    "                                'hidden_dim': int(hidden_dim), 'batch_size': int(bs)}\n",
    "with open(\"./models/hyperparameters/best_hyperparams_OOL_noHCE_with_obs.pkl\", \"wb\") as f:\n",
    "        pickle.dump(overall_best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47128c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test pre-trained EHR model on omics data\n",
    "\n",
    "for i in tqdm(outcome_list):\n",
    "    overall_best_params[i]['exp4'] = {}\n",
    "    results_dict = {}\n",
    "\n",
    "    # set bs large enough to test all data in one pass\n",
    "    # other params don't matter since there is no training here....they are just set arbitrarily and saved to avoid issues with downstream code\n",
    "    bs = 1000\n",
    "    lr = 0.1\n",
    "    dropout = best_dropout\n",
    "    lr_decay = 0.1\n",
    "    layers = best_num_layers\n",
    "    hidden_dim = best_hidden_dim\n",
    "    print(param_set)\n",
    "    val_r, val_loss, val_rmse = run_experiment(RNN_data_codes, OOL_proteomics,\n",
    "            patient_indices, RNN_data_outcomes, RNN_data_lengths, 'EHR_OOL_PT_noHCE_with_obs', \n",
    "            lr, lr_decay, bs, feature_types='EHR', model_path='./models/predictive_models/{}.pth'.format(best_model_name),\n",
    "                                               fine_tune=False, seed=42,\n",
    "                                               hidden_dim=hidden_dim,\n",
    "                                              num_layers=layers, dropout=dropout, hyperparam_tuning=False)\n",
    "\n",
    "    results_dict[val_loss] = {'num_layers': layers,'lr': lr,'lr_decay': lr_decay, 'dropout': dropout,\n",
    "                            'hidden_dim': hidden_dim, 'batch_size': bs}\n",
    "\n",
    "    print('experiment 4')\n",
    "    print('outcome {}'.format(i))\n",
    "    overall_best_params[i]['exp4'] = results_dict[min(results_dict.keys())]\n",
    "    print(results_dict[min(results_dict.keys())])\n",
    "\n",
    "with open(\"./models/hyperparameters/best_hyperparams_OOL_noHCE_with_obs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(overall_best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fea1a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid search for EHR fine-tuning experiment (only fine-tuning the EHR pre-trained model)\n",
    "grid_search = {'batch_size': [16],\n",
    "              'lr': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "              'lr_decay': [1e-1, 1e-2, 1e-3, 1e-4]}\n",
    "\n",
    "all_params = [dict(zip(grid_search.keys(), values)) for values in product(*grid_search.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec97bdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#actual hyperparam search\n",
    "\n",
    "num_layers_arr = []\n",
    "dropout_arr = []\n",
    "lr_arr = []\n",
    "lr_decay_arr = []\n",
    "hidden_dim_arr = []\n",
    "batch_size_arr = []\n",
    "split_num_arr = []\n",
    "loss_arr = []\n",
    "for i in tqdm(outcome_list):\n",
    "    overall_best_params[i]['exp5'] = {}\n",
    "\n",
    "    maternal_IDs = patient_indices['sample_ID'].str[0:7].unique()\n",
    "    \n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    split_num = 0\n",
    "    for train_index, test_index in kf.split(maternal_IDs):\n",
    "        split_num += 1\n",
    "        results_dict = {}\n",
    "        train_IDs = maternal_IDs[train_index]\n",
    "        test_IDs = maternal_IDs[test_index]\n",
    "        \n",
    "        sample_size = int(0.2 * len(train_index))\n",
    "        random_indices = np.random.choice(train_IDs.shape[0], sample_size, replace=False)\n",
    "        val_IDs = train_IDs[random_indices]\n",
    "        train_IDs = np.delete(train_IDs, random_indices)\n",
    "        \n",
    "        patient_indices['maternal_ID'] = patient_indices['sample_ID'].str[0:7]\n",
    "        patient_indices['maternal_ID_ts'] = patient_indices['maternal_ID'].astype(str)+'_'+patient_indices['sample_ID'].str[-2:]\n",
    "        input_OOL_proteomics = OOL_proteomics.merge(patient_indices[['maternal_ID_ts','array_index']], how='left', left_on='sample_ID', right_on='maternal_ID_ts').drop(['sample_ID','maternal_ID_ts','DOS'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "\n",
    "        train_indices = patient_indices[patient_indices['maternal_ID'].isin(train_IDs)]['array_index'].values\n",
    "        np.random.shuffle(train_indices)\n",
    "        test_indices = patient_indices[patient_indices['maternal_ID'].isin(test_IDs)]['array_index'].values\n",
    "        val_indices = patient_indices[patient_indices['maternal_ID'].isin(val_IDs)]['array_index'].values\n",
    "\n",
    "        # Iterate through the hyperparameter combinations\n",
    "        for param_set in tqdm(all_params):\n",
    "            bs = param_set['batch_size']\n",
    "            lr = param_set['lr']\n",
    "            dropout = best_dropout\n",
    "            lr_decay = param_set['lr_decay']\n",
    "            layers = best_num_layers\n",
    "            hidden_dim = best_hidden_dim\n",
    "            print(param_set)\n",
    "            \n",
    "            val_r, val_loss, val_rmse = run_experiment(RNN_data_codes, input_OOL_proteomics,\n",
    "                patient_indices, RNN_data_outcomes, RNN_data_lengths, 'EHR_OOL_PT_noHCE_with_obs', \n",
    "                lr, lr_decay, bs, train_indices=train_indices, test_indices=test_indices,\n",
    "                val_indices=val_indices, feature_types='EHR',\n",
    "                model_path='./models/predictive_models/{}.pth'.format(best_model_name),\n",
    "                fine_tune=True, seed=42, hidden_dim=hidden_dim, num_layers=layers, dropout=dropout,\n",
    "                                                       hyperparam_tuning=True)\n",
    "            \n",
    "            num_layers_arr.append(layers)\n",
    "            dropout_arr.append(dropout)\n",
    "            lr_arr.append(lr)\n",
    "            lr_decay_arr.append(lr_decay)\n",
    "            hidden_dim_arr.append(hidden_dim)\n",
    "            batch_size_arr.append(bs)\n",
    "            split_num_arr.append(split_num)\n",
    "            loss_arr.append(val_loss)\n",
    "\n",
    "        print('experiment 5')\n",
    "        print('outcome {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c984a2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_df = pd.DataFrame([num_layers_arr, dropout_arr, lr_arr, lr_decay_arr, hidden_dim_arr, batch_size_arr, split_num_arr,loss_arr]).T\n",
    "hyperparam_df.columns = ['num_layers','dropout','lr','lr_decay','hidden_dim','bs','split_num','val_loss']\n",
    "hyperparam_df = hyperparam_df.groupby(['num_layers','dropout','lr','lr_decay','hidden_dim','bs']).mean()\n",
    "num_layers, dropout, lr, lr_decay, hidden_dim, bs = hyperparam_df['val_loss'].idxmin()\n",
    "print(np.min(hyperparam_df['val_loss']))\n",
    "print(num_layers, dropout, lr, lr_decay, hidden_dim, bs)\n",
    "\n",
    "overall_best_params['DOS']['exp5'] = {'num_layers': int(num_layers),'lr': lr,'lr_decay': lr_decay, 'dropout': dropout,\n",
    "                                'hidden_dim': int(hidden_dim), 'batch_size': int(bs)}\n",
    "with open(\"./models/hyperparameters/best_hyperparams_OOL_noHCE_with_obs.pkl\", \"wb\") as f:\n",
    "        pickle.dump(overall_best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce375419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid search for full COMET framework\n",
    "grid_search = {'batch_size': [16],\n",
    "              'lr': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "              'lr_decay': [1e-1, 1e-2, 1e-3, 1e-4]}\n",
    "\n",
    "all_params = [dict(zip(grid_search.keys(), values)) for values in product(*grid_search.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c230823",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparam search for full COMET framework\n",
    "\n",
    "num_layers_arr = []\n",
    "dropout_arr = []\n",
    "lr_arr = []\n",
    "lr_decay_arr = []\n",
    "hidden_dim_arr = []\n",
    "batch_size_arr = []\n",
    "split_num_arr = []\n",
    "loss_arr = []\n",
    "for i in tqdm(outcome_list):\n",
    "    overall_best_params[i]['exp6'] = {}\n",
    "\n",
    "    maternal_IDs = patient_indices['sample_ID'].str[0:7].unique()\n",
    "    \n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    split_num = 0\n",
    "    for train_index, test_index in kf.split(maternal_IDs):\n",
    "        split_num += 1\n",
    "        results_dict = {}\n",
    "        train_IDs = maternal_IDs[train_index]\n",
    "        test_IDs = maternal_IDs[test_index]\n",
    "        \n",
    "        sample_size = int(0.2 * len(train_index))\n",
    "        random_indices = np.random.choice(train_IDs.shape[0], sample_size, replace=False)\n",
    "        val_IDs = train_IDs[random_indices]\n",
    "        train_IDs = np.delete(train_IDs, random_indices)\n",
    "        \n",
    "        patient_indices['maternal_ID'] = patient_indices['sample_ID'].str[0:7]\n",
    "        patient_indices['maternal_ID_ts'] = patient_indices['maternal_ID'].astype(str)+'_'+patient_indices['sample_ID'].str[-2:]\n",
    "        input_OOL_proteomics = OOL_proteomics.merge(patient_indices[['maternal_ID_ts','array_index']], how='left', left_on='sample_ID', right_on='maternal_ID_ts').drop(['sample_ID','maternal_ID_ts','DOS'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "\n",
    "        train_indices = patient_indices[patient_indices['maternal_ID'].isin(train_IDs)]['array_index'].values\n",
    "        np.random.shuffle(train_indices)\n",
    "        test_indices = patient_indices[patient_indices['maternal_ID'].isin(test_IDs)]['array_index'].values\n",
    "        val_indices = patient_indices[patient_indices['maternal_ID'].isin(val_IDs)]['array_index'].values\n",
    "\n",
    "        # Iterate through the hyperparameter combinations\n",
    "        for param_set in tqdm(all_params):\n",
    "            bs = param_set['batch_size']\n",
    "            lr = param_set['lr']\n",
    "            dropout = best_dropout\n",
    "            lr_decay = param_set['lr_decay']\n",
    "            layers = best_num_layers\n",
    "            hidden_dim = best_hidden_dim\n",
    "            print(param_set)\n",
    "            val_r, val_loss, val_rmse = run_experiment(RNN_data_codes, input_OOL_proteomics,\n",
    "                patient_indices, RNN_data_outcomes, RNN_data_lengths, 'both_PT_noHCE_with_obs', \n",
    "                lr, lr_decay, bs, train_indices=train_indices, test_indices=test_indices,\n",
    "                val_indices=val_indices,feature_types='both',\n",
    "                model_path='./models/predictive_models/{}.pth'.format(best_model_name),\n",
    "                fine_tune=True, seed=42, hidden_dim=hidden_dim, num_layers=layers, dropout=dropout,\n",
    "                                                        hyperparam_tuning=True)\n",
    "            \n",
    "            num_layers_arr.append(layers)\n",
    "            dropout_arr.append(dropout)\n",
    "            lr_arr.append(lr)\n",
    "            lr_decay_arr.append(lr_decay)\n",
    "            hidden_dim_arr.append(hidden_dim)\n",
    "            batch_size_arr.append(bs)\n",
    "            split_num_arr.append(split_num)\n",
    "            loss_arr.append(val_loss)\n",
    "\n",
    "        print('experiment 6')\n",
    "        print('outcome {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060d42b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_df = pd.DataFrame([num_layers_arr, dropout_arr, lr_arr, lr_decay_arr, hidden_dim_arr, batch_size_arr, split_num_arr,loss_arr]).T\n",
    "hyperparam_df.columns = ['num_layers','dropout','lr','lr_decay','hidden_dim','bs','split_num','val_loss']\n",
    "hyperparam_df = hyperparam_df.groupby(['num_layers','dropout','lr','lr_decay','hidden_dim','bs']).mean()\n",
    "num_layers, dropout, lr, lr_decay, hidden_dim, bs = hyperparam_df['val_loss'].idxmin()\n",
    "\n",
    "print(np.min(hyperparam_df['val_loss']))\n",
    "print(num_layers, dropout, lr, lr_decay, hidden_dim, bs)\n",
    "\n",
    "overall_best_params['DOS']['exp6'] = {'num_layers': int(num_layers),'lr': lr,'lr_decay': lr_decay, 'dropout': dropout,\n",
    "                                'hidden_dim': int(hidden_dim), 'batch_size': int(bs)}\n",
    "with open(\"./models/hyperparameters/best_hyperparams_OOL_noHCE_with_obs.pkl\", \"wb\") as f:\n",
    "        pickle.dump(overall_best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be0372d",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_best_params = pickle.load(open('./models/hyperparameters/best_hyperparams_OOL_noHCE_with_obs.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9341c597",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_num_layers = overall_best_params['DOS']['PT']['num_layers']\n",
    "best_dropout = overall_best_params['DOS']['PT']['dropout']\n",
    "best_model_name = overall_best_params['DOS']['PT']['model_name']\n",
    "best_hidden_dim = overall_best_params['DOS']['PT']['hidden_dim']\n",
    "best_num_layers, best_dropout, best_hidden_dim, best_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f3b405",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_num_layers_OOL = overall_best_params['DOS']['exp1']['num_layers']\n",
    "best_dropout_OOL = overall_best_params['DOS']['exp1']['dropout']\n",
    "best_hidden_dim_OOL = overall_best_params['DOS']['exp1']['hidden_dim']\n",
    "best_num_layers_OOL, best_dropout_OOL, best_hidden_dim_OOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff2cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 1 = baseline model EHR features\n",
    "#experiment 2 = baseline model metab features\n",
    "#experiment 3 = baseline model all features\n",
    "#experiment 4 = only pretrained model\n",
    "#experiment 5 = fine tune pretrained model \n",
    "#experiment 6 = add proteomics, fine tune pretrained model w/ frozen GRU weights\n",
    "\n",
    "num_iterations = 25\n",
    "results = {}\n",
    "for i in tqdm(outcome_list):\n",
    "    results[i] = {'exp1':[],'exp2':[],'exp3':[],'exp4':[],'exp5':[],'exp6':[]}\n",
    "    for j in tqdm(range(num_iterations)):\n",
    "        print('experiment 1')\n",
    "        val_auc = run_experiment(RNN_data_codes_OOL, OOL_proteomics,\n",
    "                patient_indices_OOL, RNN_data_outcomes_OOL, RNN_data_lengths_OOL, 'EHR_OOL_only_woHCE_wobs_fixed_{}'.format(j), \n",
    "                overall_best_params[i]['exp1']['lr'], overall_best_params[i]['exp1']['lr_decay'],\n",
    "                overall_best_params[i]['exp1']['batch_size'], feature_types='EHR', model_path='', fine_tune=False, seed=j,\n",
    "                                hidden_dim=overall_best_params[i]['exp1']['hidden_dim'],\n",
    "                                 num_layers=overall_best_params[i]['exp1']['num_layers'],\n",
    "                                 dropout=overall_best_params[i]['exp1']['dropout'], return_preds=True)\n",
    "        results[i]['exp1'].append(val_auc)\n",
    "        \n",
    "        print('experiment 2')\n",
    "        val_auc = run_experiment(RNN_data_codes_OOL, OOL_proteomics,\n",
    "                patient_indices_OOL, RNN_data_outcomes_OOL, RNN_data_lengths_OOL, 'proteomics_OOL_only_woHCE_wobs_fixed_{}'.format(j), \n",
    "                overall_best_params[i]['exp2']['lr'], overall_best_params[i]['exp2']['lr_decay'],\n",
    "                overall_best_params[i]['exp2']['batch_size'], feature_types='metab', model_path='', fine_tune=False, seed=j,\n",
    "                                hidden_dim=best_hidden_dim,num_layers=best_num_layers, dropout=best_dropout, return_preds=True)\n",
    "        results[i]['exp2'].append(val_auc)\n",
    "        \n",
    "        print('experiment 3')\n",
    "        val_auc = run_experiment(RNN_data_codes_OOL, OOL_proteomics,\n",
    "                patient_indices_OOL, RNN_data_outcomes_OOL, RNN_data_lengths_OOL, 'both_OOL_only_woHCE_wobs_fixed_{}'.format(j), \n",
    "                overall_best_params[i]['exp3']['lr'], overall_best_params[i]['exp3']['lr_decay'],\n",
    "                overall_best_params[i]['exp3']['batch_size'], feature_types='both', model_path='', fine_tune=False, seed=j,\n",
    "                                hidden_dim=overall_best_params[i]['exp3']['hidden_dim'],\n",
    "                                 num_layers=overall_best_params[i]['exp3']['num_layers'],\n",
    "                                 dropout=overall_best_params[i]['exp3']['dropout'],\n",
    "                                 return_preds=True, return_interpretability=True, return_grads=True)\n",
    "        results[i]['exp3'].append(val_auc)\n",
    "        \n",
    "        print('experiment 4')\n",
    "        val_auc = run_experiment(RNN_data_codes, OOL_proteomics,\n",
    "                patient_indices, RNN_data_outcomes, RNN_data_lengths, 'EHR_OOL_PT_woHCE_wobs_fixed_{}'.format(j), \n",
    "                overall_best_params[i]['exp4']['lr'], overall_best_params[i]['exp4']['lr_decay'],\n",
    "                overall_best_params[i]['exp4']['batch_size'], feature_types='EHR',\n",
    "                                 model_path='./models/predictive_models/{}.pth'.format(best_model_name),\n",
    "                                 fine_tune=False, seed=j,\n",
    "                                hidden_dim=best_hidden_dim,num_layers=best_num_layers,\n",
    "                                 dropout=best_dropout, return_preds=True)\n",
    "        results[i]['exp4'].append(val_auc)\n",
    "\n",
    "        print('experiment 5')\n",
    "        val_auc = run_experiment(RNN_data_codes, OOL_proteomics,\n",
    "                patient_indices, RNN_data_outcomes, RNN_data_lengths, 'EHR_OOL_PT_FT_woHCE_wobs_fixed_{}'.format(j), \n",
    "                overall_best_params[i]['exp5']['lr'], overall_best_params[i]['exp5']['lr_decay'],\n",
    "                overall_best_params[i]['exp5']['batch_size'], feature_types='EHR',\n",
    "                                 model_path='./models/predictive_models/{}.pth'.format(best_model_name),\n",
    "                                 fine_tune=True, seed=j,\n",
    "                                hidden_dim=best_hidden_dim,num_layers=best_num_layers,\n",
    "                                 dropout=best_dropout, return_preds=True)\n",
    "        results[i]['exp5'].append(val_auc)\n",
    "\n",
    "        print('experiment 6')\n",
    "        val_auc = run_experiment(RNN_data_codes, OOL_proteomics,\n",
    "                patient_indices, RNN_data_outcomes, RNN_data_lengths, 'both_OOL_PT_FT_woHCE_wobs_fixed_{}'.format(j), \n",
    "                overall_best_params[i]['exp6']['lr'], overall_best_params[i]['exp6']['lr_decay'],\n",
    "                overall_best_params[i]['exp6']['batch_size'], feature_types='both',\n",
    "                                model_path='./models/predictive_models/{}.pth'.format(best_model_name),\n",
    "                                 fine_tune=True, seed=j,\n",
    "                                hidden_dim=best_hidden_dim,num_layers=best_num_layers,\n",
    "                                 dropout=best_dropout,\n",
    "                                 return_preds=True, return_interpretability=True, return_grads=True)\n",
    "        results[i]['exp6'].append(val_auc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4000a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute performance metrics from predictions in validation set\n",
    "for exp in results['DOS'].keys():\n",
    "    true_outcomes = []\n",
    "    total_preds = []\n",
    "    indices = []\n",
    "    for i in results['DOS'][exp]:\n",
    "        true_outcomes.extend(i[3])\n",
    "        total_preds.extend(i[4])\n",
    "        indices.extend(i[5])\n",
    "    \n",
    "        df = pd.DataFrame([true_outcomes,total_preds,indices]).T\n",
    "        df.columns = ['true_outcome','pred','index']\n",
    "        df = df.groupby('index').mean()\n",
    "    print(exp)\n",
    "    print(pearsonr(df['true_outcome'], df['pred'])[0])\n",
    "    print(np.sqrt(np.sum((df['true_outcome']-df['pred'])**2)/df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fb2f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE computation for exp4\n",
    "true = []\n",
    "pred = []\n",
    "index = []\n",
    "for i in range(0, NUM_TRIALS):\n",
    "    true.extend(list(np.array(results['DOS']['exp4'][i][3])*results['DOS']['exp3'][i][8]+results['DOS']['exp3'][i][7]))\n",
    "    pred.extend(list(np.array(results['DOS']['exp4'][i][4])*results['DOS']['exp3'][i][8]+results['DOS']['exp3'][i][7]))\n",
    "    index.extend(results['DOS']['exp4'][i][5])\n",
    "    \n",
    "df = pd.DataFrame([true,pred,index]).T\n",
    "df.columns = ['true','pred','index']\n",
    "df = df.groupby('index').mean()\n",
    "np.sqrt(np.mean((df['true']-df['pred'])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dc5146",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./results/OOL_results_noHCE_with_obs.pkl\",\"wb\")\n",
    "\n",
    "# write the python object (dict) to pickle file\n",
    "pickle.dump(results,f)\n",
    "\n",
    "# close file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139e59dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pickle.load(open('./results/OOL_results_noHCE_with_obs.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65758722",
   "metadata": {},
   "source": [
    "## Feature Importance Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fbfa25",
   "metadata": {},
   "source": [
    "### Compute Latent Representation of EHR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daecbe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_data_codes_PTMODEL.shape, RNN_data_outcomes_PTMODEL.shape, RNN_data_lengths_PTMODEL.shape, patient_indices_PTMODEL.shape\n",
    "\n",
    "\n",
    "all_EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes_PTMODEL]\n",
    "all_EHR_codes = [torch.nan_to_num(x) for x in all_EHR_codes]\n",
    "all_outcomes = torch.tensor(RNN_data_outcomes_PTMODEL).float()\n",
    "\n",
    "all_loader_codes = create_dataloaders(all_EHR_codes, all_outcomes, RNN_data_lengths_PTMODEL, 100000)   \n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = GRUNet(RNN_data_codes_PTMODEL.shape[2], best_hidden_dim, [best_hidden_dim], best_num_layers, 1, best_dropout).to(device)\n",
    "\n",
    "model_state_dict = torch.load('./models/predictive_models/{}.pth'.format(best_model_name))\n",
    "model.load_state_dict(model_state_dict)\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "criterion = nn.MSELoss()\n",
    "val_predictions = []\n",
    "val_true_labels = []\n",
    "running_loss_val, num_samples_val = 0, 0\n",
    "with torch.no_grad():\n",
    "    for (inputs_codes, labels_codes, lengths_codes) in (all_loader_codes):\n",
    "            inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "            outputs,interpretability_outputs = model(inputs_codes, lengths_codes, interpretability=True)\n",
    "\n",
    "            mean_tensor = torch.tensor(np.mean(RNN_data_outcomes_PTMODEL), dtype=torch.float32, device=device)\n",
    "            std_tensor = torch.tensor(np.std(RNN_data_outcomes_PTMODEL), dtype=torch.float32, device=device)\n",
    "\n",
    "            denormalized_outputs = outputs.squeeze() * std_tensor + mean_tensor\n",
    "            denormalized_labels = labels * std_tensor + mean_tensor\n",
    "\n",
    "            loss = criterion(denormalized_outputs, denormalized_labels)\n",
    "            running_loss_val += (loss.item()*RNN_data_lengths_PTMODEL.shape[0])\n",
    "            num_samples_val += RNN_data_lengths_PTMODEL.shape[0]\n",
    "            \n",
    "            val_predictions.extend(outputs.squeeze().tolist())\n",
    "            val_true_labels.extend(labels.tolist())\n",
    "\n",
    "val_loss = running_loss_val / (num_samples_val)\n",
    "pearson_corr, _ = pearsonr(val_predictions, val_true_labels)\n",
    "val_outcome_mean, val_outcome_sd = np.mean(RNN_data_outcomes_PTMODEL), np.std(RNN_data_outcomes_PTMODEL)\n",
    "val_rmse = np.sqrt(mean_squared_error(RNN_data_outcomes_PTMODEL, np.array(val_predictions)*val_outcome_sd+val_outcome_mean))\n",
    "\n",
    "\n",
    "output = pearson_corr, val_loss, val_rmse, RNN_data_outcomes_PTMODEL, np.array(val_predictions)*val_outcome_sd+val_outcome_mean, patient_indices_PTMODEL, interpretability_outputs, val_outcome_mean, val_outcome_sd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccb9555",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(output, open('./results/full_cohort_latent_noHCE.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f63f8c",
   "metadata": {},
   "source": [
    "### Feature Importance: Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f63c402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf86b14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import DeepLift\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "#If all_data is false, we can use index 5 to compute integrated gradients on only val data, 13 for only train data\n",
    "#If all_data is true, we use train, test, and val data to compute integrated gradients\n",
    "imp_index = 5\n",
    "all_data = True\n",
    "\n",
    "\n",
    "# Path to models\n",
    "model_paths = [f'./models/predictive_models/both_OOL_PT_FT_woHCE_wobs_fixed_{i}.pth' for i in range(25)]\n",
    "\n",
    "# Initialize array to store feature importance\n",
    "importance_proteomics_all = torch.zeros((len(model_paths), OOL_proteomics.shape[1]-2))\n",
    "\n",
    "class ModelWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, proteomics, codes, lengths):\n",
    "        return self.model(codes, proteomics, lengths)\n",
    "\n",
    "for model_idx, model_path in tqdm(enumerate(model_paths)):\n",
    "    \n",
    "    \n",
    "    model = joint_model(RNN_data_codes.shape[2], best_hidden_dim, [best_hidden_dim], best_num_layers, 1,\n",
    "                    OOL_proteomics.shape[1]-2, [], [], best_dropout).to(device)\n",
    "    \n",
    "    proteomics = OOL_proteomics.merge(patient_indices[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID','DOS'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "    if all_data == False:\n",
    "        val_proteomics = proteomics[results['DOS']['exp6'][model_idx][imp_index],:]\n",
    "    else:\n",
    "        val_proteomics = proteomics\n",
    "    scaler = StandardScaler()\n",
    "    val_proteomics = scaler.fit_transform(val_proteomics)\n",
    "\n",
    "    if all_data == False:\n",
    "        EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes[results['DOS']['exp6'][model_idx][imp_index],:,:]]  \n",
    "        EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "        val_proteomics = torch.tensor(val_proteomics).float()\n",
    "        val_proteomics = torch.nan_to_num(val_proteomics)\n",
    "        val_outcomes = torch.tensor(RNN_data_outcomes[results['DOS']['exp6'][model_idx][imp_index]]).float()\n",
    "\n",
    "        outcome_mean = torch.mean(val_outcomes)\n",
    "        outcome_sd = torch.std(val_outcomes)\n",
    "\n",
    "        data_set = DataBuilder(val_proteomics, val_outcomes, scaler)\n",
    "        loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "        loader_codes = create_dataloaders(EHR_codes, val_outcomes, RNN_data_lengths[results['DOS']['exp6'][model_idx][imp_index]], 100000)\n",
    "    else:\n",
    "        EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes]  \n",
    "        EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "        val_proteomics = torch.tensor(val_proteomics).float()\n",
    "        val_proteomics = torch.nan_to_num(val_proteomics)\n",
    "        val_outcomes = torch.tensor(RNN_data_outcomes).float()\n",
    "\n",
    "        outcome_mean = torch.mean(val_outcomes)\n",
    "        outcome_sd = torch.std(val_outcomes)\n",
    "\n",
    "        data_set = DataBuilder(val_proteomics, val_outcomes, scaler)\n",
    "        loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "        loader_codes = create_dataloaders(EHR_codes, val_outcomes, RNN_data_lengths, 100000)\n",
    "\n",
    "    model_state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    model_wrapper = ModelWrapper(model)\n",
    "    ig = IntegratedGradients(model_wrapper)\n",
    "    # Compute the feature importance for this model\n",
    "    for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in zip(loader_codes, loader_proteomics):\n",
    "        inputs_codes, labels_codes = inputs_codes.to(device), labels_codes.to(device)\n",
    "        inputs_proteomics, labels_proteomics = inputs_proteomics.to(device), labels_proteomics.to(device)\n",
    "        \n",
    "        def forward_func(proteomics, codes, lengths):\n",
    "            return model(codes, proteomics, lengths)\n",
    "        \n",
    "        # Compute feature importances using a custom forward function\n",
    "        importance_proteomics = ig.attribute(inputs_proteomics, additional_forward_args=(inputs_codes, lengths_codes))\n",
    "        # Store the feature importance for this model\n",
    "        importance_proteomics_all[model_idx] = importance_proteomics.mean(dim=0).cpu().detach()\n",
    "    \n",
    "# Compute the average feature importance across all 25 models\n",
    "importance_proteomics_avg = torch.mean((importance_proteomics_all), dim=0)*1e11\n",
    "\n",
    "# Create dataframe for feature importances\n",
    "importance_df = pd.DataFrame([importance_proteomics_avg.numpy(), OOL_proteomics.drop(['DOS','sample_ID'],axis=1).columns]).T\n",
    "importance_df.columns = ['importance_PT','name']\n",
    "importance_df.sort_values('importance_PT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a135c223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import DeepLift\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "# Path to models\n",
    "model_paths = [f'./models/predictive_models/both_OOL_only_woHCE_wobs_fixed_{i}.pth' for i in range(25)]\n",
    "\n",
    "# Initialize array to store feature importance\n",
    "importance_proteomics_all = torch.zeros((len(model_paths), OOL_proteomics.shape[1]-2))\n",
    "\n",
    "class ModelWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, proteomics, codes, lengths):\n",
    "        return self.model(codes, proteomics, lengths)\n",
    "\n",
    "# Iterate over all models\n",
    "for model_idx, model_path in tqdm(enumerate(model_paths)):\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    model = joint_model(RNN_data_codes_OOL.shape[2], overall_best_params['DOS']['exp3']['hidden_dim'], [overall_best_params['DOS']['exp3']['hidden_dim']],\n",
    "                        overall_best_params['DOS']['exp3']['num_layers'], 1,\n",
    "                        OOL_proteomics.shape[1]-2, [], [], overall_best_params['DOS']['exp3']['dropout']).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    if all_data == False:\n",
    "        val_proteomics = proteomics[results['DOS']['exp3'][model_idx][imp_index],:]\n",
    "    else:\n",
    "        val_proteomics = proteomics\n",
    "    scaler = StandardScaler()\n",
    "    val_proteomics = scaler.fit_transform(val_proteomics)\n",
    "\n",
    "    if all_data == False:\n",
    "        EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes_OOL[results['DOS']['exp3'][model_idx][imp_index],:,:]]  \n",
    "        EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "        val_proteomics = torch.tensor(val_proteomics).float()\n",
    "        val_proteomics = torch.nan_to_num(val_proteomics)\n",
    "        val_outcomes = torch.tensor(RNN_data_outcomes_OOL[results['DOS']['exp3'][model_idx][imp_index]]).float()\n",
    "\n",
    "        outcome_mean = torch.mean(val_outcomes)\n",
    "        outcome_sd = torch.std(val_outcomes)\n",
    "\n",
    "        data_set = DataBuilder(val_proteomics, val_outcomes, scaler)\n",
    "        loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "        loader_codes = create_dataloaders(EHR_codes, val_outcomes, RNN_data_lengths_OOL[results['DOS']['exp3'][model_idx][imp_index]], 100000)\n",
    "    if all_data == True:\n",
    "        EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes_OOL]  \n",
    "        EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "        val_proteomics = torch.tensor(val_proteomics).float()\n",
    "        val_proteomics = torch.nan_to_num(val_proteomics)\n",
    "        val_outcomes = torch.tensor(RNN_data_outcomes_OOL).float()\n",
    "\n",
    "        outcome_mean = torch.mean(val_outcomes)\n",
    "        outcome_sd = torch.std(val_outcomes)\n",
    "\n",
    "        data_set = DataBuilder(val_proteomics, val_outcomes, scaler)\n",
    "        loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "        loader_codes = create_dataloaders(EHR_codes, val_outcomes, RNN_data_lengths_OOL, 100000)\n",
    "\n",
    "    # Load the model\n",
    "    model_state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "    model_wrapper = ModelWrapper(model)\n",
    "    ig = IntegratedGradients(model_wrapper)\n",
    "    # Compute the feature importance for this model\n",
    "    for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in zip(loader_codes, loader_proteomics):\n",
    "        inputs_codes, labels_codes = inputs_codes.to(device), labels_codes.to(device)\n",
    "        inputs_proteomics, labels_proteomics = inputs_proteomics.to(device), labels_proteomics.to(device)\n",
    "        \n",
    "        def forward_func(proteomics, codes, lengths):\n",
    "            return model(codes, proteomics, lengths)\n",
    "        \n",
    "        # Compute feature importances using a custom forward function\n",
    "        importance_proteomics = ig.attribute(inputs_proteomics, additional_forward_args=(inputs_codes, lengths_codes))\n",
    "        # Store the feature importance for this model\n",
    "        importance_proteomics_all[model_idx] = importance_proteomics.mean(dim=0).cpu().detach()\n",
    "    \n",
    "# Compute the average feature importance across all models\n",
    "importance_proteomics_avg = torch.mean((importance_proteomics_all), dim=0)*1e11\n",
    "\n",
    "# Create dataframe for feature importances\n",
    "importance_df_OOL = pd.DataFrame([importance_proteomics_avg.numpy(), OOL_proteomics.drop(['DOS','sample_ID'],axis=1).columns]).T\n",
    "importance_df_OOL.columns = ['importance_OOL','name']\n",
    "importance_df_OOL.sort_values('importance_OOL').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b916f9b1",
   "metadata": {},
   "source": [
    "## Plot function trajectories by input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e8b5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final structure is [PT_model1_epoch1, PT_mode1_epoch2,...PT_model2_epoch1....NPT_model1_epoch1...] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f91b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = joint_model(RNN_data_codes.shape[2], best_hidden_dim, [best_hidden_dim], best_num_layers, 1,\n",
    "                    OOL_proteomics.shape[1]-2, [], [], best_dropout).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "proteomics = OOL_proteomics.merge(patient_indices[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID','DOS'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "proteomics = scaler.fit_transform(proteomics)\n",
    "\n",
    "EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes]  \n",
    "EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "proteomics = torch.tensor(proteomics).float()\n",
    "proteomics = torch.nan_to_num(proteomics)\n",
    "outcomes = torch.tensor(RNN_data_outcomes).float()\n",
    "\n",
    "\n",
    "data_set = DataBuilder(proteomics, outcomes, scaler)\n",
    "loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "loader_codes = create_dataloaders(EHR_codes, outcomes, RNN_data_lengths, 100000)\n",
    "            \n",
    "    \n",
    "model_paths = [f'./models/predictive_models/both_OOL_PT_FT_woHCE_wobs_fixed_{i}_epoch{j}.pth' for i in range(25) for j in range(200)]\n",
    "model_number = 0\n",
    "\n",
    "all_outputs = []\n",
    "model_numbers = []\n",
    "epochs = []\n",
    "PT = []\n",
    "total_losses = []\n",
    "\n",
    "for i in tqdm(model_paths):\n",
    "    result = re.search('both_OOL_PT_FT_woHCE_wobs_fixed_(\\d+)_epoch(\\d+).pth', i)\n",
    "    model_number = int(result.group(1))\n",
    "    epoch_number = int(result.group(2))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            model_state_dict = torch.load(i)\n",
    "            model.load_state_dict(model_state_dict)\n",
    "            for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(loader_codes, loader_proteomics)):\n",
    "                inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                inputs_proteomics = inputs_proteomics.to(device)\n",
    "                outputs = model(inputs_codes, inputs_proteomics, lengths_codes)\n",
    "                total_losses.append(criterion(outputs.squeeze(), labels).cpu().item())\n",
    "            all_outputs.append(outputs.squeeze().cpu().numpy())\n",
    "            model_numbers.append(model_number)\n",
    "            epochs.append(epoch_number)\n",
    "            PT.append('PT')\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "model = joint_model(RNN_data_codes.shape[2], best_hidden_dim_OOL, [best_hidden_dim_OOL], best_num_layers_OOL, 1,\n",
    "                    OOL_proteomics.shape[1]-2, [], [], best_dropout_OOL).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes_OOL]  \n",
    "EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "proteomics = torch.tensor(proteomics).float()\n",
    "proteomics = torch.nan_to_num(proteomics)\n",
    "outcomes = torch.tensor(RNN_data_outcomes_OOL).float()\n",
    "\n",
    "\n",
    "data_set = DataBuilder(proteomics, outcomes, scaler)\n",
    "loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "loader_codes = create_dataloaders(EHR_codes, outcomes, RNN_data_lengths_OOL, 100000)\n",
    "        \n",
    "model_paths = [f'./models/predictive_models/both_OOL_only_woHCE_wobs_fixed_{i}_epoch{j}.pth' for i in range(25) for j in range(200)]\n",
    "model_number = 0\n",
    "\n",
    "for i in tqdm(model_paths):\n",
    "    result = re.search('both_OOL_only_woHCE_wobs_fixed_(\\d+)_epoch(\\d+).pth', i)\n",
    "    model_number = int(result.group(1))\n",
    "    epoch_number = int(result.group(2))\n",
    "    if True:\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                model_state_dict = torch.load(i)\n",
    "                model.load_state_dict(model_state_dict)\n",
    "                for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(loader_codes, loader_proteomics)):\n",
    "                    inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                    inputs_proteomics = inputs_proteomics.to(device)\n",
    "                    outputs = model(inputs_codes, inputs_proteomics, lengths_codes)\n",
    "                    total_losses.append(criterion(outputs.squeeze(), labels).cpu().item())\n",
    "                all_outputs.append(outputs.squeeze().cpu().numpy())\n",
    "                model_numbers.append(model_number)\n",
    "                epochs.append(epoch_number)\n",
    "                PT.append('NPT')\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e586fdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(all_outputs).shape, len(model_numbers), len(epochs), len(PT), len(total_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c47930",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.array(epochs)\n",
    "model_numbers = np.array(model_numbers)\n",
    "all_outputs = np.array(all_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de8aeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom lines for the legend\n",
    "line1 = Line2D([0], [0], color='none', marker='o', markersize=10, markerfacecolor='darkred', label='PT')\n",
    "line2 = Line2D([0], [0], color='none', marker='x', markersize=10, markerfacecolor='darkblue', label='NPT')\n",
    "\n",
    "# Create a t-SNE instance and fit_transform the data\n",
    "tsne = TSNE(n_components=2, random_state=42, init='pca', learning_rate='auto')\n",
    "embedding = tsne.fit_transform(np.array(all_outputs))\n",
    "\n",
    "# Create a markers array based on PT array\n",
    "marker_map = {'PT': 'o', 'NPT': 'x'}\n",
    "markers = [marker_map[pt] for pt in PT]\n",
    "\n",
    "# Don't normalize the epochs\n",
    "colors = epochs\n",
    "\n",
    "# Create colormaps that goes from light red to dark red for PT, light blue to dark blue for NPT\n",
    "cmap_pt = LinearSegmentedColormap.from_list(\"mycmap_pt\", [\"lightcoral\", \"darkred\"])\n",
    "cmap_npt = LinearSegmentedColormap.from_list(\"mycmap_npt\", [\"lightblue\", \"darkblue\"])\n",
    "\n",
    "# Plot the results with different markers and colors\n",
    "for marker_type, marker in marker_map.items():\n",
    "    mask = np.array(markers) == marker\n",
    "    cmap = cmap_pt if marker_type == 'PT' else cmap_npt\n",
    "    sc = plt.scatter(embedding[mask, 0], embedding[mask, 1], marker=marker, c=colors[mask], cmap=cmap, alpha = 0.7)\n",
    "\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.gca().set_xticks([])\n",
    "plt.gca().set_yticks([])\n",
    "plt.title('Overall parameter space', fontsize=14)\n",
    "plt.legend(handles=[line1, line2])  # add the custom legend\n",
    "plt.savefig('./overall_params.png', dpi=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1e04aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform t-SNE dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=42, init='pca', learning_rate='auto')\n",
    "embedding = tsne.fit_transform(np.array(all_outputs))\n",
    "\n",
    "# Create colormaps that go from light red to dark red for PT, and light blue to dark blue for NPT\n",
    "cmap_pt = LinearSegmentedColormap.from_list(\"mycmap_pt\", [\"lightcoral\", \"darkred\"])\n",
    "cmap_npt = LinearSegmentedColormap.from_list(\"mycmap_npt\", [\"lightblue\", \"darkblue\"])\n",
    "\n",
    "fig, axs = plt.subplots(5, 5, figsize=(20, 20))\n",
    "\n",
    "# Calculate the global x and y limits\n",
    "xlim = (embedding[:,0].min()-1, embedding[:,0].max()+1)\n",
    "ylim = (embedding[:,1].min()-1, embedding[:,1].max()+1)\n",
    "\n",
    "for model_number in range(25):\n",
    "    ax = axs[model_number // 5, model_number % 5]\n",
    "\n",
    "    for model_type in ['PT', 'NPT']:\n",
    "        # Get mask for the current trajectory\n",
    "        mask = (np.array(model_numbers) == model_number) & (np.array(PT) == model_type)\n",
    "\n",
    "        # Get the points and corresponding epochs for the current trajectory\n",
    "        trajectory_points = embedding[mask]\n",
    "        trajectory_epochs = np.array(epochs)[mask]\n",
    "\n",
    "        # Sort the points and epochs\n",
    "        sort_indices = np.argsort(trajectory_epochs)\n",
    "        sorted_points = trajectory_points[sort_indices]\n",
    "\n",
    "        # Select the colormap based on model_type\n",
    "        cmap = cmap_pt if model_type == 'PT' else cmap_npt\n",
    "\n",
    "        # Plot the points with color indicating epoch and marker indicating PT/NPT\n",
    "        ax.scatter(sorted_points[:, 0], sorted_points[:, 1], \n",
    "                   c=trajectory_epochs[sort_indices], \n",
    "                   cmap=cmap, \n",
    "                   marker=marker_map[model_type], \n",
    "                   alpha=0.6)\n",
    "\n",
    "        # Plot lines connecting the points of the same model type\n",
    "        ax.plot(sorted_points[:, 0], sorted_points[:, 1], \n",
    "                color='lightgrey' if model_type == 'PT' else 'black', \n",
    "                linestyle='--')\n",
    "\n",
    "    ax.set_aspect('auto')\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "    # Turn off tick labels\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./overall_paths_ool.png',dpi=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473719e3",
   "metadata": {},
   "source": [
    "## Function Trajectories Based on Latent Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5644206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I want to do this now with the embedding. The embedding will be (embed_dim, num_patients, num_epochs)\n",
    "#A solution is to just concat embed_dim with num_patients, (embed_dim*num_patients, num_epochs*num_models)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "model = joint_model(RNN_data_codes.shape[2], best_hidden_dim, [best_hidden_dim], best_num_layers, 1,\n",
    "                    OOL_proteomics.shape[1]-2, [], [], best_dropout).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "proteomics = OOL_proteomics.merge(patient_indices[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID','DOS'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes]  \n",
    "EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "proteomics = torch.tensor(proteomics).float()\n",
    "proteomics = torch.nan_to_num(proteomics)\n",
    "outcomes = torch.tensor(RNN_data_outcomes).float()\n",
    "\n",
    "\n",
    "data_set = DataBuilder(proteomics, outcomes, scaler)\n",
    "loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "loader_codes = create_dataloaders(EHR_codes, outcomes, RNN_data_lengths, 100000)\n",
    "        \n",
    "\n",
    "model_paths = [f'./models/predictive_models/both_OOL_PT_FT_woHCE_wobs_fixed_{i}_epoch{j}.pth' for i in range(25) for j in range(200)]\n",
    "model_number = 0\n",
    "\n",
    "all_outputs = []\n",
    "model_numbers = []\n",
    "epochs = []\n",
    "PT = []\n",
    "\n",
    "for i in tqdm(model_paths):\n",
    "    result = re.search('both_OOL_PT_FT_woHCE_wobs_fixed_(\\d+)_epoch(\\d+).pth', i)\n",
    "    model_number = int(result.group(1))\n",
    "    epoch_number = int(result.group(2))\n",
    "    if True:\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                model_state_dict = torch.load(i)\n",
    "                model.load_state_dict(model_state_dict)\n",
    "                for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(loader_codes, loader_proteomics)):\n",
    "                    inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                    inputs_proteomics = inputs_proteomics.to(device)\n",
    "                    outputs = model(inputs_codes, inputs_proteomics, lengths_codes, True)\n",
    "                latent_rep = outputs[1][0].flatten()\n",
    "                all_outputs.append(latent_rep.cpu().numpy())\n",
    "                model_numbers.append(model_number)\n",
    "                epochs.append(epoch_number)\n",
    "                PT.append('PT')\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "\n",
    "\n",
    "model = joint_model(RNN_data_codes_OOL.shape[2], best_hidden_dim_OOL, [best_hidden_dim_OOL], best_num_layers_OOL, 1,\n",
    "                    OOL_proteomics.shape[1]-2, [], [], best_dropout_OOL).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "proteomics = OOL_proteomics.merge(patient_indices_OOL[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID','DOS'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes_OOL]  \n",
    "EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "proteomics = torch.tensor(proteomics).float()\n",
    "proteomics = torch.nan_to_num(proteomics)\n",
    "outcomes = torch.tensor(RNN_data_outcomes_OOL).float()\n",
    "\n",
    "\n",
    "data_set = DataBuilder(proteomics, outcomes, scaler)\n",
    "loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "loader_codes = create_dataloaders(EHR_codes, outcomes, RNN_data_lengths_OOL, 100000)\n",
    "            \n",
    " \n",
    "    \n",
    "model_paths = [f'./models/predictive_models/both_OOL_only_woHCE_wobs_fixed_{i}_epoch{j}.pth' for i in range(25) for j in range(200)]\n",
    "model_number = 0\n",
    "\n",
    "\n",
    "for i in tqdm(model_paths):\n",
    "    result = re.search('both_OOL_only_woHCE_wobs_fixed_(\\d+)_epoch(\\d+).pth', i)\n",
    "    model_number = int(result.group(1))\n",
    "    epoch_number = int(result.group(2))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            model_state_dict = torch.load(i)\n",
    "            model.load_state_dict(model_state_dict)\n",
    "            for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(loader_codes, loader_proteomics)):\n",
    "                inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                inputs_proteomics = inputs_proteomics.to(device)\n",
    "                outputs = model(inputs_codes, inputs_proteomics, lengths_codes, True)\n",
    "            latent_rep = outputs[1][0].flatten()\n",
    "            all_outputs.append(latent_rep.cpu().numpy())\n",
    "            model_numbers.append(model_number)\n",
    "            epochs.append(epoch_number)\n",
    "            PT.append('NPT')\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3e349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(all_outputs).shape, len(model_numbers), len(epochs), len(PT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71667e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.array(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75f23d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a t-SNE instance and fit_transform the data\n",
    "tsne = TSNE(n_components=2, random_state=42, init='pca', learning_rate='auto')\n",
    "\n",
    "embedding = tsne.fit_transform(np.array(all_outputs))\n",
    "\n",
    "\n",
    "line1 = Line2D([0], [0], color='none', marker='o', markersize=10, markerfacecolor='darkred', label='PT')\n",
    "line2 = Line2D([0], [0], color='none', marker='x', markersize=10, markerfacecolor='darkblue', label='NPT')\n",
    "\n",
    "# Create a markers array based on PT array\n",
    "marker_map = {'PT': 'o', 'NPT': 'x'}\n",
    "markers = [marker_map[pt] for pt in PT]\n",
    "\n",
    "# Don't normalize the epochs\n",
    "colors = epochs\n",
    "\n",
    "# Create a colormap that goes from light green to dark blue\n",
    "cmap_pt = LinearSegmentedColormap.from_list(\"mycmap_pt\", [\"lightcoral\", \"darkred\"])\n",
    "cmap_npt = LinearSegmentedColormap.from_list(\"mycmap_npt\", [\"lightblue\", \"darkblue\"])\n",
    "\n",
    "# Plot the results with different markers and colors\n",
    "for marker_type, marker in marker_map.items():\n",
    "    mask = np.array(markers) == marker\n",
    "    cmap = cmap_pt if marker_type == 'PT' else cmap_npt\n",
    "    sc = plt.scatter(embedding[mask, 0], embedding[mask, 1], marker=marker, c=colors[mask], cmap=cmap, alpha = 0.7)\n",
    "\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "# plt.colorbar(sc, label='Epochs')\n",
    "plt.legend(handles=[line1, line2])  # add the custom legend\n",
    "plt.gca().set_xticks([])\n",
    "plt.gca().set_yticks([])\n",
    "plt.title('RNN parameter space', fontsize=14)\n",
    "plt.savefig('./RNN_param_space.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234436c9",
   "metadata": {},
   "source": [
    "### Plot losses by epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed230369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "experiments = ['exp3', 'exp6']\n",
    "colors = ['blue', 'orange']\n",
    "mean_colors = ['black', 'red']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "max_epoch = max(max(len(model_data[10]) for model_data in results['DOS'][exp]) for exp in experiments) - 5\n",
    "\n",
    "for experiment, color, mean_color in zip(experiments, colors, mean_colors):\n",
    "    train_losses_all = []\n",
    "    test_losses_all = []\n",
    "    \n",
    "    for model_data in results['DOS'][experiment]:\n",
    "        train_losses = model_data[10][:-5]  # Exclude last 5 epochs\n",
    "        test_losses = model_data[11][:-5]  # Exclude last 5 epochs\n",
    "\n",
    "        train_losses_all.append(train_losses + [np.nan]*(max_epoch-len(train_losses)))\n",
    "        test_losses_all.append(test_losses + [np.nan]*(max_epoch-len(test_losses)))\n",
    "\n",
    "        # Plot each model's losses\n",
    "        plt.plot(train_losses, test_losses, marker='o', linestyle='-', color=color, alpha=0.1)\n",
    "    \n",
    "    # Calculate the mean across each epoch for train and test losses separately\n",
    "    train_losses_mean = np.ma.masked_invalid(train_losses_all).mean(axis=0)\n",
    "    test_losses_mean = np.ma.masked_invalid(test_losses_all).mean(axis=0)\n",
    "\n",
    "    # Sort by training loss\n",
    "    sorted_indices = np.argsort(train_losses_mean)\n",
    "\n",
    "    # Only plot the mean losses when we have at least 5 data points\n",
    "    mask = np.count_nonzero(~np.isnan(train_losses_all), axis=0) >= 5\n",
    "\n",
    "    # Plot the mean losses with the specific color\n",
    "    plt.plot(train_losses_mean[sorted_indices][mask[sorted_indices]], \n",
    "             test_losses_mean[sorted_indices][mask[sorted_indices]], \n",
    "             color=mean_color, linewidth=2.0, label=f\"{experiment} mean\", zorder=100)\n",
    "\n",
    "# Create custom patches for the legend\n",
    "patch1 = mpatches.Patch(color='blue', label='Not PT models')\n",
    "patch2 = mpatches.Patch(color='orange', label='PT models')\n",
    "patch3 = mpatches.Patch(color='black', label='Not PT mean')\n",
    "patch4 = mpatches.Patch(color='red', label='PT mean')\n",
    "\n",
    "plt.gca().set_xscale('log')\n",
    "plt.gca().set_yscale('log')\n",
    "plt.xlabel('Training loss', fontsize=18)\n",
    "plt.ylabel('Test loss', fontsize=18)\n",
    "plt.title('Training losses vs Test losses for different models (log scale)',fontsize=18)\n",
    "plt.legend(handles=[patch1, patch2, patch3, patch4])\n",
    "plt.savefig('./losses.png', dpi=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0475014",
   "metadata": {},
   "source": [
    "## Repeat the plots above but with proteomics predictions only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd40445",
   "metadata": {},
   "outputs": [],
   "source": [
    "#return final_pred, (out_ehr, pred_proteomics, pred_ehr, out_combined, final_pred, self.final_combine.weight)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = joint_model(RNN_data_codes.shape[2], best_hidden_dim, [best_hidden_dim], best_num_layers, 1,\n",
    "                    OOL_proteomics.shape[1]-2, [], [], best_dropout).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "proteomics = OOL_proteomics.merge(patient_indices[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID','DOS'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "proteomics = scaler.fit_transform(proteomics)\n",
    "\n",
    "EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes]  \n",
    "EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "proteomics = torch.tensor(proteomics).float()\n",
    "proteomics = torch.nan_to_num(proteomics)\n",
    "outcomes = torch.tensor(RNN_data_outcomes).float()\n",
    "\n",
    "\n",
    "data_set = DataBuilder(proteomics, outcomes, scaler)\n",
    "loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "loader_codes = create_dataloaders(EHR_codes, outcomes, RNN_data_lengths, 100000)\n",
    "            \n",
    "\n",
    "model_paths = [f'./models/predictive_models/both_OOL_PT_FT_woHCE_wobs_fixed_{i}_epoch{j}.pth' for i in range(25) for j in range(200)]\n",
    "model_number = 0\n",
    "\n",
    "all_outputs = []\n",
    "model_numbers = []\n",
    "epochs = []\n",
    "PT = []\n",
    "\n",
    "for i in tqdm(model_paths):\n",
    "    result = re.search('both_OOL_PT_FT_woHCE_wobs_fixed_(\\d+)_epoch(\\d+).pth', i)\n",
    "    model_number = int(result.group(1))\n",
    "    epoch_number = int(result.group(2))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            model_state_dict = torch.load(i)\n",
    "            model.load_state_dict(model_state_dict)\n",
    "            for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(loader_codes, loader_proteomics)):\n",
    "                inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                inputs_proteomics = inputs_proteomics.to(device)\n",
    "                outputs = model(inputs_codes, inputs_proteomics, lengths_codes, interpretability=True)[1][1]\n",
    "            all_outputs.append(outputs.squeeze().cpu().numpy())\n",
    "            model_numbers.append(model_number)\n",
    "            epochs.append(epoch_number)\n",
    "            PT.append('PT')\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "model = joint_model(RNN_data_codes.shape[2], best_hidden_dim_OOL, [best_hidden_dim_OOL], best_num_layers_OOL, 1,\n",
    "                    OOL_proteomics.shape[1]-2, [], [], best_dropout_OOL).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "proteomics = OOL_proteomics.merge(patient_indices[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID','DOS'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "proteomics = scaler.fit_transform(proteomics)\n",
    "\n",
    "EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes_OOL]  \n",
    "EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "proteomics = torch.tensor(proteomics).float()\n",
    "proteomics = torch.nan_to_num(proteomics)\n",
    "outcomes = torch.tensor(RNN_data_outcomes_OOL).float()\n",
    "\n",
    "\n",
    "data_set = DataBuilder(proteomics, outcomes, scaler)\n",
    "loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "loader_codes = create_dataloaders(EHR_codes, outcomes, RNN_data_lengths_OOL, 100000)\n",
    "            \n",
    "\n",
    "model_paths = [f'./models/predictive_models/both_OOL_only_woHCE_wobs_fixed_{i}_epoch{j}.pth' for i in range(25) for j in range(200)]\n",
    "model_number = 0\n",
    "\n",
    "for i in tqdm(model_paths):\n",
    "    result = re.search('both_OOL_only_woHCE_wobs_fixed_(\\d+)_epoch(\\d+).pth', i)\n",
    "    model_number = int(result.group(1))\n",
    "    epoch_number = int(result.group(2))\n",
    "    if True:\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                model_state_dict = torch.load(i)\n",
    "                model.load_state_dict(model_state_dict)\n",
    "                for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(loader_codes, loader_proteomics)):\n",
    "                    inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                    inputs_proteomics = inputs_proteomics.to(device)\n",
    "                    outputs = model(inputs_codes, inputs_proteomics, lengths_codes, interpretability=True)[1][1]\n",
    "                all_outputs.append(outputs.squeeze().cpu().numpy())\n",
    "                model_numbers.append(model_number)\n",
    "                epochs.append(epoch_number)\n",
    "                PT.append('NPT')\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b82756",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.array(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6b7c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom lines for the legend\n",
    "line1 = Line2D([0], [0], color='none', marker='o', markersize=10, markerfacecolor='darkred', label='PT')\n",
    "line2 = Line2D([0], [0], color='none', marker='x', markersize=10, markerfacecolor='darkblue', label='NPT')\n",
    "\n",
    "# Create a t-SNE instance and fit_transform the data\n",
    "tsne = TSNE(n_components=2, random_state=42, init='pca', learning_rate='auto')\n",
    "embedding = tsne.fit_transform(np.array(all_outputs))\n",
    "\n",
    "# Create a markers array based on PT array\n",
    "marker_map = {'PT': 'o', 'NPT': 'x'}\n",
    "markers = [marker_map[pt] for pt in PT]\n",
    "\n",
    "# Don't normalize the epochs\n",
    "colors = epochs\n",
    "\n",
    "# Create colormaps that goes from light red to dark red for PT, light blue to dark blue for NPT\n",
    "cmap_pt = LinearSegmentedColormap.from_list(\"mycmap_pt\", [\"lightcoral\", \"darkred\"])\n",
    "cmap_npt = LinearSegmentedColormap.from_list(\"mycmap_npt\", [\"lightblue\", \"darkblue\"])\n",
    "\n",
    "# Plot the results with different markers and colors\n",
    "for marker_type, marker in marker_map.items():\n",
    "    mask = np.array(markers) == marker\n",
    "    cmap = cmap_pt if marker_type == 'PT' else cmap_npt\n",
    "    sc = plt.scatter(embedding[mask, 0], embedding[mask, 1], marker=marker, c=colors[mask], cmap=cmap, alpha = 0.7)\n",
    "\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.gca().set_xticks([])\n",
    "plt.gca().set_yticks([])\n",
    "plt.title('Protein parameter space', fontsize=14)\n",
    "plt.legend(handles=[line1, line2])  # add the custom legend\n",
    "plt.savefig('./protein_params.png', dpi=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba43fa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform t-SNE dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=42, init='pca', learning_rate='auto')\n",
    "embedding = tsne.fit_transform(np.array(all_outputs))\n",
    "\n",
    "# Create colormaps that go from light red to dark red for PT, and light blue to dark blue for NPT\n",
    "cmap_pt = LinearSegmentedColormap.from_list(\"mycmap_pt\", [\"lightcoral\", \"darkred\"])\n",
    "cmap_npt = LinearSegmentedColormap.from_list(\"mycmap_npt\", [\"lightblue\", \"darkblue\"])\n",
    "\n",
    "fig, axs = plt.subplots(5, 5, figsize=(20, 20))\n",
    "\n",
    "# Calculate the global x and y limits\n",
    "xlim = (embedding[:,0].min()-1, embedding[:,0].max()+1)\n",
    "ylim = (embedding[:,1].min()-1, embedding[:,1].max()+1)\n",
    "\n",
    "for model_number in range(25):\n",
    "    ax = axs[model_number // 5, model_number % 5]\n",
    "\n",
    "    for model_type in ['PT', 'NPT']:\n",
    "        # Get mask for the current trajectory\n",
    "        mask = (np.array(model_numbers) == model_number) & (np.array(PT) == model_type)\n",
    "\n",
    "        # Get the points and corresponding epochs for the current trajectory\n",
    "        trajectory_points = embedding[mask]\n",
    "        trajectory_epochs = np.array(epochs)[mask]\n",
    "\n",
    "        # Sort the points and epochs\n",
    "        sort_indices = np.argsort(trajectory_epochs)\n",
    "        sorted_points = trajectory_points[sort_indices]\n",
    "\n",
    "        # Select the colormap based on model_type\n",
    "        cmap = cmap_pt if model_type == 'PT' else cmap_npt\n",
    "\n",
    "        # Plot the points with color indicating epoch and marker indicating PT/NPT\n",
    "        ax.scatter(sorted_points[:, 0], sorted_points[:, 1], \n",
    "                   c=trajectory_epochs[sort_indices], \n",
    "                   cmap=cmap, \n",
    "                   marker=marker_map[model_type], \n",
    "                   alpha=0.6)\n",
    "\n",
    "        # Plot lines connecting the points of the same model type\n",
    "        ax.plot(sorted_points[:, 0], sorted_points[:, 1], \n",
    "                color='lightgrey' if model_type == 'PT' else 'black', \n",
    "                linestyle='--')\n",
    "\n",
    "    ax.set_aspect('auto')\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "    # Turn off tick labels\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./protein_paths_ool.png',dpi=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46054d61",
   "metadata": {},
   "source": [
    "## Again, with the joint predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeca848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#return final_pred, (out_ehr, pred_proteomics, pred_ehr, out_combined, final_pred, self.final_combine.weight)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = joint_model(RNN_data_codes.shape[2], best_hidden_dim, [best_hidden_dim], best_num_layers, 1,\n",
    "                    OOL_proteomics.shape[1]-2, [], [], best_dropout).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "proteomics = OOL_proteomics.merge(patient_indices[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID','DOS'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "proteomics = scaler.fit_transform(proteomics)\n",
    "\n",
    "EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes]  \n",
    "EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "proteomics = torch.tensor(proteomics).float()\n",
    "proteomics = torch.nan_to_num(proteomics)\n",
    "outcomes = torch.tensor(RNN_data_outcomes).float()\n",
    "\n",
    "\n",
    "data_set = DataBuilder(proteomics, outcomes, scaler)\n",
    "loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "loader_codes = create_dataloaders(EHR_codes, outcomes, RNN_data_lengths, 100000)\n",
    "            \n",
    "    \n",
    "model_paths = [f'./models/predictive_models/both_OOL_PT_FT_woHCE_wobs_fixed_{i}_epoch{j}.pth' for i in range(25) for j in range(200)]\n",
    "model_number = 0\n",
    "\n",
    "all_outputs = []\n",
    "model_numbers = []\n",
    "epochs = []\n",
    "PT = []\n",
    "\n",
    "for i in tqdm(model_paths):\n",
    "    result = re.search('both_OOL_PT_FT_woHCE_wobs_fixed_(\\d+)_epoch(\\d+).pth', i)\n",
    "    model_number = int(result.group(1))\n",
    "    epoch_number = int(result.group(2))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            model_state_dict = torch.load(i)\n",
    "            model.load_state_dict(model_state_dict)\n",
    "            for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(loader_codes, loader_proteomics)):\n",
    "                inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                inputs_proteomics = inputs_proteomics.to(device)\n",
    "                outputs = model(inputs_codes, inputs_proteomics, lengths_codes, interpretability=True)[1][3]\n",
    "            all_outputs.append(outputs.squeeze().cpu().numpy())\n",
    "            model_numbers.append(model_number)\n",
    "            epochs.append(epoch_number)\n",
    "            PT.append('PT')\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "model = joint_model(RNN_data_codes.shape[2], best_hidden_dim_OOL, [best_hidden_dim_OOL], best_num_layers_OOL, 1,\n",
    "                    OOL_proteomics.shape[1]-2, [], [], best_dropout_OOL).to(device)\n",
    "\n",
    "model.eval()\n",
    "model_paths = [f'./models/predictive_models/both_OOL_only_woHCE_wobs_fixed_{i}_epoch{j}.pth' for i in range(25) for j in range(200)]\n",
    "model_number = 0\n",
    "\n",
    "for i in tqdm(model_paths):\n",
    "    result = re.search('both_OOL_only_woHCE_wobs_fixed_(\\d+)_epoch(\\d+).pth', i)\n",
    "    model_number = int(result.group(1))\n",
    "    epoch_number = int(result.group(2))\n",
    "    if True:\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                model_state_dict = torch.load(i)\n",
    "                model.load_state_dict(model_state_dict)\n",
    "                for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(loader_codes, loader_proteomics)):\n",
    "                    inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                    inputs_proteomics = inputs_proteomics.to(device)\n",
    "                    outputs = model(inputs_codes, inputs_proteomics, lengths_codes, interpretability=True)[1][3]\n",
    "                all_outputs.append(outputs.squeeze().cpu().numpy())\n",
    "                model_numbers.append(model_number)\n",
    "                epochs.append(epoch_number)\n",
    "                PT.append('NPT')\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76cd18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.array(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc00bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom lines for the legend\n",
    "line1 = Line2D([0], [0], color='none', marker='o', markersize=10, markerfacecolor='black', label='PT')\n",
    "line2 = Line2D([0], [0], color='none', marker='x', markersize=10, markerfacecolor='black', label='NPT')\n",
    "\n",
    "# Create a t-SNE instance and fit_transform the data\n",
    "tsne = TSNE(n_components=2, random_state=42, init='pca', learning_rate='auto')\n",
    "embedding = tsne.fit_transform(np.array(all_outputs))\n",
    "\n",
    "# Create a markers array based on PT array\n",
    "marker_map = {'PT': 'o', 'NPT': 'x'}\n",
    "markers = [marker_map[pt] for pt in PT]\n",
    "\n",
    "# Don't normalize the epochs\n",
    "colors = epochs\n",
    "\n",
    "# Create colormaps that goes from light red to dark red for PT, light blue to dark blue for NPT\n",
    "cmap_pt = LinearSegmentedColormap.from_list(\"mycmap_pt\", [\"lightcoral\", \"darkred\"])\n",
    "cmap_npt = LinearSegmentedColormap.from_list(\"mycmap_npt\", [\"lightblue\", \"darkblue\"])\n",
    "\n",
    "# Plot the results with different markers and colors\n",
    "for marker_type, marker in marker_map.items():\n",
    "    mask = np.array(markers) == marker\n",
    "    cmap = cmap_pt if marker_type == 'PT' else cmap_npt\n",
    "    sc = plt.scatter(embedding[mask, 0], embedding[mask, 1], marker=marker, c=colors[mask], cmap=cmap, alpha = 0.7)\n",
    "\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.gca().set_xticks([])\n",
    "plt.gca().set_yticks([])\n",
    "plt.title('Joint EHR-protein parameter space', fontsize=16)\n",
    "plt.legend(handles=[line1, line2])  # add the custom legend\n",
    "plt.savefig('./joint_params.png',dpi=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b353700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform t-SNE dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=42, init='pca', learning_rate='auto')\n",
    "embedding = tsne.fit_transform(np.array(all_outputs))\n",
    "\n",
    "# Create colormaps that go from light red to dark red for PT, and light blue to dark blue for NPT\n",
    "cmap_pt = LinearSegmentedColormap.from_list(\"mycmap_pt\", [\"lightcoral\", \"darkred\"])\n",
    "cmap_npt = LinearSegmentedColormap.from_list(\"mycmap_npt\", [\"lightblue\", \"darkblue\"])\n",
    "\n",
    "fig, axs = plt.subplots(5, 5, figsize=(20, 20))\n",
    "\n",
    "# Calculate the global x and y limits\n",
    "xlim = (embedding[:,0].min()-1, embedding[:,0].max()+1)\n",
    "ylim = (embedding[:,1].min()-1, embedding[:,1].max()+1)\n",
    "\n",
    "for model_number in range(25):\n",
    "    ax = axs[model_number // 5, model_number % 5]\n",
    "\n",
    "    for model_type in ['PT', 'NPT']:\n",
    "        # Get mask for the current trajectory\n",
    "        mask = (np.array(model_numbers) == model_number) & (np.array(PT) == model_type)\n",
    "\n",
    "        # Get the points and corresponding epochs for the current trajectory\n",
    "        trajectory_points = embedding[mask]\n",
    "        trajectory_epochs = np.array(epochs)[mask]\n",
    "\n",
    "        # Sort the points and epochs\n",
    "        sort_indices = np.argsort(trajectory_epochs)\n",
    "        sorted_points = trajectory_points[sort_indices]\n",
    "\n",
    "        # Select the colormap based on model_type\n",
    "        cmap = cmap_pt if model_type == 'PT' else cmap_npt\n",
    "\n",
    "        # Plot the points with color indicating epoch and marker indicating PT/NPT\n",
    "        ax.scatter(sorted_points[:, 0], sorted_points[:, 1], \n",
    "                   c=trajectory_epochs[sort_indices], \n",
    "                   cmap=cmap, \n",
    "                   marker=marker_map[model_type], \n",
    "                   alpha=0.6)\n",
    "\n",
    "        # Plot lines connecting the points of the same model type\n",
    "        ax.plot(sorted_points[:, 0], sorted_points[:, 1], \n",
    "                color='lightgrey' if model_type == 'PT' else 'black', \n",
    "                linestyle='--')\n",
    "\n",
    "    ax.set_aspect('auto')\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "    # Turn off tick labels\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./joint_paths_ool.png',dpi=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916d7b88",
   "metadata": {},
   "source": [
    "## Lastly, EHR predictions (not latent space like above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b86277",
   "metadata": {},
   "outputs": [],
   "source": [
    "#return final_pred, (out_ehr, pred_proteomics, pred_ehr, out_combined, final_pred, self.final_combine.weight)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = joint_model(RNN_data_codes.shape[2], best_hidden_dim, [best_hidden_dim], best_num_layers, 1,\n",
    "                    OOL_proteomics.shape[1]-2, [], [], best_dropout).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "proteomics = OOL_proteomics.merge(patient_indices[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID','DOS'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "proteomics = scaler.fit_transform(proteomics)\n",
    "\n",
    "EHR_codes = [torch.tensor(data).float() for data in RNN_data_codes]  \n",
    "EHR_codes = [torch.nan_to_num(x) for x in EHR_codes]\n",
    "proteomics = torch.tensor(proteomics).float()\n",
    "proteomics = torch.nan_to_num(proteomics)\n",
    "outcomes = torch.tensor(RNN_data_outcomes).float()\n",
    "\n",
    "\n",
    "data_set = DataBuilder(proteomics, outcomes, scaler)\n",
    "loader_proteomics = DataLoader(dataset=data_set,batch_size=100000, worker_init_fn=worker_init_fn)\n",
    "loader_codes = create_dataloaders(EHR_codes, outcomes, RNN_data_lengths, 100000)\n",
    "            \n",
    "model_paths = [f'./models/predictive_models/both_OOL_PT_FT_woHCE_wobs_fixed_{i}_epoch{j}.pth' for i in range(25) for j in range(200)]\n",
    "model_number = 0\n",
    "\n",
    "all_outputs = []\n",
    "model_numbers = []\n",
    "epochs = []\n",
    "PT = []\n",
    "\n",
    "for i in tqdm(model_paths):\n",
    "    result = re.search('both_OOL_PT_FT_woHCE_wobs_fixed_(\\d+)_epoch(\\d+).pth', i)\n",
    "    model_number = int(result.group(1))\n",
    "    epoch_number = int(result.group(2))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            model_state_dict = torch.load(i)\n",
    "            model.load_state_dict(model_state_dict)\n",
    "            for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(loader_codes, loader_proteomics)):\n",
    "                inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                inputs_proteomics = inputs_proteomics.to(device)\n",
    "                outputs = model(inputs_codes, inputs_proteomics, lengths_codes, interpretability=True)[1][2]\n",
    "            all_outputs.append(outputs.squeeze().cpu().numpy())\n",
    "            model_numbers.append(model_number)\n",
    "            epochs.append(epoch_number)\n",
    "            PT.append('PT')\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "model = joint_model(RNN_data_codes.shape[2], best_hidden_dim_OOL, [best_hidden_dim_OOL], best_num_layers_OOL, 1,\n",
    "                    OOL_proteomics.shape[1]-2, [], [], best_dropout_OOL).to(device)\n",
    "\n",
    "model.eval() \n",
    "model_paths = [f'./models/predictive_models/both_OOL_only_woHCE_wobs_fixed_{i}_epoch{j}.pth' for i in range(25) for j in range(200)]\n",
    "model_number = 0\n",
    "\n",
    "for i in tqdm(model_paths):\n",
    "    result = re.search('both_OOL_only_woHCE_wobs_fixed_(\\d+)_epoch(\\d+).pth', i)\n",
    "    model_number = int(result.group(1))\n",
    "    epoch_number = int(result.group(2))\n",
    "    if True:\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                model_state_dict = torch.load(i)\n",
    "                model.load_state_dict(model_state_dict)\n",
    "                for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(loader_codes, loader_proteomics)):\n",
    "                    inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                    inputs_proteomics = inputs_proteomics.to(device)\n",
    "                    outputs = model(inputs_codes, inputs_proteomics, lengths_codes, interpretability=True)[1][2]\n",
    "                all_outputs.append(outputs.squeeze().cpu().numpy())\n",
    "                model_numbers.append(model_number)\n",
    "                epochs.append(epoch_number)\n",
    "                PT.append('NPT')\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b83d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.array(epochs)\n",
    "# Create custom lines for the legend\n",
    "line1 = Line2D([0], [0], color='none', marker='o', markersize=10, markerfacecolor='black', label='PT')\n",
    "line2 = Line2D([0], [0], color='none', marker='x', markersize=10, markerfacecolor='black', label='NPT')\n",
    "\n",
    "# Create a t-SNE instance and fit_transform the data\n",
    "tsne = TSNE(n_components=2, random_state=42, init='pca', learning_rate='auto')\n",
    "embedding = tsne.fit_transform(np.array(all_outputs))\n",
    "\n",
    "# Create a markers array based on PT array\n",
    "marker_map = {'PT': 'o', 'NPT': 'x'}\n",
    "markers = [marker_map[pt] for pt in PT]\n",
    "\n",
    "# Don't normalize the epochs\n",
    "colors = epochs\n",
    "\n",
    "# Create colormaps that goes from light red to dark red for PT, light blue to dark blue for NPT\n",
    "cmap_pt = LinearSegmentedColormap.from_list(\"mycmap_pt\", [\"lightcoral\", \"darkred\"])\n",
    "cmap_npt = LinearSegmentedColormap.from_list(\"mycmap_npt\", [\"lightblue\", \"darkblue\"])\n",
    "\n",
    "# Plot the results with different markers and colors\n",
    "for marker_type, marker in marker_map.items():\n",
    "    mask = np.array(markers) == marker\n",
    "    cmap = cmap_pt if marker_type == 'PT' else cmap_npt\n",
    "    sc = plt.scatter(embedding[mask, 0], embedding[mask, 1], marker=marker, c=colors[mask], cmap=cmap, alpha = 0.7)\n",
    "\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.gca().set_xticks([])\n",
    "plt.gca().set_yticks([])\n",
    "plt.title('EHR parameter space', fontsize=14)\n",
    "plt.legend(handles=[line1, line2])  # add the custom legend\n",
    "plt.savefig('./EHR_param_space.png',dpi=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863c7bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform t-SNE dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=42, init='pca', learning_rate='auto')\n",
    "embedding = tsne.fit_transform(np.array(all_outputs))\n",
    "\n",
    "# Create colormaps that go from light red to dark red for PT, and light blue to dark blue for NPT\n",
    "cmap_pt = LinearSegmentedColormap.from_list(\"mycmap_pt\", [\"lightcoral\", \"darkred\"])\n",
    "cmap_npt = LinearSegmentedColormap.from_list(\"mycmap_npt\", [\"lightblue\", \"darkblue\"])\n",
    "\n",
    "fig, axs = plt.subplots(5, 5, figsize=(20, 20))\n",
    "\n",
    "# Calculate the global x and y limits\n",
    "xlim = (embedding[:,0].min()-1, embedding[:,0].max()+1)\n",
    "ylim = (embedding[:,1].min()-1, embedding[:,1].max()+1)\n",
    "\n",
    "for model_number in range(25):\n",
    "    ax = axs[model_number // 5, model_number % 5]\n",
    "\n",
    "    for model_type in ['PT', 'NPT']:\n",
    "        # Get mask for the current trajectory\n",
    "        mask = (np.array(model_numbers) == model_number) & (np.array(PT) == model_type)\n",
    "\n",
    "        # Get the points and corresponding epochs for the current trajectory\n",
    "        trajectory_points = embedding[mask]\n",
    "        trajectory_epochs = np.array(epochs)[mask]\n",
    "\n",
    "        # Sort the points and epochs\n",
    "        sort_indices = np.argsort(trajectory_epochs)\n",
    "        sorted_points = trajectory_points[sort_indices]\n",
    "\n",
    "        # Select the colormap based on model_type\n",
    "        cmap = cmap_pt if model_type == 'PT' else cmap_npt\n",
    "\n",
    "        # Plot the points with color indicating epoch and marker indicating PT/NPT\n",
    "        ax.scatter(sorted_points[:, 0], sorted_points[:, 1], \n",
    "                   c=trajectory_epochs[sort_indices], \n",
    "                   cmap=cmap, \n",
    "                   marker=marker_map[model_type], \n",
    "                   alpha=0.6)\n",
    "\n",
    "        # Plot lines connecting the points of the same model type\n",
    "        ax.plot(sorted_points[:, 0], sorted_points[:, 1], \n",
    "                color='lightgrey' if model_type == 'PT' else 'black', \n",
    "                linestyle='--')\n",
    "\n",
    "    ax.set_aspect('auto')\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "    # Turn off tick labels\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./EHR_paths_ool.png',dpi=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020c4678",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
